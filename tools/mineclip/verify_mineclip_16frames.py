#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
éªŒè¯MineCLIP 16å¸§è§†é¢‘æ¨¡å¼çš„æ•ˆæœ
ä½¿ç”¨å½•åˆ¶çš„å¸§åºåˆ—æˆ–å·²æœ‰çš„logs/frames
"""

import os
import glob
import torch
import numpy as np
from PIL import Image
from mineclip import MineCLIP
from transformers import CLIPTokenizer
import matplotlib.pyplot as plt

def load_frames_from_directory(directory, max_frames=None):
    """
    ä»ç›®å½•åŠ è½½å¸§åºåˆ—
    
    Args:
        directory: å¸§ç›®å½•
        max_frames: æœ€å¤§åŠ è½½å¸§æ•°
    
    Returns:
        frames: List of [H, W, C] numpy arrays
        filenames: å¯¹åº”çš„æ–‡ä»¶å
    """
    # æŸ¥æ‰¾æ‰€æœ‰å›¾åƒæ–‡ä»¶
    image_files = sorted(glob.glob(os.path.join(directory, "*.png")))
    
    if max_frames:
        image_files = image_files[:max_frames]
    
    print(f"  æ‰¾åˆ° {len(image_files)} å¸§")
    
    frames = []
    filenames = []
    
    for img_path in image_files:
        img = Image.open(img_path).convert('RGB')
        img_array = np.array(img)  # [H, W, C]
        frames.append(img_array)
        filenames.append(os.path.basename(img_path))
    
    return frames, filenames

def compute_16frame_similarity(model, tokenizer, frames_16, text_prompt, device):
    """
    è®¡ç®—16å¸§è§†é¢‘ä¸æ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆå®˜æ–¹æ–¹å¼ï¼‰
    
    Args:
        model: MineCLIPæ¨¡å‹
        tokenizer: CLIP tokenizer
        frames_16: List of 16 frames [H, W, C]
        text_prompt: æ–‡æœ¬æè¿°
        device: è®¾å¤‡
    
    Returns:
        similarity: ç›¸ä¼¼åº¦åˆ†æ•°
    """
    with torch.no_grad():
        # MineCraftå®˜æ–¹å½’ä¸€åŒ–å‚æ•°
        MC_MEAN = torch.tensor([0.3331, 0.3245, 0.3051], device=device).view(1, 1, 3, 1, 1)
        MC_STD = torch.tensor([0.2439, 0.2493, 0.2873], device=device).view(1, 1, 3, 1, 1)
        
        # é¢„å¤„ç†å¸§
        processed = []
        for frame in frames_16:
            # è½¬tensor
            frame_t = torch.from_numpy(frame).float() / 255.0  # [H, W, C]
            # HWC -> CHW
            frame_t = frame_t.permute(2, 0, 1)  # [C, H, W]
            processed.append(frame_t)
        
        # å †å ä¸ºè§†é¢‘ [1, T, C, H, W]
        video = torch.stack(processed).unsqueeze(0).to(device)
        
        # MineCraftå½’ä¸€åŒ–
        video = (video - MC_MEAN) / MC_STD
        
        # ç¼–ç è§†é¢‘ï¼ˆå®˜æ–¹å®Œæ•´æµç¨‹ï¼‰
        video_features = model.encode_video(video)
        video_features = video_features / video_features.norm(dim=-1, keepdim=True)
        
        # ç¼–ç æ–‡æœ¬
        tokens = tokenizer(text_prompt, return_tensors="pt", padding="max_length",
                          truncation=True, max_length=77)
        text_features = model.encode_text(tokens['input_ids'].to(device))
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = (video_features @ text_features.T).item()
    
    return similarity

def analyze_sequence(sequence_dir, num_frames=16, stride=4, task_prompt="chopping a tree with hand"):
    """
    åˆ†ææ•´ä¸ªå¸§åºåˆ—
    
    Args:
        sequence_dir: å¸§åºåˆ—ç›®å½•
        num_frames: æ¯ä¸ªè§†é¢‘ç‰‡æ®µçš„å¸§æ•°
        stride: æ»‘åŠ¨çª—å£æ­¥é•¿
        task_prompt: ä»»åŠ¡æè¿°
    """
    device = "auto"
    
    print("=" * 80)
    print("MineCLIP 16å¸§è§†é¢‘æ¨¡å¼éªŒè¯")
    print("=" * 80)
    
    # åŠ è½½MineCLIP
    print("\n[1/4] åŠ è½½MineCLIP...")
    model = MineCLIP(
        arch="vit_base_p16_fz.v2.t2",
        hidden_dim=512,
        image_feature_dim=512,
        mlp_adapter_spec='v0-2.t0',
        pool_type="attn.d2.nh8.glusw",
        resolution=(160, 256)
    )
    model.load_ckpt("data/mineclip/attn.pth", strict=True)
    model.eval()
    model = model.to(device)
    
    tokenizer = CLIPTokenizer.from_pretrained("data/clip_tokenizer")
    print(f"  âœ“ MineCLIPå·²åŠ è½½")
    print(f"  ä»»åŠ¡æè¿°: '{task_prompt}'")
    
    # åŠ è½½å¸§åºåˆ—
    print(f"\n[2/4] åŠ è½½å¸§åºåˆ—...")
    print(f"  ç›®å½•: {sequence_dir}")
    
    frames, filenames = load_frames_from_directory(sequence_dir)
    print(f"  âœ“ åŠ è½½äº† {len(frames)} å¸§")
    
    if len(frames) < num_frames:
        print(f"  âš ï¸  å¸§æ•°ä¸è¶³16å¸§ï¼Œæ— æ³•è¿›è¡Œ16å¸§åˆ†æ")
        return
    
    # æ»‘åŠ¨çª—å£åˆ†æ
    print(f"\n[3/4] æ»‘åŠ¨çª—å£åˆ†æ...")
    print(f"  çª—å£å¤§å°: {num_frames}å¸§")
    print(f"  æ»‘åŠ¨æ­¥é•¿: {stride}å¸§")
    
    similarities = []
    window_indices = []
    
    for i in range(0, len(frames) - num_frames + 1, stride):
        # æå–16å¸§
        frames_16 = frames[i:i+num_frames]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        sim = compute_16frame_similarity(model, tokenizer, frames_16, task_prompt, device)
        
        similarities.append(sim)
        window_indices.append(i)
        
        print(f"  çª—å£ {len(similarities):3d} [å¸§{i:04d}-{i+num_frames-1:04d}]: ç›¸ä¼¼åº¦ = {sim:.4f}", end='\r')
    
    print(f"\n  âœ“ åˆ†æäº† {len(similarities)} ä¸ªçª—å£")
    
    # ç»Ÿè®¡åˆ†æ
    print(f"\n[4/4] ç»Ÿè®¡åˆ†æ...")
    print("=" * 80)
    
    similarities_np = np.array(similarities)
    
    print(f"\nğŸ“Š ç›¸ä¼¼åº¦ç»Ÿè®¡:")
    print(f"  æœ€å°å€¼: {similarities_np.min():.4f}")
    print(f"  æœ€å¤§å€¼: {similarities_np.max():.4f}")
    print(f"  å¹³å‡å€¼: {similarities_np.mean():.4f}")
    print(f"  æ ‡å‡†å·®: {similarities_np.std():.4f}")
    print(f"  å˜åŒ–èŒƒå›´: {similarities_np.max() - similarities_np.min():.4f}")
    
    # æ‰¾åˆ°æœ€é«˜å’Œæœ€ä½ç›¸ä¼¼åº¦çš„çª—å£
    max_idx = np.argmax(similarities_np)
    min_idx = np.argmin(similarities_np)
    
    print(f"\nğŸ” æœ€é«˜ç›¸ä¼¼åº¦çª—å£:")
    print(f"  ä½ç½®: å¸§ {window_indices[max_idx]:04d}-{window_indices[max_idx]+num_frames-1:04d}")
    print(f"  ç›¸ä¼¼åº¦: {similarities_np[max_idx]:.4f}")
    print(f"  å¸§æ–‡ä»¶: {filenames[window_indices[max_idx]]} ~ {filenames[window_indices[max_idx]+num_frames-1]}")
    
    print(f"\nğŸ”» æœ€ä½ç›¸ä¼¼åº¦çª—å£:")
    print(f"  ä½ç½®: å¸§ {window_indices[min_idx]:04d}-{window_indices[min_idx]+num_frames-1:04d}")
    print(f"  ç›¸ä¼¼åº¦: {similarities_np[min_idx]:.4f}")
    print(f"  å¸§æ–‡ä»¶: {filenames[window_indices[min_idx]]} ~ {filenames[window_indices[min_idx]+num_frames-1]}")
    
    # å¯è§†åŒ–
    print(f"\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plt.figure(figsize=(12, 6))
    plt.plot(window_indices, similarities, marker='o', markersize=3, linewidth=1)
    plt.xlabel('èµ·å§‹å¸§ç´¢å¼•')
    plt.ylabel('MineCLIPç›¸ä¼¼åº¦')
    plt.title(f'MineCLIP 16å¸§è§†é¢‘ç›¸ä¼¼åº¦å˜åŒ–\nä»»åŠ¡: "{task_prompt}"')
    plt.grid(True, alpha=0.3)
    plt.axhline(y=similarities_np.mean(), color='r', linestyle='--', label=f'å¹³å‡å€¼: {similarities_np.mean():.4f}')
    plt.legend()
    
    # ä¿å­˜å›¾è¡¨
    plot_path = os.path.join(sequence_dir, "similarity_analysis.png")
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    print(f"  âœ“ å›¾è¡¨å·²ä¿å­˜: {plot_path}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_path = os.path.join(sequence_dir, "similarity_results.txt")
    with open(results_path, 'w') as f:
        f.write(f"MineCLIP 16å¸§è§†é¢‘æ¨¡å¼éªŒè¯ç»“æœ\n")
        f.write(f"=" * 80 + "\n\n")
        f.write(f"ä»»åŠ¡æè¿°: {task_prompt}\n")
        f.write(f"çª—å£å¤§å°: {num_frames}å¸§\n")
        f.write(f"æ»‘åŠ¨æ­¥é•¿: {stride}å¸§\n")
        f.write(f"æ€»å¸§æ•°: {len(frames)}\n")
        f.write(f"åˆ†æçª—å£æ•°: {len(similarities)}\n\n")
        f.write(f"ç»Ÿè®¡ç»“æœ:\n")
        f.write(f"  æœ€å°å€¼: {similarities_np.min():.4f}\n")
        f.write(f"  æœ€å¤§å€¼: {similarities_np.max():.4f}\n")
        f.write(f"  å¹³å‡å€¼: {similarities_np.mean():.4f}\n")
        f.write(f"  æ ‡å‡†å·®: {similarities_np.std():.4f}\n")
        f.write(f"  å˜åŒ–èŒƒå›´: {similarities_np.max() - similarities_np.min():.4f}\n\n")
        f.write(f"è¯¦ç»†ç»“æœ:\n")
        for i, (idx, sim) in enumerate(zip(window_indices, similarities)):
            f.write(f"çª—å£ {i+1:3d} [å¸§{idx:04d}-{idx+num_frames-1:04d}]: {sim:.4f}\n")
    
    print(f"  âœ“ è¯¦ç»†ç»“æœå·²ä¿å­˜: {results_path}")
    
    print("\n" + "=" * 80)
    print("éªŒè¯å®Œæˆï¼")
    print("=" * 80)
    
    # è¯„ä¼°ç»“è®º
    print(f"\nğŸ¯ è¯„ä¼°ç»“è®º:")
    range_val = similarities_np.max() - similarities_np.min()
    
    if range_val > 0.05:
        print(f"  âœ… 16å¸§è§†é¢‘æ¨¡å¼æ•ˆæœè‰¯å¥½ï¼å˜åŒ–èŒƒå›´è¾¾åˆ° {range_val:.4f}")
        print(f"  â†’ å»ºè®®ä½¿ç”¨16å¸§è§†é¢‘æ¨¡å¼è¿›è¡Œè®­ç»ƒ")
    elif range_val > 0.02:
        print(f"  âš ï¸  16å¸§è§†é¢‘æ¨¡å¼æœ‰ä¸€å®šæ•ˆæœï¼Œå˜åŒ–èŒƒå›´ä¸º {range_val:.4f}")
        print(f"  â†’ å¯ä»¥å°è¯•ï¼Œä½†å¯èƒ½éœ€è¦è°ƒæ•´æç¤ºè¯")
    else:
        print(f"  âŒ 16å¸§è§†é¢‘æ¨¡å¼åŒºåˆ†åº¦è¾ƒä½ï¼Œå˜åŒ–èŒƒå›´ä»… {range_val:.4f}")
        print(f"  â†’ è€ƒè™‘å…¶ä»–æ–¹æ¡ˆï¼ˆä»»åŠ¡åˆ†è§£ã€å¥–åŠ±å¡‘å½¢ç­‰ï¼‰")
    
    print()

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="éªŒè¯MineCLIP 16å¸§è§†é¢‘æ¨¡å¼")
    parser.add_argument('--sequence-dir', type=str, default='logs/chopping_sequence',
                       help='å¸§åºåˆ—ç›®å½•')
    parser.add_argument('--num-frames', type=int, default=16,
                       help='æ¯ä¸ªè§†é¢‘ç‰‡æ®µçš„å¸§æ•°')
    parser.add_argument('--stride', type=int, default=4,
                       help='æ»‘åŠ¨çª—å£æ­¥é•¿')
    parser.add_argument('--task-prompt', type=str, default='chopping a tree with hand',
                       help='ä»»åŠ¡æè¿°')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.sequence_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {args.sequence_dir}")
        print(f"\nè¯·å…ˆå½•åˆ¶å¸§åºåˆ—ï¼š")
        print(f"  python record_chopping_sequence.py --output-dir {args.sequence_dir}")
        return
    
    analyze_sequence(args.sequence_dir, args.num_frames, args.stride, args.task_prompt)

if __name__ == "__main__":
    main()

