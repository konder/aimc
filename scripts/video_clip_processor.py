#!/usr/bin/env python3
"""
è§†é¢‘åˆ‡ç‰‡å¤„ç†è„šæœ¬

æ ¹æ®å·²ä¸‹è½½çš„è§†é¢‘å’Œ CLIP4MC å…ƒæ•°æ®ï¼Œç”Ÿæˆæ–‡æœ¬-è§†é¢‘ç‰‡æ®µå¯¹ã€‚

ä½¿ç”¨æ–¹æ³•:
    python video_clip_processor.py \
        --videos-dir ./videos \
        --info-csv ./info.csv \
        --metadata ./dataset_test.json \
        --output-dir ./clips
"""

import argparse
import csv
import json
import logging
import os
import re
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


def extract_video_id(url: str) -> Optional[str]:
    """ä» YouTube URL ä¸­æå–è§†é¢‘ ID"""
    patterns = [
        r'(?:https?://)?(?:www\.)?youtube\.com/watch\?v=([a-zA-Z0-9_-]{11})',
        r'(?:https?://)?youtu\.be/([a-zA-Z0-9_-]{11})',
        r'v=([a-zA-Z0-9_-]{11})',
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None


def parse_info_csv(csv_path: Path) -> Dict[str, str]:
    """
    è§£æ info.csvï¼Œè¿”å› video_id -> filename çš„æ˜ å°„
    
    CSV æ ¼å¼: URL,filename (å¯èƒ½æœ‰åŒå¼•å·)
    """
    vid_to_filename = {}
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        # å°è¯•è‡ªåŠ¨æ£€æµ‹åˆ†éš”ç¬¦
        sample = f.read(2048)
        f.seek(0)
        
        # æ£€æµ‹æ˜¯å¦æœ‰è¡¨å¤´
        has_header = 'url' in sample.lower() or 'http' not in sample[:100].lower()
        
        try:
            # å°è¯•ç”¨ csv æ¨¡å—è§£æ
            reader = csv.reader(f)
            if has_header:
                next(reader)  # è·³è¿‡è¡¨å¤´
            
            for row in reader:
                if len(row) >= 2:
                    url = row[0].strip().strip('"')
                    filename = row[1].strip().strip('"')
                    
                    vid = extract_video_id(url)
                    if vid:
                        vid_to_filename[vid] = filename
        except Exception as e:
            logger.warning(f"CSV è§£æå¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨è§£æ: {e}")
            f.seek(0)
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                # æ‰‹åŠ¨è§£æ
                parts = line.split(',', 1)
                if len(parts) >= 2:
                    url = parts[0].strip().strip('"')
                    filename = parts[1].strip().strip('"')
                    
                    vid = extract_video_id(url)
                    if vid:
                        vid_to_filename[vid] = filename
    
    logger.info(f"ä» info.csv è§£æäº† {len(vid_to_filename)} æ¡è®°å½•")
    return vid_to_filename


def normalize_filename(name: str) -> str:
    """æ ‡å‡†åŒ–æ–‡ä»¶åç”¨äºåŒ¹é… - åªä¿ç•™å­—æ¯æ•°å­—"""
    # ç§»é™¤æ‰©å±•å
    name = re.sub(r'\.(mp4|webm|mkv|avi|mov)$', '', name, flags=re.IGNORECASE)
    # è½¬å°å†™
    name = name.lower()
    # ç§»é™¤æ‰€æœ‰éå­—æ¯æ•°å­—å­—ç¬¦
    name = re.sub(r'[^a-z0-9]', '', name)
    return name


def extract_keywords(name: str) -> set:
    """æå–æ–‡ä»¶åä¸­çš„å…³é”®è¯"""
    # ç§»é™¤æ‰©å±•å
    name = re.sub(r'\.(mp4|webm|mkv|avi|mov)$', '', name, flags=re.IGNORECASE)
    # è½¬å°å†™
    name = name.lower()
    # åˆ†è¯ï¼ˆæŒ‰éå­—æ¯æ•°å­—åˆ†å‰²ï¼‰
    words = re.split(r'[^a-z0-9]+', name)
    # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¯å’Œå¸¸è§è¯
    stopwords = {'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'and', 'or', 'is', 'it', 'my', 'ep', 'episode', 'part', 'minecraft', 'with', 'vs'}
    keywords = set(w for w in words if len(w) > 2 and w not in stopwords)
    return keywords


def find_video_file(videos_dir: Path, filename: str, all_files: dict = None, keywords_index: dict = None) -> Optional[Path]:
    """æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼‰"""
    # ç›´æ¥åŒ¹é…
    direct_path = videos_dir / filename
    if direct_path.exists():
        return direct_path
    
    # åŠ ä¸Š .mp4 æ‰©å±•å
    if not filename.endswith('.mp4'):
        mp4_path = videos_dir / f"{filename}.mp4"
        if mp4_path.exists():
            return mp4_path
    
    # ä½¿ç”¨é¢„å»ºçš„æ–‡ä»¶ç´¢å¼•è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
    if all_files is not None:
        normalized = normalize_filename(filename)
        
        # ç²¾ç¡®åŒ¹é…æ ‡å‡†åŒ–åçš„åå­—
        if normalized in all_files:
            return all_files[normalized]
        
        # å°è¯•éƒ¨åˆ†åŒ¹é…
        for norm_name, path in all_files.items():
            # å¦‚æœæ ‡å‡†åŒ–åçš„åå­—åŒ…å«å…³ç³»
            if len(normalized) > 15 and len(norm_name) > 15:
                if normalized in norm_name or norm_name in normalized:
                    return path
                # å¦‚æœå‰15ä¸ªå­—ç¬¦ç›¸åŒ
                if normalized[:15] == norm_name[:15]:
                    return path
        
        # å…³é”®è¯åŒ¹é…
        if keywords_index:
            query_keywords = extract_keywords(filename)
            if len(query_keywords) >= 2:
                best_match = None
                best_score = 0
                for path, file_keywords in keywords_index.items():
                    # è®¡ç®—äº¤é›†
                    common = query_keywords & file_keywords
                    score = len(common)
                    # è‡³å°‘è¦æœ‰3ä¸ªå…³é”®è¯åŒ¹é…ï¼Œæˆ–è€…åŒ¹é…ç‡è¶…è¿‡50%
                    min_len = min(len(query_keywords), len(file_keywords))
                    if score >= 3 or (min_len > 0 and score / min_len > 0.5):
                        if score > best_score:
                            best_score = score
                            best_match = path
                if best_match:
                    return best_match
    
    return None


def build_file_index(videos_dir: Path) -> Tuple[dict, dict]:
    """å»ºç«‹è§†é¢‘æ–‡ä»¶ç´¢å¼•ï¼Œè¿”å› (æ ‡å‡†åŒ–åå­—ç´¢å¼•, å…³é”®è¯ç´¢å¼•)"""
    name_index = {}
    keywords_index = {}
    
    for f in videos_dir.iterdir():
        if f.is_file() and f.suffix.lower() in ['.mp4', '.webm', '.mkv', '.avi', '.mov']:
            normalized = normalize_filename(f.name)
            name_index[normalized] = f
            keywords_index[f] = extract_keywords(f.name)
    
    return name_index, keywords_index


def extract_clip(
    input_path: Path,
    output_path: Path,
    start_time: float,
    end_time: float
) -> bool:
    """ä½¿ç”¨ ffmpeg æå–è§†é¢‘ç‰‡æ®µ"""
    if output_path.exists():
        return True
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    duration = end_time - start_time
    
    cmd = [
        "ffmpeg", "-y",
        "-ss", str(start_time),
        "-i", str(input_path),
        "-t", str(duration),
        "-c:v", "libx264",
        "-c:a", "aac",
        "-preset", "fast",
        "-loglevel", "error",
        str(output_path)
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
        return result.returncode == 0 and output_path.exists()
    except Exception as e:
        logger.warning(f"åˆ‡ç‰‡å¤±è´¥: {e}")
        return False


def process_clips(
    videos_dir: Path,
    info_csv: Path,
    metadata_json: Path,
    output_dir: Path,
    debug: bool = False,
) -> List[Dict]:
    """å¤„ç†è§†é¢‘åˆ‡ç‰‡"""
    
    # 1. è§£æ info.csv
    vid_to_filename = parse_info_csv(info_csv)
    
    # 2. åŠ è½½å…ƒæ•°æ®
    with open(metadata_json, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    logger.info(f"åŠ è½½äº† {len(metadata)} æ¡å…ƒæ•°æ®è®°å½•")
    
    # è·å–å…ƒæ•°æ®ä¸­çš„å”¯ä¸€ video ID
    metadata_vids = set(item.get('vid', '') for item in metadata if item.get('vid'))
    logger.info(f"å…ƒæ•°æ®ä¸­å”¯ä¸€è§†é¢‘ ID: {len(metadata_vids)} ä¸ª")
    
    # 3. å»ºç«‹æ–‡ä»¶ç´¢å¼•
    logger.info(f"\nğŸ“ æ‰«æè§†é¢‘ç›®å½•: {videos_dir}")
    name_index, keywords_index = build_file_index(videos_dir)
    logger.info(f"  æ‰¾åˆ° {len(name_index)} ä¸ªè§†é¢‘æ–‡ä»¶")
    
    # 4. ç»Ÿè®¡å¯ç”¨è§†é¢‘
    available_videos = {}
    missing_files = []
    for vid, filename in vid_to_filename.items():
        video_path = find_video_file(videos_dir, filename, name_index, keywords_index)
        if video_path:
            available_videos[vid] = video_path
        else:
            missing_files.append((vid, filename))
    
    logger.info(f"\ninfo.csv ä¸­çš„è§†é¢‘: {len(vid_to_filename)} ä¸ª")
    logger.info(f"  - æ‰¾åˆ°æ–‡ä»¶: {len(available_videos)} ä¸ª")
    logger.info(f"  - æ–‡ä»¶ç¼ºå¤±: {len(missing_files)} ä¸ª")
    
    # æ˜¾ç¤ºç¼ºå¤±æ–‡ä»¶çš„è¯¦æƒ…
    if debug and missing_files:
        logger.info(f"\nâš ï¸ æ— æ³•åŒ¹é…çš„æ–‡ä»¶ (å‰ 10 ä¸ª):")
        for vid, filename in missing_files[:10]:
            logger.info(f"   {vid}: {filename}")
    
    # 4. åˆ†æåŒ¹é…æƒ…å†µ
    csv_vids = set(vid_to_filename.keys())
    matched_vids = csv_vids & metadata_vids
    
    logger.info(f"\nğŸ“Š åŒ¹é…åˆ†æ:")
    logger.info(f"  - info.csv ä¸­çš„ video ID: {len(csv_vids)} ä¸ª")
    logger.info(f"  - dataset ä¸­çš„ video ID: {len(metadata_vids)} ä¸ª")
    logger.info(f"  - ä¸¤è€…äº¤é›† (å¯å¤„ç†): {len(matched_vids)} ä¸ª")
    
    if debug and len(matched_vids) < 20:
        logger.info(f"  - åŒ¹é…çš„ ID: {list(matched_vids)}")
    
    # æ˜¾ç¤ºä¸åŒ¹é…çš„åŸå› 
    unmatched_csv = csv_vids - metadata_vids
    if unmatched_csv and debug:
        logger.info(f"\nâš ï¸ info.csv ä¸­æœ‰ {len(unmatched_csv)} ä¸ªè§†é¢‘ä¸åœ¨ dataset ä¸­")
        logger.info(f"   å‰ 5 ä¸ª: {list(unmatched_csv)[:5]}")
    
    # 5. ç­›é€‰å¯å¤„ç†çš„å…ƒæ•°æ® (è§†é¢‘æ–‡ä»¶å­˜åœ¨ä¸”åœ¨å…ƒæ•°æ®ä¸­)
    processable = []
    for item in metadata:
        vid = item.get('vid', '')
        if vid in available_videos:
            processable.append({
                'vid': vid,
                'video_path': available_videos[vid],
                'transcript': item.get('transcript clip', ''),
                'begin': item.get('begin position', 0),
                'end': item.get('end position', 0),
            })
    
    logger.info(f"\nâœ… å¯å¤„ç†çš„ç‰‡æ®µ: {len(processable)} æ¡")
    
    if not processable:
        logger.warning("æ²¡æœ‰å¯å¤„ç†çš„ç‰‡æ®µ")
        return []
    
    # 5. å¤„ç†åˆ‡ç‰‡
    output_dir.mkdir(parents=True, exist_ok=True)
    clips_dir = output_dir / "clips"
    clips_dir.mkdir(exist_ok=True)
    
    results = []
    success_count = 0
    fail_count = 0
    
    for i, item in enumerate(processable):
        vid = item['vid']
        begin = item['begin']
        end = item['end']
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
        clip_name = f"{vid}_{int(begin)}_{int(end)}.mp4"
        clip_path = clips_dir / clip_name
        
        # è¿›åº¦æ˜¾ç¤º
        print(f"\rå¤„ç†ä¸­: {i+1}/{len(processable)} - {vid}", end='', flush=True)
        
        # åˆ‡ç‰‡
        if extract_clip(item['video_path'], clip_path, begin, end):
            results.append({
                'vid': vid,
                'clip_path': str(clip_path),
                'transcript': item['transcript'],
                'begin_time': begin,
                'end_time': end,
                'duration': end - begin,
            })
            success_count += 1
        else:
            fail_count += 1
    
    print()  # æ¢è¡Œ
    
    # 6. ä¿å­˜ç»“æœ
    output_json = output_dir / "text_video_pairs.json"
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # 7. ç”Ÿæˆç®€å•çš„æ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œ: clip_path<TAB>transcriptï¼‰
    output_tsv = output_dir / "text_video_pairs.tsv"
    with open(output_tsv, 'w', encoding='utf-8') as f:
        for r in results:
            # æ¸…ç† transcript ä¸­çš„æ¢è¡Œ
            text = r['transcript'].replace('\n', ' ').replace('\t', ' ')
            f.write(f"{r['clip_path']}\t{text}\n")
    
    logger.info(f"\n{'='*50}")
    logger.info(f"å¤„ç†å®Œæˆ!")
    logger.info(f"  æˆåŠŸ: {success_count}")
    logger.info(f"  å¤±è´¥: {fail_count}")
    logger.info(f"  è¾“å‡ºç›®å½•: {output_dir}")
    logger.info(f"  JSON: {output_json}")
    logger.info(f"  TSV: {output_tsv}")
    logger.info(f"{'='*50}")
    
    return results


def main():
    parser = argparse.ArgumentParser(
        description="è§†é¢‘åˆ‡ç‰‡å¤„ç†è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    python video_clip_processor.py \\
        --videos-dir ./downloaded_videos \\
        --info-csv ./info.csv \\
        --metadata ./dataset_test.json \\
        --output-dir ./output
        """
    )
    
    parser.add_argument("--videos-dir", "-v", type=Path, required=True,
                       help="å·²ä¸‹è½½è§†é¢‘çš„ç›®å½•")
    parser.add_argument("--info-csv", "-i", type=Path, required=True,
                       help="info.csv æ–‡ä»¶è·¯å¾„ (URL,filename)")
    parser.add_argument("--metadata", "-m", type=Path, required=True,
                       help="CLIP4MC å…ƒæ•°æ® JSON æ–‡ä»¶")
    parser.add_argument("--output-dir", "-o", type=Path, required=True,
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--debug", "-d", action="store_true",
                       help="æ˜¾ç¤ºè¯¦ç»†è°ƒè¯•ä¿¡æ¯")
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥
    if not args.videos_dir.exists():
        logger.error(f"è§†é¢‘ç›®å½•ä¸å­˜åœ¨: {args.videos_dir}")
        sys.exit(1)
    if not args.info_csv.exists():
        logger.error(f"info.csv ä¸å­˜åœ¨: {args.info_csv}")
        sys.exit(1)
    if not args.metadata.exists():
        logger.error(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.metadata}")
        sys.exit(1)
    
    # å¤„ç†
    process_clips(
        videos_dir=args.videos_dir,
        info_csv=args.info_csv,
        metadata_json=args.metadata,
        output_dir=args.output_dir,
        debug=args.debug,
    )


if __name__ == "__main__":
    main()

