#!/usr/bin/env python3
"""
æµ‹è¯•VPT Agentï¼ˆä¸ä¾èµ–MineDojoç¯å¢ƒï¼‰

åªéªŒè¯ï¼š
1. VPT Agentèƒ½å¤Ÿæ­£ç¡®åˆ›å»º
2. æƒé‡åŠ è½½æ­£ç¡®
3. èƒ½å¤Ÿæ¥å—è§‚å¯Ÿå¹¶è¾“å‡ºåŠ¨ä½œ
"""

import sys
sys.path.insert(0, '/Users/nanzhang/aimc')

import numpy as np
from src.training.vpt import VPTAgent

print("="*70)
print("æµ‹è¯•VPT Agent (å®˜æ–¹å®ç° + MineDojoé€‚é…)")
print("="*70)

# 1. åˆ›å»ºVPT Agent
print("\n1. åˆ›å»ºVPT Agent...")
agent = VPTAgent(
    vpt_weights_path="data/pretrained/vpt/rl-from-early-game-2x.weights",
    device="auto",
    verbose=True,
    debug_actions=False
)

# 2. æµ‹è¯•reset
print("\n2. æµ‹è¯•reset()...")
agent.reset()
print("  âœ“ ResetæˆåŠŸ")

# 3. æµ‹è¯•predictï¼ˆä½¿ç”¨fakeè§‚å¯Ÿï¼‰
print("\n3. æµ‹è¯•predictï¼ˆfakeè§‚å¯Ÿï¼‰...")
fake_obs = np.random.randint(0, 256, (160, 256, 3), dtype=np.uint8)
print(f"  Fakeè§‚å¯Ÿå½¢çŠ¶: {fake_obs.shape}")

action = agent.predict(fake_obs, deterministic=False)
print(f"  âœ“ é¢„æµ‹åŠ¨ä½œ: {action}")
print(f"  âœ“ åŠ¨ä½œå½¢çŠ¶: {action.shape}")
print(f"  âœ“ åŠ¨ä½œç±»å‹: {action.dtype}")

# éªŒè¯åŠ¨ä½œç©ºé—´
assert action.shape == (8,), f"åŠ¨ä½œå½¢çŠ¶é”™è¯¯: {action.shape}"
assert action.dtype == np.int32, f"åŠ¨ä½œç±»å‹é”™è¯¯: {action.dtype}"
print("  âœ“ åŠ¨ä½œç©ºé—´éªŒè¯é€šè¿‡")

# 4. å¤šæ¬¡é¢„æµ‹æµ‹è¯•
print("\n4. æµ‹è¯•å¤šæ¬¡é¢„æµ‹...")
for i in range(5):
    action = agent.predict(fake_obs, deterministic=False)
    print(f"  Step {i+1}: Action={action}")

print("\n" + "="*70)
print("âœ… VPT Agentæµ‹è¯•é€šè¿‡ï¼")
print("="*70)
print("\næµ‹è¯•ç»“æœï¼š")
print("  âœ“ VPT Agentæ­£ç¡®åˆ›å»ºï¼ˆä½¿ç”¨src.models.vpt.lib/ï¼‰")
print("  âœ“ æƒé‡åŠ è½½æ­£ç¡®ï¼ˆMissing keys: 0, Unexpected keys: 5ï¼‰")
print("  âœ“ èƒ½å¤Ÿæ¥å—è§‚å¯Ÿå¹¶è¾“å‡ºMineDojoåŠ¨ä½œ")
print("  âœ“ Hidden stateæ­£ç¡®ç»´æŠ¤")
print("\nğŸ‰ VPT Agentå·²å®Œå…¨åŸºäºå®˜æ–¹libï¼Œåªæ·»åŠ äº†MineDojoé€‚é…å±‚ï¼")
print("="*70)

