"""
è¯„ä¼°æ¡†æ¶ - ä»»åŠ¡ç®¡ç†ä¸è°ƒåº¦
Evaluation Framework - Task Management and Scheduling

èŒè´£:
- ç®¡ç† STEVE1Evaluator å®ä¾‹
- ä» YAML åŠ è½½ä»»åŠ¡é…ç½®
- å•/æ‰¹é‡ä»»åŠ¡è°ƒåº¦
- ä¸‰é˜¶æ®µè¯„ä¼°: PrioræŒ‡æ ‡ â†’ åŠ¨ä½œç›¸ä¼¼åº¦ â†’ Policyæ‰§è¡Œ
- ç»“æœæ”¶é›†ä¸èšåˆ
- ç”Ÿæˆç»¼åˆHTMLæŠ¥å‘Š
"""

# âš ï¸ è­¦å‘Šè¿‡æ»¤å¿…é¡»åœ¨æ‰€æœ‰å¯¼å…¥ä¹‹å‰
import warnings
import os

# å±è”½å¸¸è§çš„ç¬¬ä¸‰æ–¹åº“è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, message='.*has_cuda.*deprecated.*')
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', message='.*invalid value encountered in cast.*', category=RuntimeWarning)
warnings.filterwarnings('ignore', message='.*RequestsDependencyWarning.*')
warnings.filterwarnings('ignore', message='.*Unable to find acceptable character detection.*')

# å±è”½ Intel MKL è­¦å‘Š
os.environ['MKL_DEBUG_CPU_TYPE'] = '5'
os.environ['KMP_WARNINGS'] = '0'

import sys
import logging
import json
import yaml
import pickle
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from scipy.spatial.distance import cosine

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å±è”½ MineDojo æ—¥å¿—ï¼ˆå¿…é¡»åœ¨å¯¼å…¥ minedojo ç›¸å…³æ¨¡å—ä¹‹å‰ï¼‰
logging.getLogger('minedojo.tasks').setLevel(logging.WARNING)
logging.getLogger('minedojo').setLevel(logging.WARNING)

# å¯¼å…¥è‡ªå®šä¹‰ç¯å¢ƒï¼ˆè§¦å‘ç¯å¢ƒæ³¨å†Œï¼‰
import src.envs

from src.evaluation.policy_evaluator import STEVE1Evaluator
from src.evaluation.metrics import TaskResult, TrialResult
from src.evaluation.task_loader import TaskLoader
from src.evaluation.checkpoint import CheckpointManager, CheckpointConfig
from src.evaluation.prior_evaluator import Steve1PriorEvaluator
from src.utils.evaluation_report_generator import PriorHTMLGenerator
from src.utils.device import DEVICE

# å»¶è¿Ÿå¯¼å…¥ (é¿å…å¾ªç¯ä¾èµ–)

logger = logging.getLogger(__name__)


@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®"""
    # STEVE-1 æ¨¡å‹é…ç½®
    model_path: str = "data/weights/vpt/2x.model"
    weights_path: str = "data/weights/steve1/steve1.weights"
    prior_weights: str = "data/weights/steve1/steve1_prior.pt"
    text_cond_scale: float = 6.0
    visual_cond_scale: float = 7.0
    seed: int = 42
    enable_render: bool = False
    enable_report: bool = False
    video_size: Optional[Tuple[int, int]] = None  # è§†é¢‘å°ºå¯¸ (width, height)ï¼ŒNone è¡¨ç¤ºä¸å½•åˆ¶
    
    # è¯„ä¼°é…ç½®
    n_trials: Optional[int] = None  # None è¡¨ç¤ºä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
    max_steps: Optional[int] = None  # None è¡¨ç¤ºä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
    
    # è·¯å¾„é…ç½®
    task_config_path: str = "config/eval_tasks.yaml"
    results_dir: str = "data/evaluation"
    output_dir: Optional[str] = None  # è‡ªå®šä¹‰è¾“å‡ºç›®å½•ï¼ˆä¼˜å…ˆçº§é«˜äº results_dirï¼‰
    checkpoint_dir: Optional[str] = None  # æ£€æŸ¥ç‚¹ç›®å½•ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨è¾“å‡ºç›®å½•ä¸‹çš„checkpoints/ï¼‰
    
    # æ£€æŸ¥ç‚¹é…ç½®
    enable_checkpoint: bool = True  # å¯ç”¨æ£€æŸ¥ç‚¹
    checkpoint_save_interval: int = 5  # æ¯Nä¸ªtrialä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
    checkpoint_auto_resume: bool = True  # è‡ªåŠ¨æ¢å¤
    checkpoint_cleanup_on_complete: bool = True  # å®Œæˆåæ¸…ç†æ£€æŸ¥ç‚¹
    
    # ç¯å¢ƒé‡å»ºç­–ç•¥é…ç½®
    rebuild_interval: int = 15  # æ¯Nä¸ªtrialå®Œå…¨é‡å»ºç¯å¢ƒï¼ˆ0è¡¨ç¤ºæ¯æ¬¡é‡å»ºï¼Œ-1è¡¨ç¤ºä»ä¸é‡å»ºï¼‰
    
    # ä¸‰é˜¶æ®µè¯„ä¼°å¼€å…³
    enable_prior_eval: bool = True  # å¯ç”¨ Prior æŒ‡æ ‡è¯„ä¼°
    enable_action_similarity: bool = True  # å¯ç”¨åŠ¨ä½œç›¸ä¼¼åº¦è¯„ä¼°
    enable_policy_eval: bool = True  # å¯ç”¨ Policy æ‰§è¡Œè¯„ä¼°
    
    # Prior è¯„ä¼°é…ç½®
    prior_n_samples: int = 10  # Prior ä¸€è‡´æ€§é‡‡æ ·æ¬¡æ•°
    train_samples_dir: str = "data/train_samples"  # è®­ç»ƒæ ·æœ¬ç›®å½•


@dataclass
class TaskEvaluationResult:
    """å•ä¸ªä»»åŠ¡çš„å®Œæ•´è¯„ä¼°ç»“æœï¼ˆåŒ…å«ä¸‰é˜¶æ®µï¼‰"""
    task_id: str
    instruction: str
    language: str
    category: str
    
    # Policy æ‰§è¡Œç»“æœ
    policy_result: Optional[TaskResult] = None
    
    # Prior æŒ‡æ ‡
    prior_metrics: Dict = field(default_factory=dict)
    
    # åŠ¨ä½œç›¸ä¼¼åº¦æŒ‡æ ‡
    action_similarity_metrics: Dict = field(default_factory=dict)
    
    # ç›®æ ‡æ¥è¿‘åº¦æŒ‡æ ‡ï¼ˆä» Policy æ‰§è¡Œä¸­è·å–ï¼‰
    goal_progress_metrics: Dict = field(default_factory=dict)
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        result = {
            'task_id': self.task_id,
            'instruction': self.instruction,
            'language': self.language,
            'category': self.category,
            'prior_metrics': self.prior_metrics,
            'action_similarity_metrics': self.action_similarity_metrics,
            'goal_progress_metrics': self.goal_progress_metrics,
        }
        
        if self.policy_result:
            result['policy_result'] = {
                'success_rate': self.policy_result.success_rate,
                'avg_steps': self.policy_result.avg_steps,
                'avg_time': self.policy_result.avg_time,
                'n_trials': len(self.policy_result.trials),
            }
        
        return result


class EvaluationFramework:
    """
    è¯„ä¼°æ¡†æ¶ - ä»»åŠ¡ç®¡ç†ä¸è°ƒåº¦å™¨
    
    æ¶æ„:
        EvaluationFramework (Manager/Scheduler)
            â†“ ç®¡ç†
        STEVE1Evaluator (Worker/Executor)
            â†“ æ‰§è¡Œ
        Environment + Agent
    """
    
    def __init__(
        self,
        config: Optional[EvaluationConfig] = None,
        evaluator: Optional[STEVE1Evaluator] = None
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°æ¡†æ¶
        
        Args:
            config: è¯„ä¼°é…ç½®ï¼ˆå¦‚æœNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
            evaluator: STEVE1Evaluator å®ä¾‹ï¼ˆå¦‚æœNoneåˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
        """
        # é…ç½®æ—¥å¿—è¿‡æ»¤å™¨ï¼Œè¿‡æ»¤æ‰ä¸å¿…è¦çš„è­¦å‘Š
        self._setup_log_filters()

        logger.info(f"{'='*30}")
        logger.info(f"è°ƒåº¦å™¨åŠ è½½...")
        logger.info(f"{'='*30}")
        
        self.config = config or EvaluationConfig()
        
        # åŠ è½½ä»»åŠ¡é…ç½®
        self.task_loader = TaskLoader(self.config.task_config_path)
        logger.info(f"åŠ è½½ä»»åŠ¡é…ç½®: {self.config.task_config_path}")
        logger.info(f"  å‘ç° {len(self.task_loader.tasks)} ä¸ªä»»åŠ¡")
        
        # ç¡®å®šè¾“å‡ºç›®å½•å’Œæ£€æŸ¥ç‚¹ç›®å½•
        self.base_output_dir = Path(self.config.output_dir) if self.config.output_dir else Path(self.config.results_dir)
        
        # æ£€æŸ¥ç‚¹ç®¡ç†å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œåœ¨evaluate_task_setä¸­æ ¹æ®ä»»åŠ¡è¾“å‡ºç›®å½•åˆ›å»ºï¼‰
        self.checkpoint_manager = None
        self.checkpoint_config = CheckpointConfig(
            enabled=self.config.enable_checkpoint,
            save_interval=self.config.checkpoint_save_interval,
            auto_resume=self.config.checkpoint_auto_resume,
            cleanup_on_complete=self.config.checkpoint_cleanup_on_complete
        )
        if self.config.enable_checkpoint:
            logger.info(f"æ£€æŸ¥ç‚¹åŠŸèƒ½å·²å¯ç”¨")
            logger.info(f"  ä¿å­˜é—´éš”: æ¯{self.checkpoint_config.save_interval}ä¸ªtrial")
            logger.info(f"  è‡ªåŠ¨æ¢å¤: {'æ˜¯' if self.checkpoint_config.auto_resume else 'å¦'}")
        
        # ç¯å¢ƒé‡å»ºç­–ç•¥
        if self.config.rebuild_interval == 0:
            logger.info(f"ç¯å¢ƒé‡å»ºç­–ç•¥: æ¯æ¬¡trialéƒ½é‡å»ºï¼ˆæœ€ç¨³å®šï¼Œæœ€æ…¢ï¼‰")
        elif self.config.rebuild_interval > 0:
            logger.info(f"ç¯å¢ƒé‡å»ºç­–ç•¥: æ¯{self.config.rebuild_interval}æ¬¡trialé‡å»ºï¼ˆæ¨èï¼‰")
        else:
            logger.info(f"ç¯å¢ƒé‡å»ºç­–ç•¥: ä»ä¸é‡å»ºï¼Œåªè½»é‡æ¸…ç†ï¼ˆæœ€å¿«ï¼Œå¯èƒ½ä¸ç¨³å®šï¼‰")

        # ä¿ç•™ evaluator å‚æ•°ç”¨äºå‘åå…¼å®¹
        self.evaluator = evaluator  # é€šå¸¸ä¸º None
        if self.evaluator:
            logger.info("ä½¿ç”¨æä¾›çš„è¯„ä¼°å™¨å®ä¾‹")
        
        # å…±äº«çš„ä»»åŠ¡è¯„ä¼°å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œæ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼‰
        self._shared_evaluator: Optional[STEVE1Evaluator] = None
        
        # ç»“æœå­˜å‚¨
        self.results: List[TaskResult] = []
        
        # Task-set ç›®å½•ï¼ˆç”¨äºæ‰¹é‡è¯„ä¼°æ—¶ç»„ç»‡ç»“æœï¼‰
        self.current_task_set_dir: Optional[Path] = None
        
        # Prior è¯„ä¼°å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._prior_evaluator: Optional[Steve1PriorEvaluator] = None
        
        #logger.info("è¯„ä¼°æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_log_filters(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿï¼šæ ¼å¼åŒ–ã€è¿‡æ»¤å™¨ç­‰"""
        import warnings
        from src.utils.logging_config import setup_evaluation_logging
        
        # é…ç½®ç»Ÿä¸€çš„æ—¥å¿—æ ¼å¼å’Œè¿‡æ»¤å™¨ï¼ˆç¼©çŸ­æ¨¡å—åã€è¿‡æ»¤ä¸éœ€è¦çš„æ—¥å¿—ï¼‰
        setup_evaluation_logging()
        
        # 1. è¿‡æ»¤ PyTorch çš„ UserWarningï¼ˆå¦‚ autocast è­¦å‘Šï¼‰
        warnings.filterwarnings('ignore', category=UserWarning, module='torch')
        warnings.filterwarnings('ignore', message='.*CUDA is not available.*')
        warnings.filterwarnings('ignore', message='.*Implicit dimension choice for softmax.*')
        
        # 2. å®Œå…¨é™é»˜ MineRL/Malmo æ—¥å¿—ï¼ˆåŒ…æ‹¬ ERRORï¼‰
        minerl_loggers = [
            'minerl.env.malmo.instance',
            'minerl.env._multiagent',
            'minerl.env.malmo',
            'process_watcher',
        ]
        for logger_name in minerl_loggers:
            minerl_logger = logging.getLogger(logger_name)
            minerl_logger.setLevel(logging.CRITICAL + 1)  # å®Œå…¨é™é»˜
            minerl_logger.propagate = False  # ä¸ä¼ æ’­åˆ°çˆ¶ logger
        
        # 3. è¿‡æ»¤ STEVE-1 çš„ UserWarning
        warnings.filterwarnings('ignore', category=UserWarning, module='steve1')
        
        # 4. è¿‡æ»¤ NumPy å’Œ MineRL çš„ RuntimeWarning
        warnings.filterwarnings('ignore', message='.*invalid value encountered in cast.*', category=RuntimeWarning)
        warnings.filterwarnings('ignore', message='.*minerl.utils.process_watcher.*found in sys.modules.*', category=RuntimeWarning)
        warnings.filterwarnings('ignore', category=RuntimeWarning, module='runpy')
        
        logger.debug("æ—¥å¿—ç³»ç»Ÿå·²é…ç½®ï¼šç¼©çŸ­æ¨¡å—åã€è¿‡æ»¤ä¸éœ€è¦çš„æ—¥å¿—")
    
    def _get_prior_evaluator(self) -> Steve1PriorEvaluator:
        """è·å– Prior è¯„ä¼°å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self._prior_evaluator is None:
            #logger.info("åˆå§‹åŒ– Prior è¯„ä¼°å™¨...")
            
            # ä»ç›®å½•ç»“æ„åŠ è½½è®­ç»ƒæ ·æœ¬åµŒå…¥
            train_samples_dict = self._load_train_samples(
                self.config.train_samples_dir
            )
            
            # ä»ä»»åŠ¡é…ç½®ä¸­æå–æŒ‡ä»¤å˜ä½“
            instruction_variants_dict = self._load_instruction_variants_from_config()
            
            # åˆ›å»º Prior è¯„ä¼°å™¨ï¼ˆä¸ä¼ è·¯å¾„ï¼Œæ‰‹åŠ¨è®¾ç½®æ•°æ®ï¼‰
            self._prior_evaluator = Steve1PriorEvaluator(
                prior_weights=self.config.prior_weights,
                success_visuals_path=None,  # ä¸ä¼ è·¯å¾„
                seed=self.config.seed
            )
            
            # æ‰‹åŠ¨è®¾ç½®å·²åŠ è½½çš„è®­ç»ƒæ ·æœ¬æ•°æ®
            self._prior_evaluator.success_visuals = train_samples_dict
            # if success_visuals_dict:
            #     logger.info(f"âœ“ å·²åŠ è½½ {len(success_visuals_dict)} ä¸ªä»»åŠ¡çš„æˆåŠŸç”»é¢")
            
            # æ‰‹åŠ¨è®¾ç½®æŒ‡ä»¤å˜ä½“æ•°æ®
            self._prior_evaluator.instruction_variants = instruction_variants_dict
            # if instruction_variants_dict:
            #     logger.info(f"âœ“ å·²åŠ è½½ {len(instruction_variants_dict)} ä¸ªä»»åŠ¡çš„æŒ‡ä»¤å˜ä½“")
            #     # æ‰“å°åŠ è½½çš„ä»»åŠ¡IDç”¨äºè°ƒè¯•
                #logger.debug(f"    å˜ä½“ä»»åŠ¡: {list(instruction_variants_dict.keys())[:5]}...")
            
        return self._prior_evaluator
    
    def _get_shared_evaluator(self) -> STEVE1Evaluator:
        """
        è·å–å…±äº«çš„ä»»åŠ¡è¯„ä¼°å™¨ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œæ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼‰
        
        æ¨¡å‹ï¼ˆVPTã€STEVE-1ã€MineCLIPã€Priorï¼‰åªåœ¨é¦–æ¬¡è°ƒç”¨æ—¶åŠ è½½ï¼Œ
        åç»­è°ƒç”¨å¤ç”¨åŒä¸€å®ä¾‹ï¼Œåªéœ€æ›´æ–°ç¯å¢ƒé…ç½®ã€‚
        """
        if self._shared_evaluator is None:
            logger.info("åˆå§‹åŒ–å…±äº«è¯„ä¼°å™¨ï¼ˆæ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼‰...")
            self._shared_evaluator = STEVE1Evaluator(
                model_path=self.config.model_path,
                weights_path=self.config.weights_path,
                prior_weights=self.config.prior_weights,
                text_cond_scale=self.config.text_cond_scale,
                visual_cond_scale=self.config.visual_cond_scale,
                seed=self.config.seed,
                env_name=None,  # ç¯å¢ƒç¨åæŒ‰ä»»åŠ¡é…ç½®
                env_config=None,
                rebuild_interval=self.config.rebuild_interval,
                checkpoint_manager=self.checkpoint_manager,
                checkpoint_config=self.checkpoint_config
            )
            # é¢„åŠ è½½æ¨¡å‹ï¼ˆä¸åŠ è½½ç¯å¢ƒï¼‰
            self._shared_evaluator._load_models()
            logger.info("âœ“ å…±äº«è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼ˆæ¨¡å‹å·²åŠ è½½ï¼‰")
        
        return self._shared_evaluator
    
    def _load_instruction_variants_from_config(self) -> Dict:
        """
        ä»ä»»åŠ¡é…ç½®ä¸­æå–æŒ‡ä»¤å˜ä½“æ•°æ®
        
        å°† YAML ä¸­çš„åµŒå¥—æ ¼å¼è½¬æ¢ä¸º Steve1PriorEvaluator æœŸæœ›çš„æ ¼å¼:
        {task_id: {'variants': [list of variant strings]}}
        
        Returns:
            Dict[task_id, {'variants': List[str]}]
        """
        instruction_variants = {}
        
        # éå†æ‰€æœ‰ä»»åŠ¡
        for task_id in self.task_loader.tasks.keys():
            task_config = self.task_loader.get_task(task_id)
            if not task_config:
                continue
            
            variants_config = task_config.get('instruction_variants', {})
            if not variants_config:
                logger.debug(f"  ä»»åŠ¡ {task_id} æ²¡æœ‰ instruction_variants é…ç½®")
                continue
            
            # ä»åµŒå¥—ç»“æ„ä¸­æå–æ‰€æœ‰å˜ä½“å­—ç¬¦ä¸²
            all_variants = []
            
            # instruction_variants æ˜¯ä¸€ä¸ªåµŒå¥—ç»“æ„:
            # simple_direct:
            #   name: "..."
            #   variants:
            #     - "variant 1"
            #     - "variant 2"
            for category_id, category_data in variants_config.items():
                if isinstance(category_data, dict):
                    category_variants = category_data.get('variants', [])
                    if isinstance(category_variants, list):
                        all_variants.extend(category_variants)
                elif isinstance(category_data, list):
                    # å…¼å®¹ç®€å•åˆ—è¡¨æ ¼å¼
                    all_variants.extend(category_data)
            
            if all_variants:
                instruction_variants[task_id] = {
                    'variants': all_variants
                }
                logger.debug(f"  åŠ è½½ {task_id} çš„ {len(all_variants)} ä¸ªæŒ‡ä»¤å˜ä½“")
        
        return instruction_variants
    
    def _load_train_samples(self, base_dir: str) -> Dict:
        """
        ä» train_samples ç›®å½•åŠ è½½è®­ç»ƒæ ·æœ¬çš„è§†è§‰åµŒå…¥
        
        ç›®å½•ç»“æ„: {base_dir}/{task_id}/trial{N}/visual_embeds.pkl
        
        Args:
            base_dir: è®­ç»ƒæ ·æœ¬ç›®å½•ï¼ˆé»˜è®¤ data/train_samplesï¼‰
            
        Returns:
            Dict[task_id, {'success_visual_embeds': List[np.ndarray]}]
        """
        train_samples = {}
        
        samples_dir = Path(base_dir)
        if not samples_dir.exists():
            logger.warning(f"è®­ç»ƒæ ·æœ¬ç›®å½•ä¸å­˜åœ¨: {base_dir}")
            return {}
        
        for task_dir in samples_dir.iterdir():
            if not task_dir.is_dir():
                continue
            
            task_id = task_dir.name
            embeds = []
            
            # æŸ¥æ‰¾æ‰€æœ‰ trial ç›®å½•ä¸­çš„ visual_embeds.pkl
            trial_dirs = sorted([d for d in task_dir.iterdir() if d.is_dir() and d.name.startswith('trial')])
            
            for trial_dir in trial_dirs:
                embed_file = trial_dir / 'visual_embeds.pkl'
                if embed_file.exists():
                    try:
                        with open(embed_file, 'rb') as f:
                            embed = pickle.load(f)
                        if hasattr(embed, 'numpy'):
                            embed = embed.numpy()
                        embeds.append(np.squeeze(embed))
                    except Exception as e:
                        logger.debug(f"    åŠ è½½å¤±è´¥ {embed_file}: {e}")
            
            if embeds:
                train_samples[task_id] = {
                    'success_visual_embeds': embeds
                }
                logger.debug(f"  åŠ è½½ {task_id}: {len(embeds)} ä¸ªåµŒå…¥")
        
        return train_samples
    
    def _compute_prior_metrics(
        self,
        task_id: str,
        instruction: str,
        task_config: Dict
    ) -> Dict:
        """
        è®¡ç®— Prior ç›¸å…³æŒ‡æ ‡
        
        Flow 1: Prior æ¨¡å‹è¯„ä¼°
        - ç›®æ ‡å‡†ç¡®æ€§ï¼ˆPriorè¾“å‡º vs æˆåŠŸç”»é¢ï¼‰
        - ä¸€è‡´æ€§ï¼ˆå¤šæ¬¡é‡‡æ ·ç¨³å®šæ€§ï¼‰
        - è¯­ä¹‰é²æ£’æ€§ï¼ˆæŒ‡ä»¤å˜ä½“ï¼‰
        
        Args:
            task_id: ä»»åŠ¡ID
            instruction: æŒ‡ä»¤æ–‡æœ¬
            task_config: ä»»åŠ¡é…ç½®
            
        Returns:
            Dict containing prior metrics
        """
        if not self.config.enable_prior_eval:
            return {'enabled': False}
        
        try:
            prior_evaluator = self._get_prior_evaluator()
            
            # è®¡ç®—ç›®æ ‡å‡†ç¡®æ€§
            goal_accuracy, goal_accuracy_std, n_visuals = prior_evaluator.compute_goal_accuracy(
                task_id=task_id,
                instruction=instruction
            )
            
            # è®¡ç®— MineCLIP åŸºçº¿ï¼ˆç›´æ¥æ–‡æœ¬ç¼–ç  vs æˆåŠŸç”»é¢ï¼‰
            mineclip_baseline = self._compute_mineclip_baseline(
                prior_evaluator, task_id, instruction
            )
            
            # è®¡ç®—ä¸€è‡´æ€§
            consistency = prior_evaluator.compute_consistency(
                instruction=instruction,
                n_samples=self.config.prior_n_samples
            )
            
            # è®¡ç®—è¯­ä¹‰é²æ£’æ€§ï¼ˆå¦‚æœ prior_evaluator ä¸­æœ‰è¯¥ä»»åŠ¡çš„å˜ä½“ï¼‰
            semantic_robustness = None
            n_variants = 0
            # æ£€æŸ¥ prior_evaluator.instruction_variants ä¸­æ˜¯å¦æœ‰è¯¥ä»»åŠ¡
            if task_id in prior_evaluator.instruction_variants:
                robustness_result = prior_evaluator.compute_semantic_robustness(task_id)
                if robustness_result[0] is not None:
                    semantic_robustness, n_variants = robustness_result
            
            return {
                'enabled': True,
                'goal_accuracy': goal_accuracy,
                'goal_accuracy_std': goal_accuracy_std,
                'mineclip_baseline': mineclip_baseline,
                'consistency': consistency,
                'semantic_robustness': semantic_robustness,
                'n_success_visuals': n_visuals,
                'n_variants': n_variants
            }
            
        except Exception as e:
            logger.warning(f"Prior è¯„ä¼°å¤±è´¥: {e}")
            return {'enabled': False, 'error': str(e)}
    
    def _compute_mineclip_baseline(
        self,
        prior_evaluator,
        task_id: str,
        instruction: str
    ) -> float:
        """
        è®¡ç®— MineCLIP åŸºçº¿ï¼ˆç›´æ¥æ–‡æœ¬ç¼–ç  vs æˆåŠŸç”»é¢ï¼‰
        
        è¿™æ˜¯ä¸€ä¸ªæ¶ˆèå¯¹æ¯”ï¼šMineCLIP ç›´æ¥ç¼–ç æ–‡æœ¬çš„ç»“æœ
        ä¸ Prior è¾“å‡ºè¿›è¡Œå¯¹æ¯”ï¼Œè¯„ä¼° Prior å¸¦æ¥çš„å¢ç›Š
        
        Args:
            prior_evaluator: Prior è¯„ä¼°å™¨ï¼ˆåŒ…å« mineclip å’Œ success_visualsï¼‰
            task_id: ä»»åŠ¡ID
            instruction: æŒ‡ä»¤æ–‡æœ¬
            
        Returns:
            MineCLIP åŸºçº¿ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
        """
        import torch as th
        from scipy.spatial.distance import cosine
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸç”»é¢
        if task_id not in prior_evaluator.success_visuals:
            return 0.0
        
        try:
            # è·å–æˆåŠŸç”»é¢åµŒå…¥
            success_visual_embeds = prior_evaluator.success_visuals[task_id]['success_visual_embeds']
            
            # ä½¿ç”¨ MineCLIP ç›´æ¥ç¼–ç æ–‡æœ¬
            with th.no_grad():
                z_text = prior_evaluator._mineclip.encode_text([instruction])[0].cpu().numpy()
            
            # è®¡ç®—ä¸æ¯ä¸ªæˆåŠŸç”»é¢çš„ç›¸ä¼¼åº¦
            similarities = []
            for z_visual in success_visual_embeds:
                z_visual = np.squeeze(z_visual)
                sim = 1 - cosine(z_text, z_visual)
                similarities.append(sim)
            
            return float(np.mean(similarities))
            
        except Exception as e:
            logger.warning(f"MineCLIP åŸºçº¿è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _load_trial_samples(self, task_samples_dir: Path) -> Dict:
        """
        åŠ è½½ä»»åŠ¡çš„è®­ç»ƒæ ·æœ¬æ•°æ®
        
        ç›®å½•ç»“æ„: train_samples/{task_id}/trial{N}/
            - frames/*.png
            - actions.json
            - visual_embeds.pkl
        """
        import cv2
        
        trials_data = {}
        
        # æŸ¥æ‰¾æ‰€æœ‰ trial ç›®å½•
        trial_dirs = sorted([d for d in task_samples_dir.iterdir() if d.is_dir() and d.name.startswith('trial')])
        
        for trial_dir in trial_dirs:
            trial_id = trial_dir.name
            frames = []
            
            # æ ‡å‡†ç»“æ„: trial{N}/frames/*.png
            frames_dir = trial_dir / 'frames'
            if frames_dir.exists():
                frame_files = sorted(frames_dir.glob('*.png'))
                for frame_file in frame_files:
                    frame = cv2.imread(str(frame_file))
                    if frame is not None:
                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                        frames.append(frame)
            
            # åŠ è½½åŠ¨ä½œï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            actions = []
            actions_file = trial_dir / 'actions.json'
            if actions_file.exists():
                with open(actions_file, 'r') as f:
                    actions = json.load(f)
            
            # åŠ è½½ç›®æ ‡åµŒå…¥
            goal_embed = None
            embed_file = trial_dir / 'visual_embeds.pkl'
            if embed_file.exists():
                with open(embed_file, 'rb') as f:
                    goal_embed = pickle.load(f)
            
            # åªæœ‰æœ‰å¸§æ•°æ®æ—¶æ‰æ·»åŠ 
            if frames:
                trials_data[trial_id] = {
                    'frames': frames,
                    'actions': actions,
                    'goal_embed': goal_embed
                }
        
        return trials_data
    
    def evaluate_single_task(
        self,
        task_id: str,
        n_trials: Optional[int] = None,
        max_steps: Optional[int] = None,
        parent_dir: Optional[Path] = None,  # çˆ¶ç›®å½•ï¼ˆç”¨äº task-setï¼‰
        task_index: Optional[int] = None,  # ä»»åŠ¡ç´¢å¼•ï¼ˆç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰
        total_tasks: Optional[int] = None,  # æ€»ä»»åŠ¡æ•°ï¼ˆç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰
    ) -> Tuple[TaskResult, Optional[Path]]:
        """
        è¯„ä¼°å•ä¸ªä»»åŠ¡
        
        Args:
            task_id: ä»»åŠ¡ID
            n_trials: è¯•éªŒæ¬¡æ•°ï¼ˆå¦‚æœNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼‰
            max_steps: æœ€å¤§æ­¥æ•°ï¼ˆå¦‚æœNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼‰
            parent_dir: çˆ¶ç›®å½•ï¼ˆå¦‚æœæä¾›ï¼Œä»»åŠ¡ç›®å½•å°†åˆ›å»ºåœ¨è¿™ä¸ªç›®å½•ä¸‹ï¼‰
        
        Returns:
            Tuple[TaskResult, Optional[Path]]: ä»»åŠ¡ç»“æœ + è¾“å‡ºç›®å½•è·¯å¾„
        """
        # ä»é…ç½®åŠ è½½ä»»åŠ¡
        task_config = self.task_loader.get_task(task_id)
        if not task_config:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
        
        # ç¡®å®šå‚æ•°ï¼ˆä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œå‚æ•° > ä»»åŠ¡é…ç½® > é»˜è®¤å€¼ï¼‰
        # 
        # n_trials ä¼˜å…ˆçº§:
        # 1. å‡½æ•°å‚æ•°ï¼ˆé€šå¸¸æ¥è‡ªå‘½ä»¤è¡Œ --n-trialsï¼Œå¦‚æœç”¨æˆ·æ˜¾å¼æŒ‡å®šäº†ï¼‰
        # 2. å…¨å±€é…ç½®ï¼ˆæ¥è‡ª self.config.n_trialsï¼Œå¦‚æœä¸ä¸º Noneï¼‰
        # 3. ä»»åŠ¡é…ç½®ä¸­çš„ n_trials
        # 4. é»˜è®¤å€¼ 3
        if n_trials is not None:
            pass  # ä½¿ç”¨å‡½æ•°å‚æ•°ï¼ˆå‘½ä»¤è¡Œæ˜¾å¼æŒ‡å®šï¼‰
        elif self.config.n_trials is not None:
            n_trials = self.config.n_trials  # ä½¿ç”¨å…¨å±€é…ç½®ï¼ˆå‘½ä»¤è¡Œå‚æ•°ï¼‰
        else:
            n_trials = task_config.get('n_trials', 3)  # ä½¿ç”¨ä»»åŠ¡é…ç½®æˆ–é»˜è®¤å€¼
        
        # max_steps ä¼˜å…ˆçº§:
        # 1. å‡½æ•°å‚æ•°ï¼ˆé€šå¸¸æ¥è‡ªå‘½ä»¤è¡Œ --max-stepsï¼Œå¦‚æœç”¨æˆ·æ˜¾å¼æŒ‡å®šäº†ï¼‰
        # 2. å…¨å±€é…ç½®ï¼ˆæ¥è‡ª self.config.max_stepsï¼Œå¦‚æœä¸ä¸º Noneï¼‰
        # 3. ä»»åŠ¡é…ç½®ä¸­çš„ max_steps
        # 4. é»˜è®¤å€¼ 2000
        if max_steps is not None:
            pass  # ä½¿ç”¨å‡½æ•°å‚æ•°ï¼ˆå‘½ä»¤è¡Œæ˜¾å¼æŒ‡å®šï¼‰
        elif self.config.max_steps is not None:
            max_steps = self.config.max_steps  # ä½¿ç”¨å…¨å±€é…ç½®ï¼ˆå‘½ä»¤è¡Œå‚æ•°ï¼‰
        else:
            max_steps = task_config.get('max_steps', 2000)  # ä½¿ç”¨ä»»åŠ¡é…ç½®æˆ–é»˜è®¤å€¼
        
        # ç¡®å®šæŒ‡ä»¤å’Œè¯­è¨€
        instruction = None
        language = "en"
        
        if 'en_instruction' in task_config:
            instruction = task_config['en_instruction']
            language = "en"
        elif 'zh_instruction' in task_config:
            instruction = task_config['zh_instruction']
            language = "zh"
        
        # ä»ä»»åŠ¡é…ç½®è¯»å–ç¯å¢ƒé…ç½®ï¼ˆåŒ…æ‹¬å¥–åŠ±é…ç½®ï¼‰
        env_config = task_config.get('env_config', {}).copy()  # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…ä¿®æ”¹åŸé…ç½®
        env_name = task_config.get('env_name', 'MineRLHarvestEnv-v0')
        
        # å°† max_steps æ·»åŠ åˆ° env_config ä¸­ï¼ˆä½œä¸º max_episode_stepsï¼‰
        env_config['max_episode_steps'] = max_steps
        
        # ä»å…¨å±€é…ç½®è¯»å– image_sizeï¼ˆå¦‚æœä»»åŠ¡é…ç½®ä¸­æ²¡æœ‰æŒ‡å®šï¼‰
        if 'image_size' not in env_config:
            global_config = self.task_loader.config.get('evaluation', {})
            global_image_size = global_config.get('image_size')
            if global_image_size:
                # è½¬æ¢ä¸º tuple æ ¼å¼ (height, width)
                if isinstance(global_image_size, list) and len(global_image_size) == 2:
                    env_config['image_size'] = tuple(global_image_size)
                    #logger.info(f"ä½¿ç”¨å…¨å±€ image_size: {env_config['image_size']}")
                else:
                    env_config['image_size'] = global_image_size
                    #logger.info(f"ä½¿ç”¨å…¨å±€ image_size: {env_config['image_size']}")
        
        # è·å–åŠ¨ä½œåºåˆ—æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        replay_actions_file = task_config.get('replay_actions_file', None)
        if replay_actions_file:
            logger.info(f"  æ£€æµ‹åˆ°åŠ¨ä½œåºåˆ—æ–‡ä»¶: {replay_actions_file}")
        
        # è·å–å…±äº«çš„è¯„ä¼°å™¨ï¼ˆæ¨¡å‹åªåŠ è½½ä¸€æ¬¡ï¼‰
        task_evaluator = self._get_shared_evaluator()
        
        # æ›´æ–°è¯„ä¼°å™¨çš„ä»»åŠ¡ç›¸å…³é…ç½®ï¼ˆç¯å¢ƒé…ç½®ã€æ¸²æŸ“è®¾ç½®ç­‰ï¼‰
        task_evaluator.enable_render = self.config.enable_render
        task_evaluator.video_size = self.config.video_size
        task_evaluator.enable_report = self.config.enable_report
        task_evaluator.replay_actions_file = replay_actions_file
        task_evaluator.checkpoint_manager = self.checkpoint_manager
        task_evaluator.checkpoint_config = self.checkpoint_config
        
        logger.info(f"{'='*30}")
        logger.info(f"æ‰§è¡Œä»»åŠ¡: {task_id}")
        logger.info(f"{'='*30}")
        logger.info(f"æè¿°: {task_config.get('description', 'N/A')}")
        logger.info(f"ç±»åˆ«: {task_config.get('category', 'N/A')}")
        logger.info(f"éš¾åº¦: {task_config.get('difficulty', 'N/A')}")
        logger.info(f"æŒ‡ä»¤: {instruction}")
        logger.info(f"è¯­è¨€: {language}")
        logger.info(f"è¯•éªŒæ¬¡æ•°: {n_trials}")
        logger.info(f"æœ€å¤§æ­¥æ•°: {max_steps}")
        if env_config.get('specified_biome'):
            logger.info(f"æŒ‡å®šBiome: {env_config.get('specified_biome')}")
        if replay_actions_file:
            logger.info(f"å›æ”¾æ¨¡å¼: {replay_actions_file}")
        
        # åˆ›å»ºä»»åŠ¡è¾“å‡ºç›®å½•ï¼ˆæ€»æ˜¯åˆ›å»ºï¼Œä¸ç®¡æ˜¯å¦ä¿å­˜è§†é¢‘ï¼‰
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        dir_name = f"{task_id}_{language}_{timestamp}"
        
        # å¦‚æœæä¾›äº†çˆ¶ç›®å½•ï¼Œåœ¨çˆ¶ç›®å½•ä¸‹åˆ›å»ºä»»åŠ¡ç›®å½•
        if parent_dir:
            output_dir = parent_dir / dir_name
        else:
            # å•ç‹¬ä»»åŠ¡è¯„ä¼°ï¼šä½¿ç”¨ base_output_dir
            output_dir = self.base_output_dir / dir_name
            
            # ä¸ºå•ç‹¬ä»»åŠ¡åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
            if not self.checkpoint_manager and self.checkpoint_config.enabled:
                checkpoint_dir = output_dir / "checkpoints"
                checkpoint_dir.mkdir(parents=True, exist_ok=True)
                self.checkpoint_manager = CheckpointManager(checkpoint_dir)
                logger.info(f"æ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_dir}")
        
        output_dir.mkdir(parents=True, exist_ok=True)
        #logger.info(f"  ç»“æœç›®å½•: {output_dir}")
        
        # ========== ä¸‰é˜¶æ®µè¯„ä¼° ==========
        prior_metrics = {}
        action_similarity_metrics = {}
        goal_progress_metrics = {}
        
        try:
            # === Flow 1: Prior è¯„ä¼° (ç¦»çº¿) ===
            if self.config.enable_prior_eval and instruction:
                logger.info(f"{'-'*10}")
                logger.info(f"Prior è¯„ä¼°")
                logger.info(f"{'-'*10}")
                prior_metrics = self._compute_prior_metrics(task_id, instruction, task_config)
                if prior_metrics.get('enabled'):
                    logger.info(f"ç›®æ ‡å‡†ç¡®æ€§: {prior_metrics.get('goal_accuracy', 0):.4f}")
                    logger.info(f"ä¸€è‡´æ€§: {prior_metrics.get('consistency', 0):.4f}")
                else:
                    logger.info(f"è·³è¿‡ Prior è¯„ä¼°: {prior_metrics.get('error', 'disabled')}")
            
            # === Flow 2: åŠ¨ä½œç›¸ä¼¼åº¦è¯„ä¼° (å§”æ‰˜ç»™ STEVE1Evaluator) ===
            if self.config.enable_action_similarity:
                logger.info(f"{'-'*10}")
                logger.info(f"åŠ¨ä½œç›¸ä¼¼åº¦è¯„ä¼°")
                logger.info(f"{'-'*10}")
                # è·å–ç›®æ ‡åµŒå…¥
                goal_embed = self._get_goal_embed_for_task(task_id, task_config)
                if goal_embed is not None:
                    # ä½¿ç”¨ task_evaluator è®¡ç®—ï¼ˆå¤ç”¨å·²åŠ è½½çš„æ¨¡å‹ï¼‰
                    samples_dir = Path(task_config.get('train_samples_dir', self.config.train_samples_dir))
                    action_similarity_metrics = task_evaluator.evaluate_expert_data(
                        task_id=task_id,
                        samples_dir=samples_dir,
                        goal_embed=goal_embed
                    )
                    if action_similarity_metrics.get('enabled'):
                        logger.info(f"åŠ¨ä½œç›¸ä¼¼åº¦: {action_similarity_metrics.get('action_similarity', 0):.4f}")
                        logger.info(f"ä¸“å®¶è¿›åº¦ç‡: {action_similarity_metrics.get('expert_progress_rate', 0):+.2%}")
                    else:
                        logger.info(f"è·³è¿‡åŠ¨ä½œç›¸ä¼¼åº¦: {action_similarity_metrics.get('error', 'disabled')}")
                else:
                    logger.info(f"è·³è¿‡åŠ¨ä½œç›¸ä¼¼åº¦: æ— ç›®æ ‡åµŒå…¥")
                    action_similarity_metrics = {'enabled': False, 'error': 'No goal embedding'}
            
            # === Flow 3: Policy æ‰§è¡Œè¯„ä¼° (åœ¨çº¿) ===
            policy_result = None
            if self.config.enable_policy_eval:
                logger.info(f"{'-'*10}")
                logger.info(f"Policy è¯„ä¼°")
                logger.info(f"{'-'*10}")
                policy_result = task_evaluator.evaluate_task(
                    task_id=task_id,
                    language=language,
                    n_trials=n_trials,
                    max_steps=max_steps,
                    instruction=instruction,
                    output_dir=output_dir,
                    task_index=task_index,
                    total_tasks=total_tasks,
                    env_name=env_name,      # ä¼ é€’ç¯å¢ƒåç§°
                    env_config=env_config,  # ä¼ é€’ç¯å¢ƒé…ç½®
                )
                
                # ä» Policy æ‰§è¡Œç»“æœä¸­æå–ç›®æ ‡æ¥è¿‘åº¦æŒ‡æ ‡
                if policy_result and policy_result.trials:
                    # èšåˆæ‰€æœ‰ trial çš„ç›®æ ‡æ¥è¿‘åº¦
                    all_progress_rates = []
                    all_monotonic_rates = []
                    all_initial_distances = []
                    all_final_distances = []
                    
                    for trial in policy_result.trials:
                        if hasattr(trial, 'goal_progress_rate'):
                            all_progress_rates.append(trial.goal_progress_rate)
                            all_monotonic_rates.append(trial.goal_monotonic_rate)
                            all_initial_distances.append(trial.goal_initial_distance)
                            all_final_distances.append(trial.goal_final_distance)
                    
                    if all_progress_rates:
                        goal_progress_metrics = {
                            'enabled': True,
                            'model_progress_rate': float(np.mean(all_progress_rates)),
                            'model_monotonic_rate': float(np.mean(all_monotonic_rates)),
                            'model_initial_distance': float(np.mean(all_initial_distances)),
                            'model_final_distance': float(np.mean(all_final_distances)),
                            'n_trials': len(all_progress_rates)
                        }
                        logger.info(f"æ¨¡å‹è¿›åº¦ç‡: {goal_progress_metrics['model_progress_rate']:+.2%}")
                        logger.info(f"æ¨¡å‹å•è°ƒç‡: {goal_progress_metrics['model_monotonic_rate']:.2%}")
                    else:
                        goal_progress_metrics = {'enabled': False, 'error': 'No goal progress data in trials'}
            
            # ä½¿ç”¨ policy_result ä½œä¸ºä¸»è¦ç»“æœï¼ˆå‘åå…¼å®¹ï¼‰
            result = policy_result or TaskResult(
                task_id=task_id,
                language=language,
                instruction=instruction or "",
                success_rate=0.0,
                avg_steps=0,
                avg_time=0.0,
                trials=[]
            )
            
            # åˆ›å»ºç»¼åˆè¯„ä¼°ç»“æœï¼ˆç”¨äº HTML æŠ¥å‘Šï¼‰
            combined_result = TaskEvaluationResult(
                task_id=task_id,
                instruction=instruction or "",
                language=language,
                category=task_config.get('category', 'unknown'),
                policy_result=result,
                prior_metrics=prior_metrics,
                action_similarity_metrics=action_similarity_metrics,
                goal_progress_metrics=goal_progress_metrics
            )
            
            # ä¿å­˜ä»»åŠ¡ç»“æœåˆ°ç›®å½•
            self._save_task_results(result, output_dir)
            
            # ä¿å­˜ç»¼åˆè¯„ä¼°ç»“æœ
            self._save_combined_results(combined_result, output_dir)
            
            # è¾“å‡ºè¯„ä¼°æ±‡æ€»è¡¨æ ¼
            self._print_task_summary_table(
                task_id=task_id,
                result=result,
                prior_metrics=prior_metrics,
                action_similarity_metrics=action_similarity_metrics,
                goal_progress_metrics=goal_progress_metrics
            )
            
            # ä¿å­˜ç»“æœ
            self.results.append(result)
            
            return result, output_dir
        finally:
            # åªæ¸…ç†ç¯å¢ƒï¼Œä¸å…³é—­æ•´ä¸ªè¯„ä¼°å™¨ï¼ˆä¿ç•™æ¨¡å‹ä»¥ä¾¿å¤ç”¨ï¼‰
            logger.info(f"æ¸…ç†ä»»åŠ¡ç¯å¢ƒèµ„æº...")
            task_evaluator.cleanup_env_only()
            logger.info(f"  âœ“ ç¯å¢ƒèµ„æºå·²é‡Šæ”¾")
    
    def _print_task_summary_table(
        self,
        task_id: str,
        result: TaskResult,
        prior_metrics: Dict,
        action_similarity_metrics: Dict,
        goal_progress_metrics: Dict
    ):
        """
        è¾“å‡ºä»»åŠ¡è¯„ä¼°æ±‡æ€»è¡¨æ ¼
        
        æ ¸å¿ƒæŒ‡æ ‡:
        - â‘  ç›®æ ‡åµŒå…¥åŸºçº¿ (MineCLIP)
        - â‘¡ Prior ç›®æ ‡åµŒå…¥
        - â‘¢ æ¥è¿‘ç‡åŸºçº¿
        - â‘£ Policy æ¥è¿‘ç‡
        - â‘¤ Policy æˆåŠŸç‡
        
        è¾…åŠ©æŒ‡æ ‡:
        - Prior å˜ä½“
        - Prior åŒºåˆ†åº¦
        - åŠ¨ä½œç›¸ä¼¼åº¦
        - Camera ç›¸ä¼¼åº¦
        - åŠ¨ä½œç†µ
        - æ—¶åºå¹³æ»‘åº¦
        - åŠ¨ä½œè¦†ç›–ç‡
        """
        logger.info(f"{'='*40}")
        logger.info(f"ä»»åŠ¡è¯„ä¼°æ±‡æ€»: {task_id}")
        logger.info(f"{'='*40}")
        
        # ========== æ ¸å¿ƒæŒ‡æ ‡ï¼ˆä»»åŠ¡çº§åˆ«ï¼‰ ==========
        mineclip_baseline = prior_metrics.get('mineclip_baseline', 0)
        prior_output = prior_metrics.get('goal_accuracy', 0)
        expert_progress = action_similarity_metrics.get('expert_progress_rate', 0)
        expert_monotonic = action_similarity_metrics.get('expert_monotonic_rate', 0)
        model_progress = goal_progress_metrics.get('model_progress_rate', 0)
        model_monotonic = goal_progress_metrics.get('model_monotonic_rate', 0)
        success_rate = result.success_rate if result else 0
        
        # æ ¸å¿ƒæŒ‡æ ‡æ±‡æ€»
        logger.info(f"ã€æ ¸å¿ƒæŒ‡æ ‡ã€‘")
        logger.info(f"{'æŒ‡æ ‡':<20} {'å€¼':<15}")
        logger.info(f"{'-'*35}")
        logger.info(f"{'â‘  ç›®æ ‡åµŒå…¥åŸºçº¿':<20} {mineclip_baseline:.4f}")
        logger.info(f"{'â‘¡ Prior ç›®æ ‡åµŒå…¥':<20} {prior_output:.4f}")
        logger.info(f"{'â‘¢ æ¥è¿‘ç‡åŸºçº¿':<20} {expert_progress:+.1%} / {expert_monotonic:.1%}")
        logger.info(f"{'â‘£ Policy æ¥è¿‘ç‡':<20} {model_progress:+.1%} / {model_monotonic:.1%}")
        logger.info(f"{'â‘¤ Policy æˆåŠŸç‡':<20} {success_rate:.1%}")
        
        # ========== Trial çº§åˆ«è¡¨æ ¼ ==========
        if result and result.trials:
            logger.info(f"\nã€Trial çº§åˆ«è¯¦æƒ…ã€‘")
            # è¡¨å¤´
            header = f"{'Trial':<8} {'æˆåŠŸ':<6} {'æ­¥æ•°':<8} {'æ—¶é—´(s)':<10} {'è¿›åº¦ç‡':<12} {'å•è°ƒç‡':<10}"
            logger.info(header)
            logger.info(f"{'-'*len(header)}")
            
            for i, trial in enumerate(result.trials, 1):
                success_mark = "âœ…" if trial.success else "âŒ"
                progress_rate = getattr(trial, 'goal_progress_rate', 0)
                monotonic_rate = getattr(trial, 'goal_monotonic_rate', 0)
                logger.info(
                    f"{i:<8} {success_mark:<6} {trial.steps:<8} {trial.time_seconds:<10.1f} "
                    f"{progress_rate:+.1%}      {monotonic_rate:.1%}"
                )
        
        # ========== è¾…åŠ©æŒ‡æ ‡ ==========
        logger.info(f"ã€è¾…åŠ©æŒ‡æ ‡ã€‘")
        logger.info(f"{'æŒ‡æ ‡':<20} {'å€¼':<15}")
        logger.info(f"{'-'*35}")
        
        # Prior ç›¸å…³
        variant_alignment = prior_metrics.get('semantic_robustness', 0)
        discriminability = prior_metrics.get('discriminability', 0)
        logger.info(f"{'Prior å˜ä½“':<20} {variant_alignment:.4f}")
        logger.info(f"{'Prior åŒºåˆ†åº¦':<20} {discriminability:.4f}")
        
        # Policy ç›¸å…³
        action_sim = action_similarity_metrics.get('action_similarity', 0)
        camera_sim = action_similarity_metrics.get('camera_similarity', 0)
        action_entropy = action_similarity_metrics.get('action_entropy', 0)
        temporal_smooth = action_similarity_metrics.get('temporal_smoothness', 0)
        action_coverage = action_similarity_metrics.get('action_coverage', 0)
        
        logger.info(f"{'åŠ¨ä½œç›¸ä¼¼åº¦':<20} {action_sim:.1%}")
        logger.info(f"{'Camera ç›¸ä¼¼åº¦':<20} {camera_sim:.1%}")
        logger.info(f"{'åŠ¨ä½œç†µ':<20} {action_entropy:.2f}")
        logger.info(f"{'æ—¶åºå¹³æ»‘åº¦':<20} {temporal_smooth:.1%}")
        logger.info(f"{'åŠ¨ä½œè¦†ç›–ç‡':<20} {action_coverage:.1%}")
        
        logger.info(f"{'='*40}")
    
    def _get_goal_embed_for_task(self, task_id: str, task_config: Dict) -> Optional[np.ndarray]:
        """
        è·å–ä»»åŠ¡çš„ç›®æ ‡åµŒå…¥ï¼ˆä» train_samples çš„ visual_embeds.pklï¼‰
        
        ç›®å½•ç»“æ„: train_samples/{task_id}/trial{N}/visual_embeds.pkl
        
        Args:
            task_id: ä»»åŠ¡ID
            task_config: ä»»åŠ¡é…ç½®
            
        Returns:
            ç›®æ ‡åµŒå…¥æˆ– None
        """
        # ä» train_samples ç›®å½•åŠ è½½
        samples_dir = Path(task_config.get('train_samples_dir', self.config.train_samples_dir))
        task_samples_dir = samples_dir / task_id
        
        if not task_samples_dir.exists():
            logger.debug(f"è®­ç»ƒæ ·æœ¬ç›®å½•ä¸å­˜åœ¨: {task_samples_dir}")
            return None
        
        # æŸ¥æ‰¾æ‰€æœ‰ trial ç›®å½•ä¸­çš„ visual_embeds.pkl
        trial_dirs = sorted([d for d in task_samples_dir.iterdir() if d.is_dir() and d.name.startswith('trial')])
        embeds = []
        
        for trial_dir in trial_dirs:
            embed_file = trial_dir / 'visual_embeds.pkl'
            if embed_file.exists():
                try:
                    with open(embed_file, 'rb') as f:
                        goal_embed = pickle.load(f)
                    # ç¡®ä¿æ˜¯ numpy array
                    if hasattr(goal_embed, 'numpy'):
                        goal_embed = goal_embed.numpy()
                    embeds.append(np.squeeze(goal_embed))
                except Exception as e:
                    logger.debug(f"åŠ è½½ {embed_file} å¤±è´¥: {e}")
        
        if embeds:
            # è¿”å›æ‰€æœ‰ trial åµŒå…¥çš„å¹³å‡å€¼
            return np.mean(embeds, axis=0)
        
        return None
    
    def _save_combined_results(self, combined_result: TaskEvaluationResult, output_dir: Path):
        """
        ä¿å­˜ç»¼åˆè¯„ä¼°ç»“æœï¼ˆåŒ…å«ä¸‰é˜¶æ®µæŒ‡æ ‡ï¼‰
        
        Args:
            combined_result: ç»¼åˆè¯„ä¼°ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        """
        combined_path = output_dir / "combined_evaluation.json"
        with open(combined_path, 'w', encoding='utf-8') as f:
            json.dump(combined_result.to_dict(), f, ensure_ascii=False, indent=2)
        logger.debug(f"  âœ“ ç»¼åˆè¯„ä¼°ç»“æœå·²ä¿å­˜: {combined_path.name}")
    
    def _save_task_results(self, result: TaskResult, output_dir: Path):
        """
        ä¿å­˜ä»»åŠ¡ç»“æœåˆ°æŒ‡å®šç›®å½•ï¼ˆJSONã€TXTï¼‰
        
        æ³¨æ„ï¼šè§†é¢‘ä¿å­˜ç°åœ¨ç”± policy_evaluator åœ¨ _run_single_trial ä¸­å®Œæˆ
        
        Args:
            result: ä»»åŠ¡ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        """
        # æ„å»ºç»“æœæ•°æ®
        result_data = {
            "task_id": result.task_id,
            "language": result.language,
            "instruction": result.instruction,
            "success_rate": result.success_rate,
            "avg_steps": result.avg_steps,
            "avg_time": result.avg_time,
            "trials": [
                {
                    "trial_idx": i + 1,
                    "success": trial.success,
                    "steps": trial.steps,
                    "time_seconds": trial.time_seconds,
                    "has_video": (output_dir / f"trial_{i+1}.mp4").exists()  # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                }
                for i, trial in enumerate(result.trials)
            ]
        }
        
        # ä¿å­˜JSON
        json_path = output_dir / "result.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        #logger.info(f"  âœ“ ç»“æœå·²ä¿å­˜: {json_path.name}")
        
        # ä¿å­˜TXTï¼ˆäººç±»å¯è¯»ï¼‰
        txt_path = output_dir / "result.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write(f"ä»»åŠ¡è¯„ä¼°ç»“æœ: {result.task_id}\n")
            f.write("="*80 + "\n\n")
            f.write(f"è¯­è¨€: {result.language}\n")
            f.write(f"æŒ‡ä»¤: {result.instruction}\n")
            f.write(f"æˆåŠŸç‡: {result.success_rate*100:.1f}%\n")
            f.write(f"å¹³å‡æ­¥æ•°: {result.avg_steps:.1f}\n")
            f.write(f"å¹³å‡æ—¶é—´: {result.avg_time:.1f}s\n\n")
            f.write("è¯•éªŒè¯¦æƒ…:\n")
            f.write("-"*80 + "\n")
            for i, trial in enumerate(result.trials, 1):
                status = "âœ… æˆåŠŸ" if trial.success else "âŒ å¤±è´¥"
                video_status = "ğŸ¬" if (output_dir / f"trial_{i}.mp4").exists() else ""
                f.write(f"Trial {i}: {status} | æ­¥æ•°: {trial.steps:4d} | æ—¶é—´: {trial.time_seconds:.1f}s {video_status}\n")
        #logger.info(f"  âœ“ æŠ¥å‘Šå·²ä¿å­˜: {txt_path.name}")
    
    def evaluate_task_list(
        self,
        task_ids: List[str],
        n_trials: Optional[int] = None,
        max_steps: Optional[int] = None,
        task_set_name: Optional[str] = None  # ä»»åŠ¡é›†åç§°ï¼ˆç”¨äºåˆ›å»ºç›®å½•ï¼‰
    ) -> List[TaskResult]:
        """
        æ‰¹é‡è¯„ä¼°ä»»åŠ¡åˆ—è¡¨
        
        Args:
            task_ids: ä»»åŠ¡IDåˆ—è¡¨
            n_trials: è¯•éªŒæ¬¡æ•°ï¼ˆåº”ç”¨äºæ‰€æœ‰ä»»åŠ¡ï¼‰
            max_steps: æœ€å¤§æ­¥æ•°ï¼ˆåº”ç”¨äºæ‰€æœ‰ä»»åŠ¡ï¼‰
            task_set_name: ä»»åŠ¡é›†åç§°ï¼ˆå¦‚æœæä¾›ï¼Œå°†åˆ›å»ºä¸“é—¨çš„ç›®å½•ï¼‰
        
        Returns:
            List[TaskResult]: ä»»åŠ¡ç»“æœåˆ—è¡¨
        """
        
        # å¦‚æœæä¾›äº† task_set_nameï¼Œåˆ›å»º task-set ç›®å½•
        task_set_dir = None
        if task_set_name:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            task_set_dir_name = f"{task_set_name}_{timestamp}"
            task_set_dir = self.base_output_dir / task_set_dir_name
            task_set_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"ç»“æœè¾“å‡ºç›®å½•: {task_set_dir}")
            
            # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•ï¼ˆåœ¨task_set_dirä¸‹ï¼‰
            checkpoint_dir = task_set_dir / "checkpoints"
            checkpoint_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"æ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_dir}")
            
            # ä¸ºè¿™ä¸ªä»»åŠ¡é›†åˆ›å»ºä¸“å±çš„æ£€æŸ¥ç‚¹ç®¡ç†å™¨
            self.checkpoint_manager = CheckpointManager(checkpoint_dir)
            
            # ä¿å­˜ task_set_dir ä¾›åç»­ generate_report ä½¿ç”¨
            self.current_task_set_dir = task_set_dir
        
        # Task-setçº§åˆ«çš„æ£€æŸ¥ç‚¹æ¢å¤
        completed_task_ids = []
        remaining_task_ids = task_ids.copy()
        
        if task_set_name and self.checkpoint_manager and self.checkpoint_config.enabled and self.checkpoint_config.auto_resume:
            taskset_checkpoint = self.checkpoint_manager.load_taskset_checkpoint(task_set_name)
            if taskset_checkpoint:
                # æ£€æŸ¥ä»»åŠ¡åˆ—è¡¨æ˜¯å¦åŒ¹é…
                if taskset_checkpoint['all_task_ids'] == task_ids:
                    completed_task_ids = taskset_checkpoint['completed_task_ids']
                    remaining_task_ids = [tid for tid in task_ids if tid not in completed_task_ids]
                    logger.info(f"ğŸ“¥ å‘ç°task-setæ£€æŸ¥ç‚¹ï¼Œæ¢å¤è¿›åº¦...")
                    logger.info(f"   å·²å®Œæˆ: {len(completed_task_ids)}/{len(task_ids)} tasks")
                    logger.info(f"   å‰©ä½™: {len(remaining_task_ids)} tasks")
                    logger.info(f"   å°†ä»ç¬¬{len(completed_task_ids)+1}ä¸ªä»»åŠ¡ç»§ç»­\n")
                else:
                    logger.warning(f"âš ï¸ Task-setæ£€æŸ¥ç‚¹çš„ä»»åŠ¡åˆ—è¡¨ä¸åŒ¹é…ï¼Œå¿½ç•¥æ£€æŸ¥ç‚¹")
        
        results = []
        
        # åªè¯„ä¼°å‰©ä½™çš„ä»»åŠ¡
        for i, task_id in enumerate(task_ids, 1):
            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
            if task_id in completed_task_ids:
                logger.info(f"[{i}/{len(task_ids)}] â­ï¸  è·³è¿‡å·²å®Œæˆä»»åŠ¡: {task_id}")
                continue
            
            #logger.info(f"[{i}/{len(task_ids)}] è¯„ä¼°ä»»åŠ¡: {task_id}")
            
            try:
                # evaluate_single_task è¿”å› (TaskResult, output_dir)
                result, _ = self.evaluate_single_task(
                    task_id=task_id,
                    n_trials=n_trials,
                    max_steps=max_steps,
                    parent_dir=task_set_dir,  # ä¼ é€’ task-set ç›®å½•
                    task_index=i,  # ä¼ é€’ä»»åŠ¡ç´¢å¼•
                    total_tasks=len(task_ids)  # ä¼ é€’æ€»ä»»åŠ¡æ•°
                )
                results.append(result)  # åªä¿å­˜ TaskResult
                
                # æ‰“å°ä»»åŠ¡æ‘˜è¦
                logger.info(f"{task_id} å®Œæˆ: æˆåŠŸç‡ {result.success_rate*100:.1f}%, "
                           f"å¹³å‡æ­¥æ•° {result.avg_steps:.1f}")
                
                # ä¿å­˜task-setæ£€æŸ¥ç‚¹ï¼ˆæ¯å®Œæˆä¸€ä¸ªä»»åŠ¡å°±ä¿å­˜ï¼‰
                if task_set_name and self.checkpoint_manager and self.checkpoint_config.enabled:
                    completed_task_ids.append(task_id)
                    self.checkpoint_manager.save_taskset_checkpoint(
                        task_set_name=task_set_name,
                        all_task_ids=task_ids,
                        completed_task_ids=completed_task_ids,
                        metadata={
                            "n_trials": n_trials,
                            "max_steps": max_steps
                        }
                    )
                
            except Exception as e:
                logger.error(f"  âŒ ä»»åŠ¡å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        logger.info(f"æ‰¹é‡è¯„ä¼°å®Œæˆ: {len(results)}/{len(task_ids)} ä¸ªä»»åŠ¡æˆåŠŸ")
        
        # å®Œæˆåæ¸…ç†task-setæ£€æŸ¥ç‚¹
        if task_set_name and self.checkpoint_manager and self.checkpoint_config.enabled and self.checkpoint_config.cleanup_on_complete:
            if len(completed_task_ids) == len(task_ids):  # æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆäº†
                self.checkpoint_manager.delete_taskset_checkpoint(task_set_name)
                logger.info(f"  Task-setå·²å…¨éƒ¨å®Œæˆï¼Œæ£€æŸ¥ç‚¹å·²æ¸…ç†")
        
        # æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œé‡ç½® current_task_set_dirï¼Œå› ä¸º generate_report è¿˜éœ€è¦ç”¨å®ƒ
        
        return results
    
    def evaluate_task_set(
        self,
        task_set_name: str,
        n_trials: Optional[int] = None,
        max_steps: Optional[int] = None
    ) -> List[TaskResult]:
        """
        è¯„ä¼°ä»»åŠ¡é›†ï¼ˆä» YAML é…ç½®ä¸­çš„ harvest_tasks, quick_test, baseline_test ç­‰ï¼‰
        
        Args:
            task_set_name: ä»»åŠ¡é›†åç§° ('harvest_tasks', 'quick_test', 'baseline_test')
            n_trials: è¯•éªŒæ¬¡æ•°
            max_steps: æœ€å¤§æ­¥æ•°
        
        Returns:
            List[TaskResult]: ä»»åŠ¡ç»“æœåˆ—è¡¨
        """
        # ä» YAML åŠ è½½ä»»åŠ¡é›†
        task_ids = self.task_loader.get_task_set(task_set_name)
        
        if not task_ids:
            raise ValueError(f"ä»»åŠ¡é›†ä¸å­˜åœ¨æˆ–ä¸ºç©º: {task_set_name}")
        
        
        logger.info(f"è¯„ä¼°ä»»åŠ¡é›†: {task_set_name}")
        logger.info(f"ä»»åŠ¡æ•°é‡: {len(task_ids)}")
        logger.info(f"ä»»åŠ¡åˆ—è¡¨: {', '.join(task_ids)}")
        
        return self.evaluate_task_list(
            task_ids=task_ids,
            n_trials=n_trials,
            max_steps=max_steps,
            task_set_name=task_set_name  # ä¼ é€’ä»»åŠ¡é›†åç§°
        )
    
    def print_summary(self, results: Optional[List[TaskResult]] = None):
        """
        æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦
        
        Args:
            results: ä»»åŠ¡ç»“æœåˆ—è¡¨ï¼ˆå¦‚æœNoneåˆ™ä½¿ç”¨self.resultsï¼‰
        """
        if results is None:
            results = self.results
        
        if not results:
            logger.warning("æ²¡æœ‰è¯„ä¼°ç»“æœ")
            return
        
        print(f"\n{'='*100}")
        print("è¯„ä¼°ç»“æœæ±‡æ€»")
        print(f"{'='*100}\n")
        
        # è¡¨å¤´
        print(f"{'ä»»åŠ¡ID':<30} {'æŒ‡ä»¤':<20} {'æˆåŠŸç‡':<10} {'å¹³å‡æ­¥æ•°':<12} {'å¹³å‡æ—¶é—´'}")
        print("-" * 100)
        
        # æ¯ä¸ªä»»åŠ¡çš„ç»“æœ
        for result in results:
            task_id = result.task_id[:28]  # æˆªæ–­è¿‡é•¿çš„ID
            instruction = result.instruction[:18] if result.instruction else "N/A"
            success_rate = f"{result.success_rate * 100:.1f}%"
            avg_steps = f"{result.avg_steps:.1f}"
            avg_time = f"{result.avg_time:.1f}s"
            
            print(f"{task_id:<30} {instruction:<20} {success_rate:<10} {avg_steps:<12} {avg_time}")
        
        # æ€»ä½“ç»Ÿè®¡
        print("\n" + "-" * 80)
        overall_success = sum(r.success_rate for r in results) / len(results)
        overall_steps = sum(r.avg_steps for r in results) / len(results)
        overall_time = sum(r.avg_time for r in results) / len(results)
        total_trials = sum(len(r.trials) for r in results)
        
        print(f"{'æ€»ä½“ç»Ÿè®¡':<30} {'N/A':<20} {overall_success*100:.1f}% {overall_steps:<12.1f} {overall_time:.1f}s")
        print(f"\næ€»ä»»åŠ¡æ•°: {len(results)}")
        print(f"æ€»è¯•éªŒæ•°: {total_trials}")
        print(f"{'='*100}\n")
    
    def generate_report(
        self,
        results: Optional[List[TaskResult]] = None,
        report_name: str = "evaluation_report"
    ):
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            results: ä»»åŠ¡ç»“æœåˆ—è¡¨ï¼ˆå¦‚æœNoneåˆ™ä½¿ç”¨self.resultsï¼‰
            report_name: æŠ¥å‘Šåç§°
            
        Returns:
            Tuple[str, str]: JSONæŠ¥å‘Šè·¯å¾„å’ŒTXTæŠ¥å‘Šè·¯å¾„
        """
        if results is None:
            results = self.results
        
        if not results:
            logger.warning("æ²¡æœ‰è¯„ä¼°ç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return None, None
        
        # æ„å»ºæŠ¥å‘Šæ•°æ®
        report_data = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_tasks": len(results),
                "evaluator": "STEVE-1",
                "framework": "EvaluationFramework"
            },
            "tasks": []
        }
        
        for result in results:
            task_data = {
                "task_id": result.task_id,
                "instruction": result.instruction,
                "language": result.language,
                "success_rate": result.success_rate * 100,  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                "avg_steps": result.avg_steps,
                "avg_time": result.avg_time,
                "trials": [
                    {
                        "success": trial.success,
                        "steps": trial.steps,
                        "time_seconds": trial.time_seconds,
                        "final_inventory": trial.final_inventory
                    }
                    for trial in result.trials
                ]
            }
            report_data["tasks"].append(task_data)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        report_data["summary"] = {
            "overall_success_rate": np.mean([r.success_rate for r in results]) * 100,
            "total_trials": sum(len(r.trials) for r in results),
            "successful_trials": sum(sum(1 for t in r.trials if t.success) for r in results)
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_filename = f"{report_name}_{timestamp}.json"
        
        # ä¼˜å…ˆçº§ï¼štask-set ç›®å½• > å•ä»»åŠ¡ç›®å½• > å…¨å±€ç›®å½•
        if self.current_task_set_dir:
            # å¤šä»»åŠ¡è¯„ä¼°ï¼ˆtask-setï¼‰ï¼Œä¿å­˜åˆ° task-set ç›®å½•
            json_path = self.current_task_set_dir / json_filename
            #logger.info(f"å°†æŠ¥å‘Šä¿å­˜åˆ° task-set ç›®å½•: {self.current_task_set_dir.name}")
        elif len(results) == 1:
            # å•ä»»åŠ¡è¯„ä¼°ï¼Œä¿å­˜åˆ°ä»»åŠ¡ç›®å½•ä¸‹
            task_id = results[0].task_id
            language = results[0].language
            # æŸ¥æ‰¾åŒ¹é…çš„ç›®å½•ï¼ˆæŒ‰æ—¶é—´å€’åºï¼‰
            pattern = f"{task_id}_{language}_*"
            matching_dirs = sorted(
                self.base_output_dir.glob(pattern),
                key=lambda p: p.stat().st_mtime,
                reverse=True
            )
            if matching_dirs:
                json_path = matching_dirs[0] / json_filename
                #logger.info(f"å°†æŠ¥å‘Šä¿å­˜åˆ°ä»»åŠ¡ç›®å½•: {matching_dirs[0].name}")
            else:
                json_path = self.base_output_dir / json_filename
        else:
            # å¤šä»»åŠ¡ä½†æ—  task-setï¼Œä½¿ç”¨å…¨å±€ç›®å½•
            json_path = self.base_output_dir / json_filename
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        txt_path = json_path.with_suffix('.txt')
        self._generate_text_report(report_data, txt_path)
        
        # ç”Ÿæˆä¸‰ç»´èƒ½åŠ›çŸ©é˜µåˆ†æå’ŒHTMLæŠ¥å‘Š
        matrix_analysis, html_path = self._generate_matrix_report(results, json_path.parent)
        
        logger.info(f"æŠ¥å‘Šå·²ç”Ÿæˆ:")
        logger.info(f"  JSON: {json_path}")
        logger.info(f"  TXT:  {txt_path}")
        if html_path:
            logger.info(f"  HTML: open {html_path}")
        return str(json_path), str(txt_path)
    
    def _generate_matrix_report(
        self, 
        results: List[TaskResult], 
        output_dir: Path
    ) -> Tuple[Optional[Dict], Optional[Path]]:
        """
        ç”Ÿæˆ Prior å’Œ Policy ç»¼åˆè¯„ä¼° HTML æŠ¥å‘Š
        
        Args:
            results: ä»»åŠ¡ç»“æœåˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            (analysis_data, html_path): åˆ†ææ•°æ®å’ŒHTMLè·¯å¾„
        """
        try:
            # æ”¶é›†ç»¼åˆè¯„ä¼°ç»“æœ
            logger.info(f"æ”¶é›†ç»¼åˆè¯„ä¼°ç»“æœ: {output_dir}")
            combined_results = self._collect_combined_results(output_dir)
            logger.info(f"æ”¶é›†åˆ° {len(combined_results)} ä¸ªä»»åŠ¡çš„ç»¼åˆè¯„ä¼°ç»“æœ")
            
            # æ„å»ºæŠ¥å‘Šæ•°æ®ç»“æ„ï¼ˆå…¼å®¹ PriorHTMLGeneratorï¼‰
            report_data = self._build_report_data(results, combined_results)
            
            # è°ƒè¯•ï¼šæ£€æŸ¥ goal_progress_summary
            goal_progress_summary = report_data.get('summary', {}).get('goal_progress_summary', {})
            logger.debug(f"  goal_progress_summary: {goal_progress_summary}")
            
            if not report_data:
                logger.warning("æ²¡æœ‰å¯åˆ†æçš„ä»»åŠ¡æ•°æ®")
                return None, None
            
            # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ï¼ˆå¦‚æœæœ‰å¤šä¸ªä»»åŠ¡ï¼‰
            if len(results) >= 2:
                logger.info("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
                self._generate_visualizations(results, combined_results, output_dir)
            
            # ç”Ÿæˆ HTML æŠ¥å‘Š
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            html_generator = PriorHTMLGenerator(str(output_dir))
            html_path = html_generator.generate_report(
                report_data,
                output_filename=f"evaluation_report_{timestamp}.html"
            )
            
            return report_data, html_path
            
        except Exception as e:
            logger.error(f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
            import traceback
            traceback.print_exc()
            return None, None
    
    def _generate_visualizations(
        self,
        results: List[TaskResult],
        combined_results: Dict[str, Dict],
        output_dir: Path
    ):
        """
        ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        
        å›¾1: MineCLIP vs Prior ç©ºé—´å¯¹æ¯” (t-SNE)
        å›¾2: Prior è¾“å‡ºç›¸ä¼¼åº¦çŸ©é˜µ
        å›¾3: Prior è¾“å‡ºæ–¹å·®åˆ†å¸ƒ
        å›¾4: ç›®æ ‡æ¥è¿‘åº¦æ¦‚è§ˆ
        
        Args:
            results: TaskResult åˆ—è¡¨
            combined_results: ç»¼åˆè¯„ä¼°ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        """
        try:
            import matplotlib
            matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯
            import matplotlib.pyplot as plt
            from sklearn.manifold import TSNE
            import torch as th
            
            # é…ç½®ä¸­æ–‡å­—ä½“
            plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans', 'sans-serif']
            plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
            
            # è·å– Prior è¯„ä¼°å™¨
            prior_evaluator = self._get_prior_evaluator()
            if prior_evaluator is None:
                logger.warning("  æ— æ³•è·å– Prior è¯„ä¼°å™¨ï¼Œè·³è¿‡å¯è§†åŒ–")
                return
            
            # æ”¶é›†åµŒå…¥æ•°æ®
            task_ids = []
            prior_embeds = []
            text_embeds = []
            task_info = {}
            visual_embeds_by_task = {}  # {task_id: [visual_embeds]}
            variant_embeds_by_task = {}  # {task_id: [(embed, category_id), ...]}
            
            for result in results:
                task_id = result.task_id
                instruction = result.instruction
                
                if not instruction:
                    continue
                
                task_ids.append(task_id)
                
                # è·å–ä»»åŠ¡é…ç½®
                task_config = self.task_loader.get_task(task_id)
                if task_config:
                    category = task_config.get('category', 'other')
                    if category in ['harvest', 'raw_resources', 'plants', 'animal_drops']:
                        cat_display = 'Harvest'
                    elif category == 'combat':
                        cat_display = 'Combat'
                    elif category == 'techtree':
                        cat_display = 'Techtree'
                    else:
                        cat_display = 'Other'
                    
                    tier = task_config.get('tier', 2)
                    task_info[task_id] = {'tier': tier, 'category': cat_display}
                    
                    # è·å–æŒ‡ä»¤å˜ä½“ï¼ˆå¸¦åˆ†ç±»ä¿¡æ¯ï¼‰
                    instruction_variants = task_config.get('instruction_variants', {})
                    variant_embeds_by_task[task_id] = []
                    if instruction_variants:
                        if isinstance(instruction_variants, dict):
                            # åˆ†ç±»å˜ä½“ç»“æ„: {category_id: {variants: [...]}}
                            for cat_id, cat_config in instruction_variants.items():
                                if isinstance(cat_config, dict):
                                    variants = cat_config.get('variants', [])[:2]  # æ¯ç±»å–2ä¸ª
                                elif isinstance(cat_config, list):
                                    variants = cat_config[:2]
                                else:
                                    continue
                                
                                for variant in variants:
                                    try:
                                        z_variant = prior_evaluator._get_prior_embed(variant)
                                        variant_embeds_by_task[task_id].append((z_variant, cat_id))
                                    except:
                                        pass
                        elif isinstance(instruction_variants, list):
                            # ç®€å•åˆ—è¡¨æ ¼å¼
                            for variant in instruction_variants[:10]:
                                try:
                                    z_variant = prior_evaluator._get_prior_embed(variant)
                                    variant_embeds_by_task[task_id].append((z_variant, 'uncategorized'))
                                except:
                                    pass
                else:
                    task_info[task_id] = {'tier': 2, 'category': 'Other'}
                    variant_embeds_by_task[task_id] = []
                
                # Prior åµŒå…¥
                try:
                    z_prior = prior_evaluator._get_prior_embed(instruction)
                    prior_embeds.append(z_prior)
                except Exception as e:
                    logger.debug(f"  è·å– Prior åµŒå…¥å¤±è´¥ ({task_id}): {e}")
                    prior_embeds.append(np.zeros(512))
                
                # MineCLIP æ–‡æœ¬åµŒå…¥
                try:
                    with th.no_grad():
                        z_text = prior_evaluator._mineclip.encode_text([instruction])[0].cpu().numpy()
                    text_embeds.append(z_text)
                except Exception as e:
                    logger.debug(f"  è·å– MineCLIP åµŒå…¥å¤±è´¥ ({task_id}): {e}")
                    text_embeds.append(np.zeros(512))
                
                # æˆåŠŸè§†é¢‘åµŒå…¥
                visual_embeds_by_task[task_id] = []
                if hasattr(prior_evaluator, 'success_visuals') and task_id in prior_evaluator.success_visuals:
                    task_visuals = prior_evaluator.success_visuals[task_id]
                    # success_visuals[task_id] æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« 'success_visual_embeds' å­—æ®µ
                    if isinstance(task_visuals, dict) and 'success_visual_embeds' in task_visuals:
                        embeds = task_visuals['success_visual_embeds']
                        visual_embeds_by_task[task_id] = list(embeds[:3]) if hasattr(embeds, '__getitem__') else []
                    elif isinstance(task_visuals, (list, np.ndarray)):
                        visual_embeds_by_task[task_id] = list(task_visuals[:3])
            
            if len(task_ids) < 2:
                logger.warning("  ä»»åŠ¡æ•°å°‘äº2ä¸ªï¼Œè·³è¿‡å¯è§†åŒ–")
                return
            
            prior_embeds = np.array(prior_embeds)
            text_embeds = np.array(text_embeds)
            
            # === å›¾1: MineCLIP vs Prior ç©ºé—´å¯¹æ¯” ===
            try:
                logger.info("  ç”Ÿæˆå›¾1: MineCLIP vs Prior ç©ºé—´å¯¹æ¯”...")
                self._plot_mineclip_vs_prior(text_embeds, prior_embeds, task_ids, task_info, output_dir)
            except Exception as e:
                logger.warning(f"    å›¾1ç”Ÿæˆå¤±è´¥: {e}")
            
            # === å›¾2: å˜ä½“è¾“å‡º vs ç›®æ ‡è§†é¢‘ ===
            try:
                if any(variant_embeds_by_task.values()) and any(visual_embeds_by_task.values()):
                    logger.info("  ç”Ÿæˆå›¾2: å˜ä½“è¾“å‡º vs ç›®æ ‡è§†é¢‘...")
                    self._plot_variants_vs_visual(variant_embeds_by_task, visual_embeds_by_task, task_ids, task_info, output_dir)
            except Exception as e:
                logger.warning(f"    å›¾2ç”Ÿæˆå¤±è´¥: {e}")
            
            # === å›¾3: Prior vs ç›®æ ‡è§†é¢‘ ===
            try:
                if any(visual_embeds_by_task.values()):
                    logger.info("  ç”Ÿæˆå›¾3: Prior vs ç›®æ ‡è§†é¢‘...")
                    self._plot_prior_vs_visual(prior_embeds, visual_embeds_by_task, task_ids, task_info, output_dir)
            except Exception as e:
                logger.warning(f"    å›¾3ç”Ÿæˆå¤±è´¥: {e}")
            
            # === è¾…åŠ©å›¾1: Prior è¾“å‡ºç›¸ä¼¼åº¦çŸ©é˜µ ===
            try:
                logger.info("  ç”Ÿæˆè¾…åŠ©å›¾1: Prior è¾“å‡ºç›¸ä¼¼åº¦çŸ©é˜µ...")
                self._plot_similarity_matrix(prior_embeds, task_ids, output_dir)
            except Exception as e:
                logger.warning(f"    è¾…åŠ©å›¾1ç”Ÿæˆå¤±è´¥: {e}")
            
            # === è¾…åŠ©å›¾2: Prior è¾“å‡ºæ–¹å·®åˆ†å¸ƒ ===
            try:
                logger.info("  ç”Ÿæˆè¾…åŠ©å›¾2: Prior è¾“å‡ºæ–¹å·®åˆ†å¸ƒ...")
                self._plot_variance_distribution(prior_embeds, output_dir)
            except Exception as e:
                logger.warning(f"    è¾…åŠ©å›¾2ç”Ÿæˆå¤±è´¥: {e}")
            
            # === å›¾4: ç›®æ ‡æ¥è¿‘åº¦æ¦‚è§ˆ ===
            goal_progress_data = {}
            for task_id, combined in combined_results.items():
                action_metrics = combined.get('action_similarity_metrics', {})
                goal_metrics = combined.get('goal_progress_metrics', {})
                if action_metrics.get('enabled') or goal_metrics.get('enabled'):
                    goal_progress_data[task_id] = {
                        'expert_progress_rate': action_metrics.get('expert_progress_rate', 0),
                        'expert_monotonic_rate': action_metrics.get('expert_monotonic_rate', 0),
                        'model_progress_rate': goal_metrics.get('model_progress_rate', 0),
                        'model_monotonic_rate': goal_metrics.get('model_monotonic_rate', 0),
                    }
            
            # === å›¾4: ç›®æ ‡æ¥è¿‘åº¦æ¦‚è§ˆ ===
            try:
                if goal_progress_data:
                    logger.info("  ç”Ÿæˆå›¾4: ç›®æ ‡æ¥è¿‘åº¦æ¦‚è§ˆ...")
                    self._plot_goal_progress_overview(goal_progress_data, output_dir)
            except Exception as e:
                logger.warning(f"    å›¾4ç”Ÿæˆå¤±è´¥: {e}")
            
            # === Policy å¯è§†åŒ–å›¾è¡¨ ===
            # æ”¶é›†åŠ¨ä½œåˆ†å¸ƒå’Œè¯¦ç»†æ•°æ®
            all_expert_dist = {}
            all_model_dist = {}
            all_expert_actions = []
            all_model_actions = []
            all_frame_similarities = {}
            all_camera_similarities = {}
            all_expert_distances = {}
            all_model_distances = {}
            
            for task_id, combined in combined_results.items():
                action_metrics = combined.get('action_similarity_metrics', {})
                goal_metrics = combined.get('goal_progress_metrics', {})
                
                # åŠ¨ä½œåˆ†å¸ƒ
                if action_metrics.get('expert_action_distribution'):
                    for k, v in action_metrics['expert_action_distribution'].items():
                        all_expert_dist[k] = all_expert_dist.get(k, 0) + v
                if action_metrics.get('model_action_distribution'):
                    for k, v in action_metrics['model_action_distribution'].items():
                        all_model_dist[k] = all_model_dist.get(k, 0) + v
                
                # åŠ¨ä½œåˆ—è¡¨ï¼ˆç”¨äºæ··æ·†çŸ©é˜µï¼‰
                expert_list = action_metrics.get('expert_actions_list', [])
                model_list = action_metrics.get('model_actions_list', [])
                all_expert_actions.extend(expert_list)
                all_model_actions.extend(model_list)
                
                # å¸§çº§åˆ«ç›¸ä¼¼åº¦
                if action_metrics.get('frame_similarities'):
                    all_frame_similarities[task_id] = action_metrics['frame_similarities']
                if action_metrics.get('camera_similarities'):
                    all_camera_similarities[task_id] = action_metrics['camera_similarities']
                
                # ä¸“å®¶å’Œæ¨¡å‹è·ç¦»æ—¶é—´çº¿
                if action_metrics.get('expert_distances'):
                    all_expert_distances[task_id] = action_metrics['expert_distances']
                
                # ä» trial ç»“æœè·å–æ¨¡å‹è·ç¦»
                policy_result = combined.get('policy_result', {})
                if policy_result.get('trials'):
                    for trial in policy_result['trials']:
                        if trial.get('goal_distances'):
                            all_model_distances[task_id] = trial['goal_distances']
                            break
            
            # === å›¾5: åŠ¨ä½œåˆ†å¸ƒå¯¹æ¯” ===
            try:
                if all_expert_dist or all_model_dist:
                    logger.info("  ç”Ÿæˆå›¾5: åŠ¨ä½œåˆ†å¸ƒå¯¹æ¯”...")
                    self._plot_action_distribution(all_expert_dist, all_model_dist, output_dir)
            except Exception as e:
                logger.warning(f"    å›¾5ç”Ÿæˆå¤±è´¥: {e}")
            
            # === å›¾6: åŠ¨ä½œæ··æ·†çŸ©é˜µ ===
            try:
                if len(all_expert_actions) > 10 and len(all_model_actions) > 10:
                    logger.info("  ç”Ÿæˆå›¾6: åŠ¨ä½œæ··æ·†çŸ©é˜µ...")
                    self._plot_confusion_matrix(all_expert_actions, all_model_actions, output_dir)
            except Exception as e:
                logger.warning(f"    å›¾6ç”Ÿæˆå¤±è´¥: {e}")
            
            # === å›¾7: é€å¸§ç›¸ä¼¼åº¦æ—¶é—´çº¿ï¼ˆæ‰€æœ‰ä»»åŠ¡æ±‡æ€»ï¼‰===
            try:
                if all_frame_similarities:
                    # åˆå¹¶æ‰€æœ‰ä»»åŠ¡çš„æ•°æ®
                    merged_frame_sim = []
                    merged_camera_sim = []
                    for task_id in all_frame_similarities:
                        merged_frame_sim.extend(all_frame_similarities[task_id])
                        if task_id in all_camera_similarities:
                            merged_camera_sim.extend(all_camera_similarities[task_id])
                    
                    n_tasks = len(all_frame_similarities)
                    logger.info(f"  ç”Ÿæˆå›¾7: é€å¸§ç›¸ä¼¼åº¦æ—¶é—´çº¿ï¼ˆ{n_tasks} ä¸ªä»»åŠ¡æ±‡æ€»ï¼‰...")
                    self._plot_similarity_timeline_aggregated(
                        all_frame_similarities,
                        all_camera_similarities,
                        output_dir
                    )
            except Exception as e:
                logger.warning(f"    å›¾7ç”Ÿæˆå¤±è´¥: {e}")
            
            # === å›¾8: ç›®æ ‡æ¥è¿‘åº¦å¯¹æ¯”ï¼ˆæ‰€æœ‰ä»»åŠ¡æ±‡æ€»ï¼‰===
            try:
                if all_expert_distances and all_model_distances:
                    common_tasks = set(all_expert_distances.keys()) & set(all_model_distances.keys())
                    if common_tasks:
                        logger.info(f"  ç”Ÿæˆå›¾8: ç›®æ ‡æ¥è¿‘åº¦å¯¹æ¯”ï¼ˆ{len(common_tasks)} ä¸ªä»»åŠ¡æ±‡æ€»ï¼‰...")
                        self._plot_goal_progress_comparison_aggregated(
                            all_expert_distances,
                            all_model_distances,
                            output_dir
                        )
            except Exception as e:
                logger.warning(f"    å›¾8ç”Ÿæˆå¤±è´¥: {e}")
            
            logger.info(f"  âœ“ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_dir}")
            
        except ImportError as e:
            logger.warning(f"  å¯è§†åŒ–ä¾èµ–ç¼ºå¤±: {e}")
        except Exception as e:
            import traceback
            logger.warning(f"  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            logger.warning(f"  è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
    
    def _plot_mineclip_vs_prior(
        self,
        text_embeds: np.ndarray,
        prior_embeds: np.ndarray,
        task_ids: List[str],
        task_info: Dict,
        output_dir: Path
    ):
        """
        å›¾1: MineCLIP vs Prior ç©ºé—´å¯¹æ¯” (t-SNE)
        ç”Ÿæˆä¸¤å¼ å›¾ï¼šæŒ‰ Tier å’ŒæŒ‰ Category ç€è‰²
        """
        import matplotlib.pyplot as plt
        from sklearn.manifold import TSNE
        
        # é¢œè‰²å®šä¹‰
        TIER_COLORS = {1: '#4CAF50', 2: '#FF9800', 3: '#F44336'}
        TIER_NAMES = {1: 'Tier 1', 2: 'Tier 2', 3: 'Tier 3'}
        CAT_COLORS = {'Harvest': '#4CAF50', 'Combat': '#F44336', 'Techtree': '#2196F3', 'Other': '#9E9E9E'}
        
        # åˆå¹¶åµŒå…¥
        all_embeds = np.vstack([text_embeds, prior_embeds])
        n = len(task_ids)
        
        if len(all_embeds) < 3:
            return
        
        try:
            perplexity = min(30, max(2, len(all_embeds) - 1))
            tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
            coords = tsne.fit_transform(all_embeds)
            
            # === å›¾1a: æŒ‰ Tier ç€è‰² ===
            fig, ax = plt.subplots(figsize=(10, 8))
            drawn_tiers = set()
            for i, task in enumerate(task_ids):
                info = task_info.get(task, {'tier': 2})
                tier = info['tier']
                color = TIER_COLORS.get(tier, '#9E9E9E')
                tier_name = TIER_NAMES.get(tier, f'Tier {tier}')
                
                label_mc = f'â—‹ MineCLIP ({tier_name})' if tier not in drawn_tiers else None
                ax.scatter(coords[i, 0], coords[i, 1], marker='o', s=100, 
                          c=[color], alpha=0.5, edgecolors='white', linewidth=1, label=label_mc)
                
                label_prior = f'â–² Prior ({tier_name})' if tier not in drawn_tiers else None
                ax.scatter(coords[n + i, 0], coords[n + i, 1], marker='^', s=130, 
                          c=[color], edgecolors='black', linewidth=1.5, label=label_prior)
                drawn_tiers.add(tier)
                
                ax.plot([coords[i, 0], coords[n + i, 0]], [coords[i, 1], coords[n + i, 1]], 
                       c=color, linestyle='--', alpha=0.3, linewidth=1)
            
            ax.legend(loc='upper right', fontsize=8, ncol=2, framealpha=0.9)
            ax.set_title('MineCLIP vs Prior (by Tier, t-SNE)', fontsize=14, fontweight='bold')
            ax.set_xlabel('t-SNE Dimension 1')
            ax.set_ylabel('t-SNE Dimension 2')
            ax.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(output_dir / "viz_1a_mineclip_vs_prior_tier.png", dpi=150, bbox_inches='tight')
            plt.close(fig)
            
            # === å›¾1b: æŒ‰ Category ç€è‰² ===
            fig, ax = plt.subplots(figsize=(10, 8))
            drawn_cats = set()
            for i, task in enumerate(task_ids):
                info = task_info.get(task, {'category': 'Other'})
                cat = info['category']
                color = CAT_COLORS.get(cat, '#9E9E9E')
                
                label_mc = f'â—‹ MineCLIP ({cat})' if cat not in drawn_cats else None
                ax.scatter(coords[i, 0], coords[i, 1], marker='o', s=100, 
                          c=[color], alpha=0.5, edgecolors='white', linewidth=1, label=label_mc)
                
                label_prior = f'â–² Prior ({cat})' if cat not in drawn_cats else None
                ax.scatter(coords[n + i, 0], coords[n + i, 1], marker='^', s=130, 
                          c=[color], edgecolors='black', linewidth=1.5, label=label_prior)
                drawn_cats.add(cat)
                
                ax.plot([coords[i, 0], coords[n + i, 0]], [coords[i, 1], coords[n + i, 1]], 
                       c=color, linestyle='--', alpha=0.3, linewidth=1)
            
            ax.legend(loc='upper right', fontsize=8, ncol=2, framealpha=0.9)
            ax.set_title('MineCLIP vs Prior (by Category, t-SNE)', fontsize=14, fontweight='bold')
            ax.set_xlabel('t-SNE Dimension 1')
            ax.set_ylabel('t-SNE Dimension 2')
            ax.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(output_dir / "viz_1b_mineclip_vs_prior_category.png", dpi=150, bbox_inches='tight')
            plt.close(fig)
            
        except Exception as e:
            logger.warning(f"    å›¾1ç”Ÿæˆå¤±è´¥: {e}")
    
    def _plot_variants_vs_visual(
        self,
        variant_embeds_by_task: Dict[str, List],  # {task_id: [(embed, category_id), ...]}
        visual_embeds_by_task: Dict[str, List[np.ndarray]],
        task_ids: List[str],
        task_info: Dict,
        output_dir: Path
    ):
        """
        å›¾2: å˜ä½“è¾“å‡º vs ç›®æ ‡è§†é¢‘ (t-SNE)
        ç”Ÿæˆä¸¤å¼ å›¾ï¼šæŒ‰å˜ä½“ç±»åˆ«ç€è‰² å’Œ æŒ‰ä»»åŠ¡ç€è‰²
        """
        import matplotlib.pyplot as plt
        from sklearn.manifold import TSNE
        
        # é¢œè‰²è°ƒè‰²æ¿
        PALETTE = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',
                   '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
        
        # å˜ä½“ç±»åˆ«æ˜¾ç¤ºåç§°å’Œé¢œè‰²
        CATEGORY_DISPLAY = {
            'simple_direct': 'Simple Direct',
            'reddit_casual': 'Reddit Casual',
            'reddit_posts': 'Reddit Posts',
            'reddit_comments': 'Reddit Comments',
            'location_based': 'Location Based',
            'purpose_oriented': 'Purpose Oriented',
            'building_purpose': 'Building Purpose',
            'action_detailed': 'Action Detailed',
            'conversational': 'Conversational',
            'beginner_advice': 'Beginner Advice',
            'casual_slang': 'Casual Slang',
            'survival_urgency': 'Survival Urgency',
            'meme_culture': 'Meme Culture',
            'meme_humor': 'Meme Humor',
            'humor_meme': 'Humor Meme',
            'uncategorized': 'Uncategorized',
        }
        
        CATEGORY_COLORS = {
            'simple_direct': '#4CAF50',
            'reddit_casual': '#2196F3',
            'reddit_posts': '#03A9F4',
            'reddit_comments': '#00BCD4',
            'location_based': '#009688',
            'purpose_oriented': '#FF9800',
            'building_purpose': '#FF5722',
            'action_detailed': '#9C27B0',
            'conversational': '#E91E63',
            'beginner_advice': '#795548',
            'casual_slang': '#607D8B',
            'survival_urgency': '#F44336',
            'meme_culture': '#673AB7',
            'meme_humor': '#673AB7',
            'humor_meme': '#673AB7',
            'uncategorized': '#9E9E9E',
        }
        
        # æ”¶é›†æ‰€æœ‰æ•°æ®
        all_embeds = []
        embed_labels = []      # task_id
        embed_types = []       # 'variant' or 'visual'
        embed_categories = []  # å˜ä½“ç±»åˆ«
        
        for task_id in task_ids:
            # å˜ä½“åµŒå…¥ï¼ˆå¸¦åˆ†ç±»ï¼‰
            variants = variant_embeds_by_task.get(task_id, [])
            for item in variants[:10]:
                if isinstance(item, tuple):
                    embed, cat_id = item
                else:
                    embed, cat_id = item, 'uncategorized'
                all_embeds.append(embed)
                embed_labels.append(task_id)
                embed_types.append('variant')
                embed_categories.append(cat_id)
            
            # è§†è§‰åµŒå…¥
            visuals = visual_embeds_by_task.get(task_id, [])
            for v in visuals[:2]:
                all_embeds.append(v)
                embed_labels.append(task_id)
                embed_types.append('visual')
                embed_categories.append('_visual_')
        
        if len(all_embeds) < 3:
            logger.warning("    æ•°æ®ä¸è¶³ï¼Œè·³è¿‡å›¾2")
            return
        
        try:
            all_embeds = np.array(all_embeds)
            perplexity = min(30, max(2, len(all_embeds) - 1))
            tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
            coords = tsne.fit_transform(all_embeds)
            
            # ç»Ÿè®¡æœ‰å¤šå°‘ä¸ªçœŸæ­£çš„å˜ä½“ç±»åˆ«
            real_categories = [c for c in set(embed_categories) if c not in ['_visual_', 'uncategorized']]
            
            # === å›¾2a: æŒ‰å˜ä½“ç±»åˆ«ç€è‰²ï¼ˆå¦‚æœæœ‰åˆ†ç±»å˜ä½“ï¼‰===
            if real_categories:
                fig, ax = plt.subplots(figsize=(12, 8))
                
                # ç»˜åˆ¶ç›®æ ‡è§†é¢‘
                visual_mask = np.array([t == 'visual' for t in embed_types])
                visual_coords = coords[visual_mask]
                if len(visual_coords) > 0:
                    ax.scatter(visual_coords[:, 0], visual_coords[:, 1], marker='*', s=250,
                              c='#1565C0', edgecolors='white', linewidth=1, 
                              label='Target Video', zorder=10)
                
                # æŒ‰ç±»åˆ«ç»˜åˆ¶å˜ä½“
                all_cats = sorted(real_categories) + (['uncategorized'] if 'uncategorized' in embed_categories else [])
                for category in all_cats:
                    mask = np.array([(c == category and t == 'variant') for c, t in zip(embed_categories, embed_types)])
                    cat_coords = coords[mask]
                    
                    if len(cat_coords) == 0:
                        continue
                    
                    color = CATEGORY_COLORS.get(category, PALETTE[hash(category) % len(PALETTE)])
                    display_name = CATEGORY_DISPLAY.get(category, category)
                    
                    ax.scatter(cat_coords[:, 0], cat_coords[:, 1], marker='^', s=80,
                              c=[color], alpha=0.7, label=display_name)
                
                ax.legend(loc='upper right', fontsize=7, ncol=2, framealpha=0.9)
                ax.set_title('Variants vs Target Video (by Category, t-SNE)', fontsize=14, fontweight='bold')
                ax.set_xlabel('t-SNE Dimension 1')
                ax.set_ylabel('t-SNE Dimension 2')
                ax.grid(True, alpha=0.3)
                plt.tight_layout()
                plt.savefig(output_dir / "viz_2a_variants_by_category.png", dpi=150, bbox_inches='tight')
                plt.close(fig)
            
            # === å›¾2b: æŒ‰ä»»åŠ¡ç€è‰² ===
            fig, ax = plt.subplots(figsize=(12, 8))
            
            # ç»˜åˆ¶ç›®æ ‡è§†é¢‘ï¼ˆç»Ÿä¸€é¢œè‰²ï¼‰
            visual_mask = np.array([t == 'visual' for t in embed_types])
            visual_coords = coords[visual_mask]
            if len(visual_coords) > 0:
                ax.scatter(visual_coords[:, 0], visual_coords[:, 1], marker='*', s=250,
                          c='#1565C0', edgecolors='white', linewidth=1, 
                          label='Target Video', zorder=10)
            
            # æŒ‰ä»»åŠ¡ç€è‰²å˜ä½“
            unique_tasks = sorted(set(l for l, t in zip(embed_labels, embed_types) if t == 'variant'))
            task_colors = {t: PALETTE[i % len(PALETTE)] for i, t in enumerate(unique_tasks)}
            
            for task_id in unique_tasks:
                mask = np.array([(l == task_id and t == 'variant') for l, t in zip(embed_labels, embed_types)])
                task_coords = coords[mask]
                
                if len(task_coords) == 0:
                    continue
                
                color = task_colors[task_id]
                short_name = task_id.split('_')[-1][:8]
                
                ax.scatter(task_coords[:, 0], task_coords[:, 1], marker='^', s=80,
                          c=[color], alpha=0.7, label=short_name)
            
            ax.legend(loc='upper right', fontsize=8, ncol=2, framealpha=0.9)
            ax.set_title('Variants vs Target Video (by Task, t-SNE)', fontsize=14, fontweight='bold')
            ax.set_xlabel('t-SNE Dimension 1')
            ax.set_ylabel('t-SNE Dimension 2')
            ax.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(output_dir / "viz_2b_variants_by_task.png", dpi=150, bbox_inches='tight')
            plt.close(fig)
            
            # å¤åˆ¶ä¸ºé»˜è®¤å›¾ï¼ˆå¦‚æœæœ‰åˆ†ç±»å›¾ç”¨åˆ†ç±»å›¾ï¼Œå¦åˆ™ç”¨ä»»åŠ¡å›¾ï¼‰
            import shutil
            if real_categories and (output_dir / "viz_2a_variants_by_category.png").exists():
                shutil.copy(output_dir / "viz_2a_variants_by_category.png", 
                           output_dir / "viz_2_variants_vs_visual.png")
            else:
                shutil.copy(output_dir / "viz_2b_variants_by_task.png", 
                           output_dir / "viz_2_variants_vs_visual.png")
            
        except Exception as e:
            logger.warning(f"    å›¾2ç”Ÿæˆå¤±è´¥: {e}")
    
    def _plot_prior_vs_visual(
        self,
        prior_embeds: np.ndarray,
        visual_embeds_by_task: Dict[str, List[np.ndarray]],
        task_ids: List[str],
        task_info: Dict,
        output_dir: Path
    ):
        """
        å›¾3: Prior vs ç›®æ ‡è§†é¢‘ (t-SNE)
        ç”Ÿæˆä¸¤å¼ å›¾ï¼šæŒ‰ Tier å’ŒæŒ‰ Category ç€è‰²
        """
        import matplotlib.pyplot as plt
        from sklearn.manifold import TSNE
        
        TIER_COLORS = {1: '#4CAF50', 2: '#FF9800', 3: '#F44336'}
        TIER_NAMES = {1: 'Tier 1', 2: 'Tier 2', 3: 'Tier 3'}
        CAT_COLORS = {'Harvest': '#4CAF50', 'Combat': '#F44336', 'Techtree': '#2196F3', 'Other': '#9E9E9E'}
        
        # æ”¶é›†æ‰€æœ‰æ•°æ®
        all_embeds = []
        embed_labels = []
        embed_types = []
        embed_tiers = []
        embed_cats = []
        
        for i, task_id in enumerate(task_ids):
            info = task_info.get(task_id, {'tier': 2, 'category': 'Other'})
            tier = info['tier']
            cat = info['category']
            
            # Prior åµŒå…¥
            all_embeds.append(prior_embeds[i])
            embed_labels.append(task_id)
            embed_types.append('prior')
            embed_tiers.append(tier)
            embed_cats.append(cat)
            
            # è§†è§‰åµŒå…¥
            visuals = visual_embeds_by_task.get(task_id, [])
            for v in visuals[:2]:
                all_embeds.append(v)
                embed_labels.append(task_id)
                embed_types.append('visual')
                embed_tiers.append(tier)
                embed_cats.append(cat)
        
        if len(all_embeds) < 3:
            logger.warning("    æ•°æ®ä¸è¶³ï¼Œè·³è¿‡å›¾3")
            return
        
        try:
            all_embeds = np.array(all_embeds)
            perplexity = min(30, max(2, len(all_embeds) - 1))
            tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
            coords = tsne.fit_transform(all_embeds)
            
            # === å›¾3a: æŒ‰ Tier ç€è‰² ===
            fig, ax = plt.subplots(figsize=(10, 8))
            for tier in sorted(set(embed_tiers)):
                color = TIER_COLORS.get(tier, '#9E9E9E')
                tier_name = TIER_NAMES.get(tier, f'Tier {tier}')
                
                prior_mask = np.array([(t == tier and et == 'prior') for t, et in zip(embed_tiers, embed_types)])
                prior_coords = coords[prior_mask]
                if len(prior_coords) > 0:
                    ax.scatter(prior_coords[:, 0], prior_coords[:, 1], marker='^', s=180,
                              c=[color], edgecolors='black', linewidth=1.5, 
                              label=f'Prior ({tier_name})', zorder=5)
                
                visual_mask = np.array([(t == tier and et == 'visual') for t, et in zip(embed_tiers, embed_types)])
                visual_coords = coords[visual_mask]
                if len(visual_coords) > 0:
                    ax.scatter(visual_coords[:, 0], visual_coords[:, 1], marker='o', s=50,
                              c=[color], alpha=0.4, label=f'Visual ({tier_name})')
            
            ax.legend(loc='upper right', fontsize=8, ncol=2, framealpha=0.9)
            ax.set_title('Prior vs Target Video (by Tier, t-SNE)', fontsize=14, fontweight='bold')
            ax.set_xlabel('t-SNE Dimension 1')
            ax.set_ylabel('t-SNE Dimension 2')
            ax.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(output_dir / "viz_3a_prior_vs_visual_tier.png", dpi=150, bbox_inches='tight')
            plt.close(fig)
            
            # === å›¾3b: æŒ‰ Category ç€è‰² ===
            fig, ax = plt.subplots(figsize=(10, 8))
            for cat in sorted(set(embed_cats)):
                color = CAT_COLORS.get(cat, '#9E9E9E')
                
                prior_mask = np.array([(c == cat and et == 'prior') for c, et in zip(embed_cats, embed_types)])
                prior_coords = coords[prior_mask]
                prior_labels = [l for l, m in zip(embed_labels, prior_mask) if m]
                if len(prior_coords) > 0:
                    ax.scatter(prior_coords[:, 0], prior_coords[:, 1], marker='^', s=180,
                              c=[color], edgecolors='black', linewidth=1.5, 
                              label=f'Prior ({cat})', zorder=5)
                    # æ·»åŠ æ ‡ç­¾
                    for j, (pc, label) in enumerate(zip(prior_coords, prior_labels)):
                        short_label = label.split('_')[-1][:5]
                        ax.annotate(short_label, (pc[0], pc[1]), fontsize=6, alpha=0.5,
                                   xytext=(3, 3), textcoords='offset points')
                
                visual_mask = np.array([(c == cat and et == 'visual') for c, et in zip(embed_cats, embed_types)])
                visual_coords = coords[visual_mask]
                if len(visual_coords) > 0:
                    ax.scatter(visual_coords[:, 0], visual_coords[:, 1], marker='o', s=50,
                              c=[color], alpha=0.4, label=f'Visual ({cat})')
            
            ax.legend(loc='upper right', fontsize=8, ncol=2, framealpha=0.9)
            ax.set_title('Prior vs Target Video (by Category, t-SNE)', fontsize=14, fontweight='bold')
            ax.set_xlabel('t-SNE Dimension 1')
            ax.set_ylabel('t-SNE Dimension 2')
            ax.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(output_dir / "viz_3b_prior_vs_visual_category.png", dpi=150, bbox_inches='tight')
            plt.close(fig)
            
        except Exception as e:
            logger.warning(f"    å›¾3ç”Ÿæˆå¤±è´¥: {e}")
    
    def _plot_similarity_matrix(
        self,
        prior_embeds: np.ndarray,
        task_ids: List[str],
        output_dir: Path
    ):
        """
        å›¾2: Prior è¾“å‡ºç›¸ä¼¼åº¦çŸ©é˜µ
        """
        import matplotlib.pyplot as plt
        from scipy.spatial.distance import cosine
        
        n = len(task_ids)
        sim_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                sim_matrix[i, j] = 1 - cosine(prior_embeds[i], prior_embeds[j])
        
        fig, ax = plt.subplots(figsize=(10, 8))
        im = ax.imshow(sim_matrix, cmap='RdYlGn', vmin=0.5, vmax=1.0)
        
        # è®¾ç½®æ ‡ç­¾
        short_labels = [t.split('_')[-1][:8] for t in task_ids]
        ax.set_xticks(range(n))
        ax.set_yticks(range(n))
        ax.set_xticklabels(short_labels, rotation=45, ha='right', fontsize=8)
        ax.set_yticklabels(short_labels, fontsize=8)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(n):
            for j in range(n):
                color = 'white' if sim_matrix[i, j] < 0.75 else 'black'
                ax.text(j, i, f'{sim_matrix[i, j]:.2f}', ha='center', va='center', 
                       fontsize=7, color=color)
        
        plt.colorbar(im, ax=ax, label='Cosine Similarity')
        ax.set_title('Prior Output Similarity Matrix', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_dir / "task_similarity_matrix.png", dpi=150, bbox_inches='tight')
        plt.close(fig)
    
    def _plot_variance_distribution(
        self,
        prior_embeds: np.ndarray,
        output_dir: Path
    ):
        """
        å›¾3: Prior è¾“å‡ºæ–¹å·®åˆ†å¸ƒ
        """
        import matplotlib.pyplot as plt
        
        # è®¡ç®—æ¯ä¸ªç»´åº¦çš„æ–¹å·®
        variances = np.var(prior_embeds, axis=0)
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # å·¦å›¾ï¼šæ–¹å·®åˆ†å¸ƒç›´æ–¹å›¾
        ax1 = axes[0]
        ax1.hist(variances, bins=50, color='#2196F3', alpha=0.7, edgecolor='white')
        ax1.axvline(np.mean(variances), color='red', linestyle='--', label=f'Mean: {np.mean(variances):.4f}')
        ax1.axvline(np.median(variances), color='orange', linestyle='--', label=f'Median: {np.median(variances):.4f}')
        ax1.set_xlabel('Variance')
        ax1.set_ylabel('Dimension Count')
        ax1.set_title('Prior Output Variance Distribution')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # å³å›¾ï¼šæ–¹å·®ç´¯ç§¯åˆ†å¸ƒ
        ax2 = axes[1]
        sorted_vars = np.sort(variances)[::-1]
        cumsum = np.cumsum(sorted_vars)
        cumsum_ratio = cumsum / cumsum[-1]
        ax2.plot(range(len(sorted_vars)), cumsum_ratio, color='#4CAF50', linewidth=2)
        ax2.axhline(0.9, color='red', linestyle='--', alpha=0.7, label='90% Variance')
        ax2.set_xlabel('Dimensions (sorted by variance)')
        ax2.set_ylabel('Cumulative Variance Ratio')
        ax2.set_title('Prior Output Cumulative Variance')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / "variance_distribution.png", dpi=150, bbox_inches='tight')
        plt.close(fig)
    
    def _plot_goal_progress_overview(
        self,
        goal_progress_data: Dict,
        output_dir: Path
    ):
        """
        å›¾4: ç›®æ ‡æ¥è¿‘åº¦æ¦‚è§ˆ
        å·¦å›¾ï¼šå„ä»»åŠ¡è¿›åº¦ç‡å¯¹æ¯”ï¼ˆä¸“å®¶ vs æ¨¡å‹ï¼‰
        å³å›¾ï¼šè¿›åº¦ç‡ vs å•è°ƒç‡æ•£ç‚¹å›¾
        """
        import matplotlib.pyplot as plt
        
        task_ids = list(goal_progress_data.keys())
        n = len(task_ids)
        
        if n == 0:
            return
        
        expert_progress = [goal_progress_data[t].get('expert_progress_rate', 0) for t in task_ids]
        model_progress = [goal_progress_data[t].get('model_progress_rate', 0) for t in task_ids]
        expert_monotonic = [goal_progress_data[t].get('expert_monotonic_rate', 0) for t in task_ids]
        model_monotonic = [goal_progress_data[t].get('model_monotonic_rate', 0) for t in task_ids]
        
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))
        
        # å·¦å›¾ï¼šè¿›åº¦ç‡å¯¹æ¯”æ¡å½¢å›¾
        ax1 = axes[0]
        x = np.arange(n)
        width = 0.35
        
        bars1 = ax1.bar(x - width/2, [p * 100 for p in expert_progress], width, 
                       label='Expert Baseline', color='#2196F3', alpha=0.8)
        bars2 = ax1.bar(x + width/2, [p * 100 for p in model_progress], width,
                       label='Policy', color='#4CAF50', alpha=0.8)
        
        ax1.axhline(0, color='black', linestyle='-', linewidth=0.5)
        ax1.set_xlabel('Task')
        ax1.set_ylabel('Progress Rate (%)')
        ax1.set_title('Goal Progress Rate Comparison')
        short_labels = [t.split('_')[-1][:10] for t in task_ids]
        ax1.set_xticks(x)
        ax1.set_xticklabels(short_labels, rotation=45, ha='right', fontsize=8)
        ax1.legend()
        ax1.grid(True, alpha=0.3, axis='y')
        
        # å³å›¾ï¼šè¿›åº¦ç‡ vs å•è°ƒç‡æ•£ç‚¹å›¾
        ax2 = axes[1]
        ax2.scatter([p * 100 for p in expert_progress], [m * 100 for m in expert_monotonic],
                   c='#2196F3', s=100, alpha=0.7, label='Expert Baseline', marker='o')
        ax2.scatter([p * 100 for p in model_progress], [m * 100 for m in model_monotonic],
                   c='#4CAF50', s=100, alpha=0.7, label='Policy', marker='^')
        
        # æ·»åŠ ä»»åŠ¡æ ‡ç­¾
        for i, task in enumerate(task_ids):
            short_name = task.split('_')[-1][:6]
            ax2.annotate(short_name, (expert_progress[i] * 100, expert_monotonic[i] * 100),
                        fontsize=7, alpha=0.7)
        
        ax2.axhline(50, color='gray', linestyle='--', alpha=0.5)
        ax2.axvline(0, color='gray', linestyle='--', alpha=0.5)
        ax2.set_xlabel('Progress Rate (%)')
        ax2.set_ylabel('Monotonic Rate (%)')
        ax2.set_title('Progress Rate vs Monotonic Rate')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / "viz_5_goal_progress_overview.png", dpi=150, bbox_inches='tight')
        plt.close(fig)
    
    def _plot_action_distribution(
        self,
        expert_dist: Dict[str, float],
        model_dist: Dict[str, float],
        output_dir: Path
    ):
        """
        å›¾5: åŠ¨ä½œåˆ†å¸ƒå¯¹æ¯”
        æ˜¾ç¤ºä¸“å®¶å’Œæ¨¡å‹çš„åŠ¨ä½œç±»å‹åˆ†å¸ƒ
        """
        import matplotlib.pyplot as plt
        
        # å½’ä¸€åŒ–åˆ†å¸ƒ
        expert_total = sum(expert_dist.values()) or 1
        model_total = sum(model_dist.values()) or 1
        
        expert_dist = {k: v / expert_total for k, v in expert_dist.items()}
        model_dist = {k: v / model_total for k, v in model_dist.items()}
        
        # æ‰€æœ‰åŠ¨ä½œç±»å‹
        all_actions = sorted(set(expert_dist.keys()) | set(model_dist.keys()))
        
        if not all_actions:
            return
        
        fig, ax = plt.subplots(figsize=(12, 6))
        
        x = np.arange(len(all_actions))
        width = 0.35
        
        expert_vals = [expert_dist.get(a, 0) for a in all_actions]
        model_vals = [model_dist.get(a, 0) for a in all_actions]
        
        ax.bar(x - width/2, expert_vals, width, label='Expert', color='#2196F3', alpha=0.8)
        ax.bar(x + width/2, model_vals, width, label='Model', color='#FF9800', alpha=0.8)
        
        ax.set_xticks(x)
        ax.set_xticklabels(all_actions, rotation=45, ha='right')
        ax.set_ylabel('Proportion', fontsize=12)
        ax.set_xlabel('Action Type', fontsize=12)
        ax.set_title('Action Distribution: Expert vs Model', fontsize=14, fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.savefig(output_dir / "viz_6_action_distribution.png", dpi=150, bbox_inches='tight')
        plt.close(fig)
    
    def _plot_confusion_matrix(
        self,
        expert_actions: List[str],
        model_actions: List[str],
        output_dir: Path
    ):
        """
        å›¾6: åŠ¨ä½œæ··æ·†çŸ©é˜µ
        æ˜¾ç¤ºæ¨¡å‹é¢„æµ‹ä¸ä¸“å®¶åŠ¨ä½œçš„åŒ¹é…æƒ…å†µ
        """
        import matplotlib.pyplot as plt
        
        if not expert_actions or not model_actions:
            return
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        min_len = min(len(expert_actions), len(model_actions))
        expert_actions = expert_actions[:min_len]
        model_actions = model_actions[:min_len]
        
        # æ„å»ºæ··æ·†çŸ©é˜µ
        all_types = sorted(set(expert_actions) | set(model_actions))
        n = len(all_types)
        
        if n < 2:
            return
        
        matrix = np.zeros((n, n))
        type_to_idx = {t: i for i, t in enumerate(all_types)}
        
        for e, m in zip(expert_actions, model_actions):
            matrix[type_to_idx[e], type_to_idx[m]] += 1
        
        # å½’ä¸€åŒ–ï¼ˆæŒ‰è¡Œï¼‰
        row_sums = matrix.sum(axis=1, keepdims=True)
        matrix_normalized = np.divide(matrix, row_sums, where=row_sums > 0)
        
        fig, ax = plt.subplots(figsize=(10, 8))
        
        im = ax.imshow(matrix_normalized, cmap='Blues')
        
        ax.set_xticks(range(n))
        ax.set_yticks(range(n))
        ax.set_xticklabels(all_types, rotation=45, ha='right')
        ax.set_yticklabels(all_types)
        
        # æ·»åŠ æ•°å€¼
        for i in range(n):
            for j in range(n):
                val = matrix_normalized[i, j]
                color = 'white' if val > 0.5 else 'black'
                ax.text(j, i, f'{val:.2f}', ha='center', va='center', color=color, fontsize=9)
        
        ax.set_xlabel('Model Prediction', fontsize=12)
        ax.set_ylabel('Expert Action', fontsize=12)
        ax.set_title('Action Confusion Matrix (Row Normalized)', fontsize=14, fontweight='bold')
        
        plt.colorbar(im, ax=ax, label='Proportion')
        plt.tight_layout()
        
        plt.savefig(output_dir / "viz_7_confusion_matrix.png", dpi=150, bbox_inches='tight')
        plt.close(fig)
    
    def _plot_similarity_timeline(
        self,
        frame_similarities: List[float],
        camera_similarities: List[float],
        task_id: str,
        output_dir: Path
    ):
        """
        å›¾7: é€å¸§ç›¸ä¼¼åº¦æ—¶é—´çº¿
        æ˜¾ç¤ºåŠ¨ä½œç›¸ä¼¼åº¦å’Œ Camera ç›¸ä¼¼åº¦éšæ—¶é—´çš„å˜åŒ–
        """
        import matplotlib.pyplot as plt
        
        n = len(frame_similarities)
        if n < 5:
            return
        
        steps = list(range(n))
        
        fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)
        
        # 1. Action ç›¸ä¼¼åº¦
        ax1 = axes[0]
        ax1.fill_between(steps, frame_similarities, alpha=0.3, color='#2196F3')
        ax1.plot(steps, frame_similarities, color='#2196F3', linewidth=1.5, label='Action Similarity')
        
        # æ·»åŠ ç§»åŠ¨å¹³å‡çº¿
        window = min(20, n // 5) if n > 20 else 5
        if n > window:
            action_ma = np.convolve(frame_similarities, np.ones(window)/window, mode='valid')
            ax1.plot(range(window-1, n), action_ma, color='#1565C0', linewidth=2,
                    linestyle='--', label=f'Moving Avg ({window})')
        
        ax1.set_ylabel('Action Similarity', fontsize=11)
        ax1.set_ylim(0, 1.05)
        ax1.set_title(f'Frame-wise Similarity Timeline: {task_id}', fontsize=12, fontweight='bold')
        ax1.legend(loc='lower right')
        ax1.grid(True, alpha=0.3)
        
        # è®¡ç®—å¹¶æ˜¾ç¤ºå¹³å‡å€¼
        avg_action = np.mean(frame_similarities)
        ax1.axhline(y=avg_action, color='#F44336', linestyle=':', linewidth=1.5, alpha=0.8)
        ax1.text(n * 0.02, avg_action + 0.03, f'Avg: {avg_action:.1%}', color='#F44336', fontsize=10)
        
        # 2. Camera ç›¸ä¼¼åº¦
        ax2 = axes[1]
        if camera_similarities:
            camera_n = len(camera_similarities)
            ax2.fill_between(range(camera_n), camera_similarities, alpha=0.3, color='#4CAF50')
            ax2.plot(range(camera_n), camera_similarities, color='#4CAF50', linewidth=1.5, label='Camera Similarity')
            
            # æ·»åŠ ç§»åŠ¨å¹³å‡çº¿
            if camera_n > window:
                camera_ma = np.convolve(camera_similarities, np.ones(window)/window, mode='valid')
                ax2.plot(range(window-1, camera_n), camera_ma, color='#2E7D32', linewidth=2,
                        linestyle='--', label=f'Moving Avg ({window})')
            
            avg_camera = np.mean(camera_similarities)
            ax2.axhline(y=avg_camera, color='#F44336', linestyle=':', linewidth=1.5, alpha=0.8)
            ax2.text(camera_n * 0.02, avg_camera + 0.03, f'Avg: {avg_camera:.1%}', color='#F44336', fontsize=10)
        
        ax2.set_ylabel('Camera Similarity', fontsize=11)
        ax2.set_xlabel('Frame Index', fontsize=11)
        ax2.set_ylim(0, 1.05)
        ax2.legend(loc='lower right')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / f"viz_8_similarity_timeline_{task_id}.png", dpi=150, bbox_inches='tight')
        plt.close(fig)
    
    def _plot_goal_progress_comparison(
        self,
        expert_distances: List[float],
        model_distances: List[float],
        task_id: str,
        output_dir: Path
    ):
        """
        å›¾8: ç›®æ ‡æ¥è¿‘åº¦å¯¹æ¯”å›¾
        å¯¹æ¯”ä¸“å®¶å’Œæ¨¡å‹çš„ç›®æ ‡è·ç¦»å˜åŒ–
        """
        import matplotlib.pyplot as plt
        
        if not expert_distances or not model_distances:
            return
        
        fig, ax = plt.subplots(figsize=(14, 6))
        
        # ä¸“å®¶è·ç¦»
        expert_n = len(expert_distances)
        expert_steps = np.linspace(0, 100, expert_n)  # å½’ä¸€åŒ–åˆ° 0-100%
        ax.plot(expert_steps, expert_distances, color='#2196F3', linewidth=2, 
               label='Expert Baseline', alpha=0.9)
        ax.fill_between(expert_steps, expert_distances, alpha=0.15, color='#2196F3')
        
        # æ¨¡å‹è·ç¦»
        model_n = len(model_distances)
        model_steps = np.linspace(0, 100, model_n)  # å½’ä¸€åŒ–åˆ° 0-100%
        ax.plot(model_steps, model_distances, color='#4CAF50', linewidth=2,
               label='Policy Model', alpha=0.9)
        ax.fill_between(model_steps, model_distances, alpha=0.15, color='#4CAF50')
        
        # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
        ax.scatter([0], [expert_distances[0]], color='#2196F3', s=80, marker='s', zorder=5, label='Expert Start')
        ax.scatter([100], [expert_distances[-1]], color='#2196F3', s=80, marker='D', zorder=5, label='Expert End')
        ax.scatter([0], [model_distances[0]], color='#4CAF50', s=80, marker='s', zorder=5, label='Model Start')
        ax.scatter([100], [model_distances[-1]], color='#4CAF50', s=80, marker='D', zorder=5, label='Model End')
        
        # è®¡ç®—è¿›åº¦ç‡
        expert_progress = (expert_distances[0] - expert_distances[-1]) / expert_distances[0] if expert_distances[0] > 1e-6 else 0
        model_progress = (model_distances[0] - model_distances[-1]) / model_distances[0] if model_distances[0] > 1e-6 else 0
        
        # æ·»åŠ ä¿¡æ¯æ¡†
        info_text = f'Expert Progress: {expert_progress:+.1%}\nModel Progress: {model_progress:+.1%}'
        ax.text(0.98, 0.98, info_text, transform=ax.transAxes,
               fontsize=11, fontweight='bold',
               ha='right', va='top', 
               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='#ddd'))
        
        ax.set_xlabel('Progress (%)', fontsize=11)
        ax.set_ylabel('Distance to Goal\n(lower is better)', fontsize=11)
        ax.set_title(f'Goal Progress Comparison: {task_id}', fontsize=12, fontweight='bold')
        ax.legend(loc='upper right', fontsize=8, ncol=2)
        ax.grid(True, alpha=0.3)
        ax.set_ylim(bottom=0)
        
        plt.tight_layout()
        plt.savefig(output_dir / f"viz_9_goal_comparison_{task_id}.png", dpi=150, bbox_inches='tight')
        plt.close(fig)
    
    def _plot_similarity_timeline_aggregated(
        self,
        all_frame_similarities: Dict[str, List[float]],
        all_camera_similarities: Dict[str, List[float]],
        output_dir: Path
    ):
        """
        å›¾7: é€å¸§ç›¸ä¼¼åº¦æ—¶é—´çº¿ï¼ˆæ‰€æœ‰ä»»åŠ¡æ±‡æ€»ï¼‰
        æ˜¾ç¤ºæ‰€æœ‰ä»»åŠ¡æ‰€æœ‰ trial çš„å¹³å‡ç›¸ä¼¼åº¦è¶‹åŠ¿
        """
        import matplotlib.pyplot as plt
        
        # æ”¶é›†æ‰€æœ‰ä»»åŠ¡çš„æ•°æ®é•¿åº¦
        all_lengths = [len(sims) for sims in all_frame_similarities.values()]
        if not all_lengths or max(all_lengths) < 5:
            return
        
        # å½’ä¸€åŒ–åˆ°ç›¸åŒé•¿åº¦ï¼ˆ100ä¸ªç‚¹ï¼‰è¿›è¡Œå¹³å‡
        NORM_LEN = 100
        
        def normalize_and_average(all_sims_dict: Dict[str, List[float]]) -> np.ndarray:
            """å°†ä¸åŒé•¿åº¦çš„åºåˆ—å½’ä¸€åŒ–åˆ°ç›¸åŒé•¿åº¦åå¹³å‡"""
            normalized_all = []
            for task_id, sims in all_sims_dict.items():
                if len(sims) < 2:
                    continue
                # æ’å€¼åˆ° NORM_LEN ä¸ªç‚¹
                x_old = np.linspace(0, 1, len(sims))
                x_new = np.linspace(0, 1, NORM_LEN)
                normalized = np.interp(x_new, x_old, sims)
                normalized_all.append(normalized)
            
            if not normalized_all:
                return np.array([])
            return np.mean(normalized_all, axis=0)
        
        # è®¡ç®—å¹³å‡è¶‹åŠ¿
        avg_action_sim = normalize_and_average(all_frame_similarities)
        avg_camera_sim = normalize_and_average(all_camera_similarities) if all_camera_similarities else np.array([])
        
        if len(avg_action_sim) < 5:
            return
        
        n_tasks = len(all_frame_similarities)
        total_frames = sum(len(sims) for sims in all_frame_similarities.values())
        
        fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)
        steps = np.arange(NORM_LEN)
        
        # 1. Action ç›¸ä¼¼åº¦
        ax1 = axes[0]
        ax1.fill_between(steps, avg_action_sim, alpha=0.3, color='#2196F3')
        ax1.plot(steps, avg_action_sim, color='#2196F3', linewidth=2, label='Avg Action Similarity')
        
        # æ·»åŠ ç§»åŠ¨å¹³å‡çº¿
        window = 10
        action_ma = np.convolve(avg_action_sim, np.ones(window)/window, mode='valid')
        ax1.plot(range(window-1, NORM_LEN), action_ma, color='#1565C0', linewidth=2,
                linestyle='--', label=f'Moving Avg ({window})')
        
        ax1.set_ylabel('Action Similarity', fontsize=11)
        ax1.set_ylim(0, 1.05)
        ax1.set_title(f'Frame-wise Similarity Timeline (All Tasks Aggregated: {n_tasks} tasks, {total_frames} frames)', 
                     fontsize=12, fontweight='bold')
        ax1.legend(loc='lower right')
        ax1.grid(True, alpha=0.3)
        
        # æ˜¾ç¤ºæ•´ä½“å¹³å‡å€¼
        overall_avg = np.mean(avg_action_sim)
        ax1.axhline(y=overall_avg, color='#F44336', linestyle=':', linewidth=1.5, alpha=0.8)
        ax1.text(NORM_LEN * 0.02, overall_avg + 0.03, f'Overall Avg: {overall_avg:.1%}', 
                color='#F44336', fontsize=10)
        
        # 2. Camera ç›¸ä¼¼åº¦
        ax2 = axes[1]
        if len(avg_camera_sim) > 0:
            ax2.fill_between(steps, avg_camera_sim, alpha=0.3, color='#4CAF50')
            ax2.plot(steps, avg_camera_sim, color='#4CAF50', linewidth=2, label='Avg Camera Similarity')
            
            camera_ma = np.convolve(avg_camera_sim, np.ones(window)/window, mode='valid')
            ax2.plot(range(window-1, NORM_LEN), camera_ma, color='#2E7D32', linewidth=2,
                    linestyle='--', label=f'Moving Avg ({window})')
            
            camera_avg = np.mean(avg_camera_sim)
            ax2.axhline(y=camera_avg, color='#F44336', linestyle=':', linewidth=1.5, alpha=0.8)
            ax2.text(NORM_LEN * 0.02, camera_avg + 0.03, f'Overall Avg: {camera_avg:.1%}', 
                    color='#F44336', fontsize=10)
        
        ax2.set_ylabel('Camera Similarity', fontsize=11)
        ax2.set_xlabel('Normalized Progress (%)', fontsize=11)
        ax2.set_ylim(0, 1.05)
        ax2.legend(loc='lower right')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(output_dir / "viz_8_similarity_timeline_aggregated.png", dpi=150, bbox_inches='tight')
        plt.close(fig)
    
    def _plot_goal_progress_comparison_aggregated(
        self,
        all_expert_distances: Dict[str, List[float]],
        all_model_distances: Dict[str, List[float]],
        output_dir: Path
    ):
        """
        å›¾8: ç›®æ ‡æ¥è¿‘åº¦å¯¹æ¯”å›¾ï¼ˆæ‰€æœ‰ä»»åŠ¡æ±‡æ€»ï¼‰
        å¯¹æ¯”æ‰€æœ‰ä»»åŠ¡çš„ä¸“å®¶å’Œæ¨¡å‹å¹³å‡ç›®æ ‡è·ç¦»å˜åŒ–
        """
        import matplotlib.pyplot as plt
        
        # æ‰¾åˆ°å…±åŒçš„ä»»åŠ¡
        common_tasks = set(all_expert_distances.keys()) & set(all_model_distances.keys())
        if not common_tasks:
            return
        
        # å½’ä¸€åŒ–åˆ°ç›¸åŒé•¿åº¦ï¼ˆ100ä¸ªç‚¹ï¼‰
        NORM_LEN = 100
        
        def normalize_to_length(distances: List[float], target_len: int) -> np.ndarray:
            """å°†åºåˆ—æ’å€¼åˆ°ç›®æ ‡é•¿åº¦"""
            if len(distances) < 2:
                return np.array([distances[0]] * target_len if distances else [0] * target_len)
            x_old = np.linspace(0, 1, len(distances))
            x_new = np.linspace(0, 1, target_len)
            return np.interp(x_new, x_old, distances)
        
        # æ”¶é›†å¹¶å½’ä¸€åŒ–æ‰€æœ‰æ•°æ®
        expert_normalized = []
        model_normalized = []
        
        for task_id in common_tasks:
            expert_norm = normalize_to_length(all_expert_distances[task_id], NORM_LEN)
            model_norm = normalize_to_length(all_model_distances[task_id], NORM_LEN)
            expert_normalized.append(expert_norm)
            model_normalized.append(model_norm)
        
        # è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
        expert_mean = np.mean(expert_normalized, axis=0)
        expert_std = np.std(expert_normalized, axis=0)
        model_mean = np.mean(model_normalized, axis=0)
        model_std = np.std(model_normalized, axis=0)
        
        fig, ax = plt.subplots(figsize=(14, 6))
        progress = np.linspace(0, 100, NORM_LEN)
        
        # ä¸“å®¶å¹³å‡è·ç¦»ï¼ˆå¸¦ç½®ä¿¡åŒºé—´ï¼‰
        ax.plot(progress, expert_mean, color='#2196F3', linewidth=2.5, 
               label=f'Expert Baseline (n={len(common_tasks)})', alpha=0.9)
        ax.fill_between(progress, expert_mean - expert_std, expert_mean + expert_std,
                       alpha=0.15, color='#2196F3')
        
        # æ¨¡å‹å¹³å‡è·ç¦»ï¼ˆå¸¦ç½®ä¿¡åŒºé—´ï¼‰
        ax.plot(progress, model_mean, color='#4CAF50', linewidth=2.5,
               label=f'Policy Model (n={len(common_tasks)})', alpha=0.9)
        ax.fill_between(progress, model_mean - model_std, model_mean + model_std,
                       alpha=0.15, color='#4CAF50')
        
        # æ ‡è®°èµ·ç‚¹å’Œç»ˆç‚¹
        ax.scatter([0], [expert_mean[0]], color='#2196F3', s=100, marker='s', zorder=5)
        ax.scatter([100], [expert_mean[-1]], color='#2196F3', s=100, marker='D', zorder=5)
        ax.scatter([0], [model_mean[0]], color='#4CAF50', s=100, marker='s', zorder=5)
        ax.scatter([100], [model_mean[-1]], color='#4CAF50', s=100, marker='D', zorder=5)
        
        # è®¡ç®—å¹³å‡è¿›åº¦ç‡
        expert_progress = (expert_mean[0] - expert_mean[-1]) / expert_mean[0] if expert_mean[0] > 1e-6 else 0
        model_progress = (model_mean[0] - model_mean[-1]) / model_mean[0] if model_mean[0] > 1e-6 else 0
        
        # æ·»åŠ ä¿¡æ¯æ¡†
        info_text = (f'Avg Expert Progress: {expert_progress:+.1%}\n'
                    f'Avg Model Progress: {model_progress:+.1%}\n'
                    f'Tasks: {len(common_tasks)}')
        ax.text(0.98, 0.98, info_text, transform=ax.transAxes,
               fontsize=11, fontweight='bold',
               ha='right', va='top', 
               bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='#ddd'))
        
        ax.set_xlabel('Progress (%)', fontsize=11)
        ax.set_ylabel('Distance to Goal\n(lower is better)', fontsize=11)
        ax.set_title(f'Goal Progress Comparison (All Tasks Aggregated)', fontsize=12, fontweight='bold')
        ax.legend(loc='upper right', fontsize=10)
        ax.grid(True, alpha=0.3)
        ax.set_ylim(bottom=0)
        
        plt.tight_layout()
        plt.savefig(output_dir / "viz_9_goal_comparison_aggregated.png", dpi=150, bbox_inches='tight')
        plt.close(fig)
    
    def _collect_combined_results(self, output_dir: Path) -> Dict[str, Dict]:
        """
        ä»ä»»åŠ¡è¾“å‡ºç›®å½•æ”¶é›†ç»¼åˆè¯„ä¼°ç»“æœ
        
        æ”¯æŒä¸¤ç§ç›®å½•ç»“æ„ï¼š
        1. å•ä»»åŠ¡: output_dir/combined_evaluation.json
        2. å¤šä»»åŠ¡: output_dir/{task_dir}/combined_evaluation.json
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            Dict[task_id, combined_result]
        """
        combined_results = {}
        
        # é¦–å…ˆæ£€æŸ¥æ ¹ç›®å½•æ˜¯å¦æœ‰ combined_evaluation.jsonï¼ˆå•ä»»åŠ¡è¯„ä¼°ï¼‰
        root_combined_file = output_dir / "combined_evaluation.json"
        if root_combined_file.exists():
            try:
                with open(root_combined_file, 'r', encoding='utf-8') as f:
                    combined = json.load(f)
                    task_id = combined.get('task_id', output_dir.name.split('_')[0])
                    combined_results[task_id] = combined
                    logger.debug(f"  åŠ è½½ {task_id} çš„ç»¼åˆè¯„ä¼°ç»“æœ (æ ¹ç›®å½•)")
            except Exception as e:
                logger.warning(f"  åŠ è½½ {root_combined_file} å¤±è´¥: {e}")
        
        # æŸ¥æ‰¾æ‰€æœ‰å­ä»»åŠ¡ç›®å½•ï¼ˆå¤šä»»åŠ¡è¯„ä¼°ï¼‰
        for task_dir in output_dir.iterdir():
            if not task_dir.is_dir():
                continue
            
            # è·³è¿‡ checkpoints ç­‰éä»»åŠ¡ç›®å½•
            if task_dir.name in ['checkpoints', '.DS_Store']:
                continue
            
            combined_file = task_dir / "combined_evaluation.json"
            if combined_file.exists():
                try:
                    with open(combined_file, 'r', encoding='utf-8') as f:
                        combined = json.load(f)
                        task_id = combined.get('task_id', task_dir.name.split('_')[0])
                        combined_results[task_id] = combined
                        logger.info(f"  åŠ è½½ {task_id} çš„ç»¼åˆè¯„ä¼°ç»“æœ: action={combined.get('action_similarity_metrics', {}).get('enabled')}, goal={combined.get('goal_progress_metrics', {}).get('enabled')}")
                except Exception as e:
                    logger.warning(f"  åŠ è½½ {combined_file} å¤±è´¥: {e}")
        
        logger.info(f"æ”¶é›†åˆ° {len(combined_results)} ä¸ªä»»åŠ¡çš„ç»¼åˆè¯„ä¼°ç»“æœ")
        return combined_results
    
    def _compute_goal_progress_summary(self, goal_progress_data: Dict) -> Dict:
        """
        è®¡ç®— goal_progress æ•°æ®çš„æ±‡æ€»ç»Ÿè®¡
        
        Args:
            goal_progress_data: æ¯ä¸ªä»»åŠ¡çš„ goal_progress æ•°æ®
            
        Returns:
            æ±‡æ€»ç»Ÿè®¡å­—å…¸
        """
        if not goal_progress_data:
            return {
                'avg_expert_progress_rate': 0,
                'avg_expert_monotonic_rate': 0,
                'avg_model_progress_rate': 0,
                'avg_model_monotonic_rate': 0,
                'avg_action_similarity': 0,
                'n_tasks': 0
            }
        
        expert_progress_rates = []
        expert_monotonic_rates = []
        model_progress_rates = []
        model_monotonic_rates = []
        action_similarities = []
        
        for task_id, data in goal_progress_data.items():
            # ä¸“å®¶æŒ‡æ ‡
            if data.get('expert_progress_rate', 0) != 0 or data.get('expert_monotonic_rate', 0) != 0:
                expert_progress_rates.append(data.get('expert_progress_rate', 0))
                expert_monotonic_rates.append(data.get('expert_monotonic_rate', 0))
            
            # æ¨¡å‹æŒ‡æ ‡
            if data.get('model_progress_rate', 0) != 0 or data.get('model_monotonic_rate', 0) != 0:
                model_progress_rates.append(data.get('model_progress_rate', 0))
                model_monotonic_rates.append(data.get('model_monotonic_rate', 0))
            
            # åŠ¨ä½œç›¸ä¼¼åº¦
            action_sim = data.get('action_similarity', 0)
            if action_sim > 0:
                action_similarities.append(action_sim)
        
        # è®¡ç®—è¾…åŠ©æŒ‡æ ‡
        camera_similarities = []
        action_entropies = []
        temporal_smoothnesses = []
        action_coverages = []
        
        for task_id, data in goal_progress_data.items():
            if data.get('camera_similarity', 0) > 0:
                camera_similarities.append(data.get('camera_similarity', 0))
            if data.get('action_entropy', 0) > 0:
                action_entropies.append(data.get('action_entropy', 0))
            if data.get('temporal_smoothness', 0) > 0:
                temporal_smoothnesses.append(data.get('temporal_smoothness', 0))
            if data.get('action_coverage', 0) > 0:
                action_coverages.append(data.get('action_coverage', 0))
        
        return {
            'avg_expert_progress_rate': np.mean(expert_progress_rates) if expert_progress_rates else 0,
            'avg_expert_monotonic_rate': np.mean(expert_monotonic_rates) if expert_monotonic_rates else 0,
            'avg_model_progress_rate': np.mean(model_progress_rates) if model_progress_rates else 0,
            'avg_model_monotonic_rate': np.mean(model_monotonic_rates) if model_monotonic_rates else 0,
            'avg_action_similarity': np.mean(action_similarities) if action_similarities else 0,
            # è¾…åŠ©æŒ‡æ ‡
            'avg_camera_similarity': np.mean(camera_similarities) if camera_similarities else 0,
            'avg_action_entropy': np.mean(action_entropies) if action_entropies else 0,
            'avg_temporal_smoothness': np.mean(temporal_smoothnesses) if temporal_smoothnesses else 0,
            'avg_action_coverage': np.mean(action_coverages) if action_coverages else 0,
            'n_tasks': len(goal_progress_data)
        }
    
    def _compute_auxiliary_metrics(
        self,
        combined_results: Dict[str, Dict],
        prior_gain_data: Dict,
        goal_progress_data: Dict
    ) -> Dict:
        """
        è®¡ç®—è¾…åŠ©æŒ‡æ ‡
        
        Args:
            combined_results: ç»¼åˆè¯„ä¼°ç»“æœ
            prior_gain_data: Prior å¢ç›Šæ•°æ®
            goal_progress_data: Goal Progress æ•°æ®
            
        Returns:
            è¾…åŠ©æŒ‡æ ‡å­—å…¸
        """
        # Prior è¾…åŠ©æŒ‡æ ‡
        prior_goal_accuracies = []
        prior_consistencies = []
        
        for task_id, combined in combined_results.items():
            prior_metrics = combined.get('prior_metrics', {})
            if prior_metrics.get('enabled'):
                prior_goal_accuracies.append(prior_metrics.get('goal_accuracy', 0))
                prior_consistencies.append(prior_metrics.get('consistency', 0))
        
        # è®¡ç®— Prior è¾“å‡ºçš„å‡å€¼æ–¹å·®ï¼ˆè·¨ä»»åŠ¡ï¼‰
        prior_mean_variance = 0.0
        if prior_goal_accuracies:
            prior_mean_variance = float(np.var(prior_goal_accuracies))
        
        # è®¡ç®— Prior åŒºåˆ†åº¦ï¼ˆ1 - å¹³å‡ç›¸ä¼¼åº¦ï¼‰
        # è¿™é‡Œç®€åŒ–è®¡ç®—ï¼šä½¿ç”¨ goal_accuracy çš„æ ‡å‡†å·®ä½œä¸ºåŒºåˆ†åº¦çš„ä»£ç†
        prior_discriminability = 0.0
        if len(prior_goal_accuracies) > 1:
            # åŒºåˆ†åº¦ï¼šä¸åŒä»»åŠ¡ä¹‹é—´ goal_accuracy çš„å·®å¼‚ç¨‹åº¦
            prior_discriminability = float(np.std(prior_goal_accuracies))
        
        # æ„å»ºæ¯ä¸ªä»»åŠ¡çš„è¾…åŠ©æŒ‡æ ‡
        task_auxiliary = {}
        for task_id, combined in combined_results.items():
            prior_metrics = combined.get('prior_metrics', {})
            action_metrics = combined.get('action_similarity_metrics', {})
            policy_result = combined.get('policy_result', {})
            
            task_auxiliary[task_id] = {
                # Prior è¾…åŠ©æŒ‡æ ‡
                'prior_variant_alignment': prior_metrics.get('semantic_robustness', 0) or 0,  # Prior å˜ä½“
                'prior_discriminability': prior_discriminability,  # å…¨å±€åŒºåˆ†åº¦ï¼ˆæ‰€æœ‰ä»»åŠ¡å…±äº«ï¼‰
                'prior_goal_accuracy_std': prior_metrics.get('goal_accuracy_std', 0),
                'prior_goal_accuracy': prior_metrics.get('goal_accuracy', 0),
                'prior_consistency': prior_metrics.get('consistency', 0),
                # Policy è¾…åŠ©æŒ‡æ ‡
                'action_similarity': action_metrics.get('action_similarity', 0),
                'camera_similarity': action_metrics.get('camera_similarity', 0),
                'action_entropy': action_metrics.get('action_entropy', 0),
                'temporal_smoothness': action_metrics.get('temporal_smoothness', 0),
                'action_coverage': action_metrics.get('action_coverage', 0),
                # æˆåŠŸç‡
                'success_rate': policy_result.get('success_rate', 0)
            }
        
        # Policy è¾…åŠ©æŒ‡æ ‡æ±‡æ€»
        goal_summary = self._compute_goal_progress_summary(goal_progress_data)
        
        # è®¡ç®—å¹³å‡ Prior å˜ä½“å¯¹é½åº¦
        prior_variant_alignments = [
            combined.get('prior_metrics', {}).get('semantic_robustness', 0) or 0
            for combined in combined_results.values()
            if combined.get('prior_metrics', {}).get('semantic_robustness') is not None
        ]
        avg_prior_variant_alignment = float(np.mean(prior_variant_alignments)) if prior_variant_alignments else 0
        
        # è®¡ç®—æ¯ä¸ªä»»åŠ¡ goal_accuracy_std çš„å¹³å‡å€¼
        prior_goal_accuracy_stds = [
            combined.get('prior_metrics', {}).get('goal_accuracy_std', 0)
            for combined in combined_results.values()
            if combined.get('prior_metrics', {}).get('enabled', False)
        ]
        avg_prior_goal_accuracy_std = float(np.mean(prior_goal_accuracy_stds)) if prior_goal_accuracy_stds else 0
        
        return {
            # Prior è¾…åŠ©æŒ‡æ ‡æ±‡æ€»
            'prior_mean_variance': avg_prior_goal_accuracy_std,  # æ¯ä¸ªä»»åŠ¡è¾“å‡ºæ–¹å·®çš„å¹³å‡
            'avg_prior_discriminability': prior_discriminability,  # æ·»åŠ  avg_ å‰ç¼€
            'prior_discriminability': prior_discriminability,  # ä¿æŒå…¼å®¹
            'avg_prior_variant_alignment': avg_prior_variant_alignment,
            'avg_prior_consistency': np.mean(prior_consistencies) if prior_consistencies else 0,
            
            # Policy è¾…åŠ©æŒ‡æ ‡æ±‡æ€»ï¼ˆä» goal_summary è·å–ï¼‰
            'avg_action_similarity': goal_summary.get('avg_action_similarity', 0),
            'avg_camera_similarity': goal_summary.get('avg_camera_similarity', 0),
            'avg_action_entropy': goal_summary.get('avg_action_entropy', 0),
            'avg_temporal_smoothness': goal_summary.get('avg_temporal_smoothness', 0),
            'avg_action_coverage': goal_summary.get('avg_action_coverage', 0),
            
            # æ¯ä¸ªä»»åŠ¡çš„è¾…åŠ©æŒ‡æ ‡
            'task_auxiliary': task_auxiliary
        }
    
    def _build_report_data(
        self, 
        results: List[TaskResult],
        combined_results: Dict[str, Dict]
    ) -> Dict:
        """
        æ„å»º PriorHTMLGenerator å…¼å®¹çš„æŠ¥å‘Šæ•°æ®ç»“æ„
        
        Args:
            results: TaskResult åˆ—è¡¨
            combined_results: ç»¼åˆè¯„ä¼°ç»“æœ
            
        Returns:
            Dict æŠ¥å‘Šæ•°æ®
        """
        config_filename = Path(self.config.task_config_path).name
        task_ids = [r.task_id for r in results]
        
        # æ„å»º output_quality æ•°æ®ï¼ˆPrior æŒ‡æ ‡ï¼‰
        # å­—æ®µåè¦ä¸ HTML ç”Ÿæˆå™¨åŒ¹é…: alignment_text, alignment_prior
        prior_gain_data = {}
        for task_id, combined in combined_results.items():
            prior_metrics = combined.get('prior_metrics', {})
            if prior_metrics.get('enabled'):
                goal_accuracy = prior_metrics.get('goal_accuracy', 0)
                mineclip_baseline = prior_metrics.get('mineclip_baseline', 0)
                prior_gain_data[task_id] = {
                    'alignment_text': mineclip_baseline,  # MineCLIP åŸºçº¿
                    'alignment_prior': goal_accuracy,  # Prior è¾“å‡ºä¸ç›®æ ‡åµŒå…¥çš„ç›¸ä¼¼åº¦
                    'prior_sim': goal_accuracy,  # å…¼å®¹å­—æ®µ
                    'baseline_sim': mineclip_baseline,
                    'prior_gain': goal_accuracy - mineclip_baseline,  # Prior å¢ç›Š
                }
        
        # æ„å»º intrinsic_quality æ•°æ®ï¼ˆä¸€è‡´æ€§ã€é²æ£’æ€§ï¼‰
        # å­—æ®µåè¦ä¸ HTML ç”Ÿæˆå™¨åŒ¹é…
        consistency_data = {}
        robustness_data = {}
        for task_id, combined in combined_results.items():
            prior_metrics = combined.get('prior_metrics', {})
            if prior_metrics.get('enabled'):
                consistency_data[task_id] = prior_metrics.get('consistency', 0)
                semantic_robustness = prior_metrics.get('semantic_robustness')
                if semantic_robustness is not None:
                    goal_accuracy = prior_metrics.get('goal_accuracy', 0)
                    robustness_data[task_id] = {
                        'robustness': semantic_robustness,
                        'n_variants': prior_metrics.get('n_variants', 0),
                        # å˜ä½“å¯¹é½åº¦ = ä¸»æŒ‡ä»¤ç›¸ä¼¼åº¦ * é²æ£’æ€§
                        'variant_alignment': goal_accuracy * semantic_robustness
                    }
        
        # æ„å»º goal_progress æ•°æ®ï¼ˆPolicy æŒ‡æ ‡ï¼‰
        goal_progress_data = {}
        for task_id, combined in combined_results.items():
            action_metrics = combined.get('action_similarity_metrics', {})
            goal_metrics = combined.get('goal_progress_metrics', {})
            
            if action_metrics.get('enabled') or goal_metrics.get('enabled'):
                goal_progress_data[task_id] = {
                    # ä¸“å®¶åŸºçº¿
                    'expert_progress_rate': action_metrics.get('expert_progress_rate', 0),
                    'expert_monotonic_rate': action_metrics.get('expert_monotonic_rate', 0),
                    'expert_initial_distance': action_metrics.get('expert_initial_distance', 0),
                    'expert_final_distance': action_metrics.get('expert_final_distance', 0),
                    # æ¨¡å‹æ¥è¿‘åº¦
                    'model_progress_rate': goal_metrics.get('model_progress_rate', 0),
                    'model_monotonic_rate': goal_metrics.get('model_monotonic_rate', 0),
                    'model_initial_distance': goal_metrics.get('model_initial_distance', 0),
                    'model_final_distance': goal_metrics.get('model_final_distance', 0),
                    # åŠ¨ä½œç›¸ä¼¼åº¦
                    'action_similarity': action_metrics.get('action_similarity', 0),
                    # é¢å¤–çš„ Policy è¾…åŠ©æŒ‡æ ‡
                    'camera_similarity': action_metrics.get('camera_similarity', 0),
                    'action_entropy': action_metrics.get('action_entropy', 0),
                    'temporal_smoothness': action_metrics.get('temporal_smoothness', 0),
                    'action_coverage': action_metrics.get('action_coverage', 0),
                }
        
        logger.debug(f"æ„å»º goal_progress_data: {len(goal_progress_data)} ä¸ªä»»åŠ¡")
        
        # æ„å»º task_info
        task_info = {}
        # difficulty åˆ° tier çš„æ˜ å°„
        difficulty_to_tier = {'easy': 1, 'medium': 2, 'hard': 3, 'extreme': 4}
        
        for result in results:
            task_config = self.task_loader.get_task(result.task_id)
            if task_config:
                # ä» difficulty æ¨æ–­ tier
                difficulty = task_config.get('difficulty', 'easy')
                tier = difficulty_to_tier.get(difficulty, 1)
                
                # è®¡ç®—å˜ä½“æ•°é‡ï¼ˆinstruction_variants æ˜¯åµŒå¥—å­—å…¸ï¼‰
                variants_config = task_config.get('instruction_variants', {})
                n_variants = 0
                n_variant_categories = 0
                if isinstance(variants_config, dict):
                    n_variant_categories = len(variants_config)
                    for cat_data in variants_config.values():
                        if isinstance(cat_data, dict):
                            n_variants += len(cat_data.get('variants', []))
                
                task_info[result.task_id] = {
                    'tier': tier,
                    'difficulty': difficulty,
                    'category': task_config.get('category', 'unknown'),
                    'n_variant_categories': n_variant_categories,
                    'n_variants': n_variants,
                }
            else:
                task_info[result.task_id] = {
                    'tier': 1,
                    'difficulty': 'unknown',
                    'category': 'unknown',
                    'n_variant_categories': 0,
                    'n_variants': 0,
                }
        
        # æ„å»ºå®Œæ•´æŠ¥å‘Šæ•°æ®
        report_data = {
            'config_file': config_filename,
            'n_tasks': len(results),
            'task_ids': task_ids,
            
            # å†…åœ¨è´¨é‡ç»´åº¦
            'intrinsic_quality': {
                'dimension_name': 'å†…åœ¨è´¨é‡',
                'enabled': bool(consistency_data),
                'metrics': {
                    'consistency': {
                        'task_consistency': consistency_data,
                        'n_samples': self.config.prior_n_samples
                    },
                    'semantic_robustness': {
                        'task_robustness': robustness_data
                    }
                },
                'visualizations': {}
            },
            
            # è¾“å‡ºè´¨é‡ç»´åº¦
            'output_quality': {
                'dimension_name': 'è¾“å‡ºè´¨é‡',
                'enabled': bool(prior_gain_data),
                'metrics': {
                    'prior_gain': {
                        'task_gains': prior_gain_data
                    }
                },
                'visualizations': {}
            },
            
            # å¯æ§æ€§ç»´åº¦ï¼ˆæš‚ä¸ä½¿ç”¨ï¼‰
            'controllability': None,
            
            # ä»»åŠ¡çº§ç»“æœï¼ˆåŒ…å« goal_progressï¼‰
            'task_results': {
                'goal_progress': {
                    'enabled': bool(goal_progress_data),
                    'task_progress': goal_progress_data,
                    'n_tasks_with_data': len(goal_progress_data)
                }
            },
            
            # ä»»åŠ¡ä¿¡æ¯
            'task_info': task_info,
            
            # æ€»ç»“
            'summary': {
                'total_tasks': len(results),
                'avg_success_rate': np.mean([r.success_rate for r in results]) if results else 0,
                'avg_prior_accuracy': np.mean([d.get('prior_sim', 0) for d in prior_gain_data.values()]) if prior_gain_data else 0,
                # goal_progress_summary for Average row in HTML
                'goal_progress_summary': self._compute_goal_progress_summary(goal_progress_data)
            },
            
            # è¾…åŠ©æŒ‡æ ‡ï¼ˆPrior å’Œ Policyï¼‰
            'auxiliary_metrics': self._compute_auxiliary_metrics(combined_results, prior_gain_data, goal_progress_data)
        }
        
        return report_data
    
    def _generate_text_report(self, report_data: Dict[str, Any], output_path: Path):
        """ç”Ÿæˆäººç±»å¯è¯»çš„æ–‡æœ¬æŠ¥å‘Š"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("STEVE-1 è¯„ä¼°æŠ¥å‘Š\n")
            f.write("="*80 + "\n\n")
            
            # å…ƒæ•°æ®
            f.write(f"ç”Ÿæˆæ—¶é—´: {report_data['metadata']['timestamp']}\n")
            f.write(f"ä»»åŠ¡æ•°é‡: {report_data['metadata']['total_tasks']}\n")
            f.write(f"è¯„ä¼°æ¡†æ¶: {report_data['metadata']['framework']}\n\n")
            
            # æ€»ä½“ç»Ÿè®¡
            summary = report_data['summary']
            f.write("æ€»ä½“ç»Ÿè®¡:\n")
            f.write(f"  æ€»æˆåŠŸç‡: {summary['overall_success_rate']:.1f}%\n")
            f.write(f"  æ€»è¯•éªŒæ•°: {summary['total_trials']}\n")
            f.write(f"  æˆåŠŸè¯•éªŒæ•°: {summary['successful_trials']}\n\n")
            
            # ===== æ·»åŠ è¡¨æ ¼æ±‡æ€» =====
            f.write("="*80 + "\n")
            f.write("è¯„ä¼°ç»“æœæ±‡æ€»\n")
            f.write("="*80 + "\n\n")
            
            # è¡¨å¤´
            f.write(f"{'ä»»åŠ¡ID':<30} {'æŒ‡ä»¤':<20} {'æˆåŠŸç‡':<10} {'å¹³å‡æ­¥æ•°':<12} {'å¹³å‡æ—¶é—´'}\n")
            f.write("-" * 80 + "\n")
            
            # æ¯ä¸ªä»»åŠ¡çš„æ±‡æ€»
            for task in report_data['tasks']:
                task_id = task['task_id'][:28]  # æˆªæ–­è¿‡é•¿çš„ID
                instruction = (task['instruction'][:18] if task['instruction'] else "N/A")
                success_rate = f"{task['success_rate']:.1f}%"
                avg_steps = f"{task['avg_steps']:.1f}"
                avg_time = f"{task['avg_time']:.1f}s"
                
                f.write(f"{task_id:<30} {instruction:<20} {success_rate:<10} {avg_steps:<12} {avg_time}\n")
            
            # æ€»ä½“ç»Ÿè®¡è¡Œ
            f.write("\n" + "-" * 80 + "\n")
            f.write(f"{'æ€»ä½“ç»Ÿè®¡':<30} {'N/A':<20} {summary['overall_success_rate']:.1f}% ")
            
            # è®¡ç®—å¹³å‡æ­¥æ•°å’Œæ—¶é—´
            avg_steps_all = sum(task['avg_steps'] for task in report_data['tasks']) / len(report_data['tasks'])
            avg_time_all = sum(task['avg_time'] for task in report_data['tasks']) / len(report_data['tasks'])
            f.write(f"{avg_steps_all:<12.1f} {avg_time_all:.1f}s\n")
            
            f.write(f"\næ€»ä»»åŠ¡æ•°: {report_data['metadata']['total_tasks']}\n")
            f.write(f"æ€»è¯•éªŒæ•°: {summary['total_trials']}\n")
            f.write("="*80 + "\n\n")
            
            # ===== è¯¦ç»†ä»»åŠ¡ä¿¡æ¯ =====
            f.write("="*80 + "\n")
            f.write("ä»»åŠ¡è¯¦æƒ…\n")
            f.write("="*80 + "\n\n")
            
            for task in report_data['tasks']:
                f.write(f"ä»»åŠ¡: {task['task_id']}\n")
                f.write(f"  æŒ‡ä»¤: {task['instruction']}\n")
                f.write(f"  è¯­è¨€: {task['language']}\n")
                f.write(f"  æˆåŠŸç‡: {task['success_rate']:.1f}%\n")
                f.write(f"  å¹³å‡æ­¥æ•°: {task['avg_steps']:.0f}\n")
                f.write(f"  å¹³å‡æ—¶é—´: {task['avg_time']:.1f}s\n")
                f.write(f"  è¯•éªŒè¯¦æƒ…:\n")
                for i, trial in enumerate(task['trials'], 1):
                    status = "âœ… æˆåŠŸ" if trial['success'] else "âŒ å¤±è´¥"
                    f.write(f"    Trial {i}: {status} | æ­¥æ•°: {trial['steps']:4d} | æ—¶é—´: {trial['time_seconds']:.1f}s")
                    
                    # æ·»åŠ æœ€ç»ˆåº“å­˜ä¿¡æ¯
                    if 'final_inventory' in trial and trial['final_inventory']:
                        inventory_items = [f"{item}Ã—{count}" for item, count in trial['final_inventory'].items()]
                        f.write(f" | åº“å­˜: {', '.join(inventory_items)}")
                    
                    f.write("\n")
                f.write("\n")
    
    def close(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        # å…³é—­å…±äº«çš„è¯„ä¼°å™¨ï¼ˆé‡Šæ”¾æ‰€æœ‰æ¨¡å‹ï¼‰
        if self._shared_evaluator:
            self._shared_evaluator.close()
            self._shared_evaluator = None
        
        # å‘åå…¼å®¹ï¼šå¦‚æœæœ‰ç‹¬ç«‹çš„ evaluator
        if self.evaluator:
            self.evaluator.close()
        
        logger.info("è¯„ä¼°æ¡†æ¶å·²å…³é—­")


# å‘½ä»¤è¡Œæ¥å£
if __name__ == "__main__":
    import argparse
    import warnings
    from src.utils.logging_config import setup_evaluation_logging
    
    # é…ç½®æ—¥å¿—ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„æ ¼å¼å’Œè¿‡æ»¤å™¨ï¼‰
    setup_evaluation_logging()
    
    # è¿‡æ»¤ä¸å¿…è¦çš„è­¦å‘Šä¿¡æ¯ï¼ˆåœ¨æ¡†æ¶åˆå§‹åŒ–ä¹‹å‰ï¼‰
    # 1. PyTorch è­¦å‘Š
    warnings.filterwarnings('ignore', category=UserWarning, module='torch')
    warnings.filterwarnings('ignore', message='.*CUDA is not available.*')
    warnings.filterwarnings('ignore', message='.*Implicit dimension choice for softmax.*')
    warnings.filterwarnings('ignore', message='.*has_cuda.*')
    
    # 2. å®Œå…¨é™é»˜ MineRL/Malmo æ—¥å¿—ï¼ˆåŒ…æ‹¬ ERRORï¼‰
    minerl_loggers = [
        'minerl.env.malmo.instance',
        'minerl.env._multiagent',
        'minerl.env.malmo',
        'process_watcher',
    ]
    for logger_name in minerl_loggers:
        minerl_logger = logging.getLogger(logger_name)
        minerl_logger.setLevel(logging.CRITICAL + 1)  # å®Œå…¨é™é»˜
        minerl_logger.propagate = False  # ä¸ä¼ æ’­åˆ°çˆ¶ logger
    
    # 3. STEVE-1 è­¦å‘Š
    warnings.filterwarnings('ignore', category=UserWarning, module='steve1')
    
    parser = argparse.ArgumentParser(description='STEVE-1 è¯„ä¼°æ¡†æ¶')
    parser.add_argument(
        '--config',
        type=str,
        default='config/eval_tasks.yaml',
        help='ä»»åŠ¡é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: config/eval_tasks.yamlï¼‰'
    )
    parser.add_argument(
        '--task',
        type=str,
        help='è¯„ä¼°å•ä¸ªä»»åŠ¡ï¼ˆä»»åŠ¡IDï¼‰'
    )
    parser.add_argument(
        '--task-set',
        type=str,
        help='è¯„ä¼°ä»»åŠ¡é›†ï¼ˆå¦‚ harvest_tasks, quick_test, baseline_testï¼‰'
    )
    parser.add_argument(
        '--task-list',
        type=str,
        nargs='+',
        help='è¯„ä¼°ä»»åŠ¡åˆ—è¡¨ï¼ˆå¤šä¸ªä»»åŠ¡IDï¼‰'
    )
    parser.add_argument(
        '--n-trials',
        type=int,
        default=None,  # None è¡¨ç¤ºä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼
        help='æ¯ä¸ªä»»åŠ¡çš„è¯•éªŒæ¬¡æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰'
    )
    parser.add_argument(
        '--max-steps',
        type=int,
        default=None,  # None è¡¨ç¤ºä½¿ç”¨é…ç½®æ–‡ä»¶çš„å€¼
        help='æ¯ä¸ªè¯•éªŒçš„æœ€å¤§æ­¥æ•°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼‰'
    )
    parser.add_argument(
        '--render',
        action='store_true',
        help='å¯ç”¨æ¸¸æˆçª—å£æ¸²æŸ“ï¼ˆæ˜¾ç¤ºç”»é¢ï¼‰'
    )
    parser.add_argument(
        '--enable_video',
        action='store_true',
        help='å¯ç”¨è§†é¢‘å½•åˆ¶ï¼ˆå›ºå®šå°ºå¯¸ 640x360ï¼‰'
    )
    parser.add_argument(
        '--enable_report',
        action='store_true',
        help='ç”Ÿæˆ HTML æŠ¥å‘Š'
    )
    parser.add_argument(
        '--output-dir',
        type=str,
        default=None,
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: data/evaluation/ï¼‰'
    )
    
    args = parser.parse_args()
    
    # è§†é¢‘å½•åˆ¶ï¼šå¦‚æœå¯ç”¨ï¼Œä½¿ç”¨å›ºå®šå°ºå¯¸ 640x360
    video_size = (640, 360) if args.enable_video else None
    
    # åˆ›å»ºé…ç½®
    config = EvaluationConfig(
        task_config_path=args.config,
        n_trials=args.n_trials,
        max_steps=args.max_steps,
        enable_render=args.render,
        enable_report=args.enable_report,
        video_size=video_size,
        output_dir=args.output_dir  # ä¼ é€’è¾“å‡ºç›®å½•å‚æ•°
    )
    
    # åˆ›å»ºè¯„ä¼°æ¡†æ¶
    framework = EvaluationFramework(config=config)
    
    try:
        results = []
        
        # æ ¹æ®å‚æ•°é€‰æ‹©è¯„ä¼°æ¨¡å¼
        if args.task:
            # å•ä¸ªä»»åŠ¡ï¼ˆä¼ é€’å‘½ä»¤è¡Œå‚æ•°ï¼Œç¡®ä¿ä¼˜å…ˆçº§ï¼‰
            result, _ = framework.evaluate_single_task(
                args.task,
                n_trials=config.n_trials,
                max_steps=config.max_steps
            )
            results = [result]
        
        elif args.task_set:
            # ä»»åŠ¡é›†ï¼ˆä¼ é€’å‘½ä»¤è¡Œå‚æ•°ï¼Œç¡®ä¿ä¼˜å…ˆçº§ï¼‰
            results = framework.evaluate_task_set(
                args.task_set,
                n_trials=config.n_trials,
                max_steps=config.max_steps
            )
        
        elif args.task_list:
            # ä»»åŠ¡åˆ—è¡¨ï¼ˆä¼ é€’å‘½ä»¤è¡Œå‚æ•°ï¼Œç¡®ä¿ä¼˜å…ˆçº§ï¼‰
            results = framework.evaluate_task_list(
                args.task_list,
                n_trials=config.n_trials,
                max_steps=config.max_steps
            )
        
        else:
            # é»˜è®¤ï¼šå¿«é€Ÿæµ‹è¯•ï¼ˆä¼ é€’å‘½ä»¤è¡Œå‚æ•°ï¼Œç¡®ä¿ä¼˜å…ˆçº§ï¼‰
            logger.info("æœªæŒ‡å®šä»»åŠ¡ï¼Œè¿è¡Œå¿«é€Ÿæµ‹è¯•...")
            results = framework.evaluate_task_set(
                'quick_test',
                n_trials=config.n_trials,
                max_steps=config.max_steps
            )
        
        # æ‰“å°æ‘˜è¦
        framework.print_summary(results)
        
        # ç”ŸæˆæŠ¥å‘Š
        framework.generate_report(results)
        
        # é‡ç½® task-set ç›®å½•ï¼ˆé¿å…å½±å“åç»­è¯„ä¼°ï¼‰
        framework.current_task_set_dir = None
        
    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        framework.close()
