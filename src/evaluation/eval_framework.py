"""
è¯„ä¼°æ¡†æ¶ - ä»»åŠ¡ç®¡ç†ä¸è°ƒåº¦
Evaluation Framework - Task Management and Scheduling

èŒè´£:
- ç®¡ç† STEVE1Evaluator å®ä¾‹
- ä» YAML åŠ è½½ä»»åŠ¡é…ç½®
- å•/æ‰¹é‡ä»»åŠ¡è°ƒåº¦
- ç»“æœæ”¶é›†ä¸èšåˆ
- ç”ŸæˆæŠ¥å‘Šå’Œç»Ÿè®¡
"""

import sys
import logging
import json
import yaml
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥è‡ªå®šä¹‰ç¯å¢ƒï¼ˆè§¦å‘ç¯å¢ƒæ³¨å†Œï¼‰
import src.envs

from src.evaluation.steve1_evaluator import STEVE1Evaluator
from src.evaluation.metrics import TaskResult
from src.evaluation.task_loader import TaskLoader
from src.evaluation.report_generator import ReportGenerator
from src.evaluation.matrix_analyzer import MatrixAnalyzer
from src.evaluation.html_report_generator import HTMLReportGenerator

logger = logging.getLogger(__name__)


@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®"""
    # STEVE-1 æ¨¡å‹é…ç½®
    model_path: str = "data/weights/vpt/2x.model"
    weights_path: str = "data/weights/steve1/steve1.weights"
    prior_weights: str = "data/weights/steve1/steve1_prior.pt"
    text_cond_scale: float = 6.0
    seed: int = 42
    enable_render: bool = False
    enable_report: bool = False
    video_size: Optional[Tuple[int, int]] = None  # è§†é¢‘å°ºå¯¸ (width, height)ï¼ŒNone è¡¨ç¤ºä¸å½•åˆ¶
    
    # è¯„ä¼°é…ç½®
    n_trials: int = 3  # é»˜è®¤æ¯ä¸ªä»»åŠ¡è¿è¡Œæ¬¡æ•°
    max_steps: int = 2000  # é»˜è®¤æœ€å¤§æ­¥æ•°
    
    # è·¯å¾„é…ç½®
    task_config_path: str = "config/eval_tasks.yaml"
    results_dir: str = "results/evaluation"


class EvaluationFramework:
    """
    è¯„ä¼°æ¡†æ¶ - ä»»åŠ¡ç®¡ç†ä¸è°ƒåº¦å™¨
    
    æ¶æ„:
        EvaluationFramework (Manager/Scheduler)
            â†“ ç®¡ç†
        STEVE1Evaluator (Worker/Executor)
            â†“ æ‰§è¡Œ
        Environment + Agent
    """
    
    def __init__(
        self,
        config: Optional[EvaluationConfig] = None,
        evaluator: Optional[STEVE1Evaluator] = None
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°æ¡†æ¶
        
        Args:
            config: è¯„ä¼°é…ç½®ï¼ˆå¦‚æœNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ï¼‰
            evaluator: STEVE1Evaluator å®ä¾‹ï¼ˆå¦‚æœNoneåˆ™è‡ªåŠ¨åˆ›å»ºï¼‰
        """
        # é…ç½®æ—¥å¿—è¿‡æ»¤å™¨ï¼Œè¿‡æ»¤æ‰ä¸å¿…è¦çš„è­¦å‘Š
        self._setup_log_filters()

        logger.info(f"{'='*30}")
        logger.info(f"è°ƒåº¦å™¨åŠ è½½...")
        logger.info(f"{'='*30}")
        
        self.config = config or EvaluationConfig()
        
        # åŠ è½½ä»»åŠ¡é…ç½®
        self.task_loader = TaskLoader(self.config.task_config_path)
        logger.info(f"åŠ è½½ä»»åŠ¡é…ç½®: {self.config.task_config_path}")
        logger.info(f"  å‘ç° {len(self.task_loader.tasks)} ä¸ªä»»åŠ¡")
        
        # åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
        self.report_generator = ReportGenerator(self.config.results_dir)
        self.matrix_analyzer = MatrixAnalyzer()
        self.html_generator = HTMLReportGenerator(self.config.results_dir)

        # ä¿ç•™ evaluator å‚æ•°ç”¨äºå‘åå…¼å®¹ï¼Œä½†ä¸åœ¨åˆå§‹åŒ–æ—¶åˆ›å»º
        # æ¯ä¸ªä»»åŠ¡ä¼šåˆ›å»ºä¸“ç”¨çš„ evaluatorï¼Œé¿å…ç¯å¢ƒé…ç½®å†²çª
        self.evaluator = evaluator  # é€šå¸¸ä¸º None
        if self.evaluator:
            logger.info("ä½¿ç”¨æä¾›çš„è¯„ä¼°å™¨å®ä¾‹")
        
        # ç»“æœå­˜å‚¨
        self.results: List[TaskResult] = []
        
        # Task-set ç›®å½•ï¼ˆç”¨äºæ‰¹é‡è¯„ä¼°æ—¶ç»„ç»‡ç»“æœï¼‰
        self.current_task_set_dir: Optional[Path] = None
        
        logger.info("è¯„ä¼°æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_log_filters(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿï¼šæ ¼å¼åŒ–ã€è¿‡æ»¤å™¨ç­‰"""
        import warnings
        from src.utils.logging_config import setup_evaluation_logging
        
        # é…ç½®ç»Ÿä¸€çš„æ—¥å¿—æ ¼å¼å’Œè¿‡æ»¤å™¨ï¼ˆç¼©çŸ­æ¨¡å—åã€è¿‡æ»¤ä¸éœ€è¦çš„æ—¥å¿—ï¼‰
        setup_evaluation_logging()
        
        # 1. è¿‡æ»¤ PyTorch çš„ UserWarningï¼ˆå¦‚ autocast è­¦å‘Šï¼‰
        warnings.filterwarnings('ignore', category=UserWarning, module='torch')
        warnings.filterwarnings('ignore', message='.*CUDA is not available.*')
        warnings.filterwarnings('ignore', message='.*Implicit dimension choice for softmax.*')
        
        # 2. è¿‡æ»¤ MineRL/Malmo çš„ WARNING æ—¥å¿—
        # è®¾ç½® minerl ç›¸å…³ logger çš„çº§åˆ«ä¸º ERROR
        minerl_loggers = [
            'minerl.env.malmo.instance',
            'minerl.env._multiagent',
            'minerl.env.malmo',
        ]
        for logger_name in minerl_loggers:
            minerl_logger = logging.getLogger(logger_name)
            minerl_logger.setLevel(logging.ERROR)  # åªæ˜¾ç¤º ERROR åŠä»¥ä¸Šçº§åˆ«
        
        # 3. è¿‡æ»¤ STEVE-1 çš„ UserWarning
        warnings.filterwarnings('ignore', category=UserWarning, module='steve1')
        
        logger.debug("æ—¥å¿—ç³»ç»Ÿå·²é…ç½®ï¼šç¼©çŸ­æ¨¡å—åã€è¿‡æ»¤ä¸éœ€è¦çš„æ—¥å¿—")
    
    def evaluate_single_task(
        self,
        task_id: str,
        n_trials: Optional[int] = None,
        max_steps: Optional[int] = None,
        parent_dir: Optional[Path] = None,  # çˆ¶ç›®å½•ï¼ˆç”¨äº task-setï¼‰
    ) -> Tuple[TaskResult, Optional[Path]]:
        """
        è¯„ä¼°å•ä¸ªä»»åŠ¡
        
        Args:
            task_id: ä»»åŠ¡ID
            n_trials: è¯•éªŒæ¬¡æ•°ï¼ˆå¦‚æœNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼‰
            max_steps: æœ€å¤§æ­¥æ•°ï¼ˆå¦‚æœNoneåˆ™ä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼‰
            parent_dir: çˆ¶ç›®å½•ï¼ˆå¦‚æœæä¾›ï¼Œä»»åŠ¡ç›®å½•å°†åˆ›å»ºåœ¨è¿™ä¸ªç›®å½•ä¸‹ï¼‰
        
        Returns:
            Tuple[TaskResult, Optional[Path]]: ä»»åŠ¡ç»“æœ + è¾“å‡ºç›®å½•è·¯å¾„
        """
        # ä»é…ç½®åŠ è½½ä»»åŠ¡
        task_config = self.task_loader.get_task(task_id)
        if not task_config:
            raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
        
        # ç¡®å®šå‚æ•°ï¼ˆä¼˜å…ˆçº§ï¼šå‡½æ•°å‚æ•° > å…¨å±€é…ç½® > ä»»åŠ¡é…ç½®ï¼‰
        # æ³¨æ„ï¼šå‘½ä»¤è¡Œå‚æ•°ï¼ˆn_trials, max_stepsï¼‰åº”è¯¥ä¼˜å…ˆäºä»»åŠ¡é…ç½®
        n_trials = n_trials if n_trials is not None else (self.config.n_trials if self.config.n_trials != 3 else task_config.get('n_trials', 3))
        max_steps = max_steps if max_steps is not None else (self.config.max_steps if self.config.max_steps != 2000 else task_config.get('max_steps', 2000))
        
        # ç¡®å®šæŒ‡ä»¤å’Œè¯­è¨€
        instruction = None
        language = "en"
        
        if 'en_instruction' in task_config:
            instruction = task_config['en_instruction']
            language = "en"
        elif 'zh_instruction' in task_config:
            instruction = task_config['zh_instruction']
            language = "zh"
        
        # ä»ä»»åŠ¡é…ç½®è¯»å–ç¯å¢ƒé…ç½®ï¼ˆåŒ…æ‹¬å¥–åŠ±é…ç½®ï¼‰
        env_config = task_config.get('env_config', {}).copy()  # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…ä¿®æ”¹åŸé…ç½®
        env_name = task_config.get('env_name', 'MineRLHarvestEnv-v0')
        
        # å°† max_steps æ·»åŠ åˆ° env_config ä¸­ï¼ˆä½œä¸º max_episode_stepsï¼‰
        env_config['max_episode_steps'] = max_steps
        
        # ä»å…¨å±€é…ç½®è¯»å– image_sizeï¼ˆå¦‚æœä»»åŠ¡é…ç½®ä¸­æ²¡æœ‰æŒ‡å®šï¼‰
        if 'image_size' not in env_config:
            global_config = self.task_loader.config.get('evaluation', {})
            global_image_size = global_config.get('image_size')
            if global_image_size:
                # è½¬æ¢ä¸º tuple æ ¼å¼ (height, width)
                if isinstance(global_image_size, list) and len(global_image_size) == 2:
                    env_config['image_size'] = tuple(global_image_size)
                    logger.info(f"  ä½¿ç”¨å…¨å±€ image_size: {env_config['image_size']}")
                else:
                    env_config['image_size'] = global_image_size
                    logger.info(f"  ä½¿ç”¨å…¨å±€ image_size: {env_config['image_size']}")
        
        # è·å–åŠ¨ä½œåºåˆ—æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
        replay_actions_file = task_config.get('replay_actions_file', None)
        if replay_actions_file:
            logger.info(f"  æ£€æµ‹åˆ°åŠ¨ä½œåºåˆ—æ–‡ä»¶: {replay_actions_file}")
        
        # ä¸ºå½“å‰ä»»åŠ¡åˆ›å»ºä¸“ç”¨çš„ evaluatorï¼ˆç¡®ä¿ç¯å¢ƒé…ç½®æ­£ç¡®ï¼‰
        logger.info("åˆ›å»ºä»»åŠ¡ä¸“ç”¨è¯„ä¼°å™¨...")
        task_evaluator = STEVE1Evaluator(
            model_path=self.config.model_path,
            weights_path=self.config.weights_path,
            prior_weights=self.config.prior_weights,
            text_cond_scale=self.config.text_cond_scale,
            seed=self.config.seed,
            enable_render=self.config.enable_render,
            video_size=self.config.video_size,  # è§†é¢‘å°ºå¯¸ï¼ŒNone è¡¨ç¤ºä¸å½•åˆ¶
            env_name=env_name,
            env_config=env_config,  # ä¼ é€’ç¯å¢ƒé…ç½®ï¼ˆåŒ…å« max_episode_stepsï¼‰
            enable_report=self.config.enable_report,
            replay_actions_file=replay_actions_file  # ä¼ é€’åŠ¨ä½œåºåˆ—æ–‡ä»¶è·¯å¾„
        )
        
        logger.info(f"{'='*30}")
        logger.info(f"è°ƒåº¦ä»»åŠ¡: {task_id}")
        logger.info(f"{'='*30}")
        logger.info(f"  æè¿°: {task_config.get('description', 'N/A')}")
        logger.info(f"  ç±»åˆ«: {task_config.get('category', 'N/A')}")
        logger.info(f"  éš¾åº¦: {task_config.get('difficulty', 'N/A')}")
        logger.info(f"  æŒ‡ä»¤: {instruction}")
        logger.info(f"  è¯­è¨€: {language}")
        logger.info(f"  è¯•éªŒæ¬¡æ•°: {n_trials}")
        logger.info(f"  æœ€å¤§æ­¥æ•°: {max_steps}")
        if env_config.get('specified_biome'):
            logger.info(f"  ğŸŒ æŒ‡å®šBiome: {env_config.get('specified_biome')}")
        if replay_actions_file:
            logger.info(f"  ğŸ¬ å›æ”¾æ¨¡å¼: {replay_actions_file}")
        
        # åˆ›å»ºä»»åŠ¡è¾“å‡ºç›®å½•ï¼ˆæ€»æ˜¯åˆ›å»ºï¼Œä¸ç®¡æ˜¯å¦ä¿å­˜è§†é¢‘ï¼‰
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        dir_name = f"{task_id}_{language}_{timestamp}"
        
        # å¦‚æœæä¾›äº†çˆ¶ç›®å½•ï¼Œåœ¨çˆ¶ç›®å½•ä¸‹åˆ›å»ºä»»åŠ¡ç›®å½•
        if parent_dir:
            output_dir = parent_dir / dir_name
        else:
            output_dir = Path(self.config.results_dir) / dir_name
        
        output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"  ç»“æœç›®å½•: {output_dir}")
        
        try:
            # è°ƒç”¨è¯„ä¼°å™¨æ‰§è¡Œ
            result = task_evaluator.evaluate_task(
                task_id=task_id,
                language=language,
                n_trials=n_trials,
                max_steps=max_steps,
                instruction=instruction,
                output_dir=output_dir,  # ä¼ é€’è¾“å‡ºç›®å½•ç»™evaluator
            )
            
            # ä¿å­˜ä»»åŠ¡ç»“æœåˆ°ç›®å½•
            self._save_task_results(result, output_dir)
            
            # ä¿å­˜ç»“æœ
            self.results.append(result)
            
            return result
        finally:
            # âš ï¸ é‡è¦ï¼šç«‹å³å…³é—­ä»»åŠ¡è¯„ä¼°å™¨ï¼Œé‡Šæ”¾èµ„æº
            logger.info(f"  å…³é—­ä»»åŠ¡è¯„ä¼°å™¨ï¼Œé‡Šæ”¾ç¯å¢ƒèµ„æº...")
            task_evaluator.close()
            logger.info(f"  âœ“ èµ„æºå·²é‡Šæ”¾")
    
    def _save_task_results(self, result: TaskResult, output_dir: Path):
        """
        ä¿å­˜ä»»åŠ¡ç»“æœåˆ°æŒ‡å®šç›®å½•ï¼ˆJSONã€TXTï¼‰
        
        æ³¨æ„ï¼šè§†é¢‘ä¿å­˜ç°åœ¨ç”± steve1_evaluator åœ¨ _run_single_trial ä¸­å®Œæˆ
        
        Args:
            result: ä»»åŠ¡ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
        """
        # æ„å»ºç»“æœæ•°æ®
        result_data = {
            "task_id": result.task_id,
            "language": result.language,
            "instruction": result.instruction,
            "success_rate": result.success_rate,
            "avg_steps": result.avg_steps,
            "avg_time": result.avg_time,
            "trials": [
                {
                    "trial_idx": i + 1,
                    "success": trial.success,
                    "steps": trial.steps,
                    "time_seconds": trial.time_seconds,
                    "has_video": (output_dir / f"trial_{i+1}.mp4").exists()  # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                }
                for i, trial in enumerate(result.trials)
            ]
        }
        
        # ä¿å­˜JSON
        json_path = output_dir / "result.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        #logger.info(f"  âœ“ ç»“æœå·²ä¿å­˜: {json_path.name}")
        
        # ä¿å­˜TXTï¼ˆäººç±»å¯è¯»ï¼‰
        txt_path = output_dir / "result.txt"
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write(f"ä»»åŠ¡è¯„ä¼°ç»“æœ: {result.task_id}\n")
            f.write("="*80 + "\n\n")
            f.write(f"è¯­è¨€: {result.language}\n")
            f.write(f"æŒ‡ä»¤: {result.instruction}\n")
            f.write(f"æˆåŠŸç‡: {result.success_rate*100:.1f}%\n")
            f.write(f"å¹³å‡æ­¥æ•°: {result.avg_steps:.1f}\n")
            f.write(f"å¹³å‡æ—¶é—´: {result.avg_time:.1f}s\n\n")
            f.write("è¯•éªŒè¯¦æƒ…:\n")
            f.write("-"*80 + "\n")
            for i, trial in enumerate(result.trials, 1):
                status = "âœ… æˆåŠŸ" if trial.success else "âŒ å¤±è´¥"
                video_status = "ğŸ¬" if (output_dir / f"trial_{i}.mp4").exists() else ""
                f.write(f"Trial {i}: {status} | æ­¥æ•°: {trial.steps:4d} | æ—¶é—´: {trial.time_seconds:.1f}s {video_status}\n")
        #logger.info(f"  âœ“ æŠ¥å‘Šå·²ä¿å­˜: {txt_path.name}")
    
    def evaluate_task_list(
        self,
        task_ids: List[str],
        n_trials: Optional[int] = None,
        max_steps: Optional[int] = None,
        task_set_name: Optional[str] = None  # ä»»åŠ¡é›†åç§°ï¼ˆç”¨äºåˆ›å»ºç›®å½•ï¼‰
    ) -> List[TaskResult]:
        """
        æ‰¹é‡è¯„ä¼°ä»»åŠ¡åˆ—è¡¨
        
        Args:
            task_ids: ä»»åŠ¡IDåˆ—è¡¨
            n_trials: è¯•éªŒæ¬¡æ•°ï¼ˆåº”ç”¨äºæ‰€æœ‰ä»»åŠ¡ï¼‰
            max_steps: æœ€å¤§æ­¥æ•°ï¼ˆåº”ç”¨äºæ‰€æœ‰ä»»åŠ¡ï¼‰
            task_set_name: ä»»åŠ¡é›†åç§°ï¼ˆå¦‚æœæä¾›ï¼Œå°†åˆ›å»ºä¸“é—¨çš„ç›®å½•ï¼‰
        
        Returns:
            List[TaskResult]: ä»»åŠ¡ç»“æœåˆ—è¡¨
        """
        logger.info(f"\n{'='*80}")
        logger.info(f"æ‰¹é‡è¯„ä¼°å¼€å§‹: {len(task_ids)} ä¸ªä»»åŠ¡")
        logger.info(f"{'='*80}\n")
        
        # å¦‚æœæä¾›äº† task_set_nameï¼Œåˆ›å»º task-set ç›®å½•
        task_set_dir = None
        if task_set_name:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            task_set_dir_name = f"{task_set_name}_{timestamp}"
            task_set_dir = Path(self.config.results_dir) / task_set_dir_name
            task_set_dir.mkdir(parents=True, exist_ok=True)
            logger.info(f"ğŸ“ Task-set ç›®å½•: {task_set_dir}")
            logger.info(f"{'='*80}\n")
            # ä¿å­˜ task_set_dir ä¾›åç»­ generate_report ä½¿ç”¨
            self.current_task_set_dir = task_set_dir
        
        results = []
        
        for i, task_id in enumerate(task_ids, 1):
            logger.info(f"\n[{i}/{len(task_ids)}] è¯„ä¼°ä»»åŠ¡: {task_id}")
            
            try:
                # evaluate_single_task ç°åœ¨è¿”å› tuple
                result = self.evaluate_single_task(
                    task_id=task_id,
                    n_trials=n_trials,
                    max_steps=max_steps,
                    parent_dir=task_set_dir  # ä¼ é€’ task-set ç›®å½•
                )
                results.append(result)  # åªä¿å­˜ TaskResult
                
                # æ‰“å°ä»»åŠ¡æ‘˜è¦
                logger.info(f"  âœ… å®Œæˆ: æˆåŠŸç‡ {result.success_rate*100:.1f}%, "
                           f"å¹³å‡æ­¥æ•° {result.avg_steps:.1f}")
                
            except Exception as e:
                logger.error(f"  âŒ ä»»åŠ¡å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
        
        logger.info(f"\n{'='*80}")
        logger.info(f"æ‰¹é‡è¯„ä¼°å®Œæˆ: {len(results)}/{len(task_ids)} ä¸ªä»»åŠ¡æˆåŠŸ")
        logger.info(f"{'='*80}\n")
        
        # æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œé‡ç½® current_task_set_dirï¼Œå› ä¸º generate_report è¿˜éœ€è¦ç”¨å®ƒ
        
        return results
    
    def evaluate_task_set(
        self,
        task_set_name: str,
        n_trials: Optional[int] = None,
        max_steps: Optional[int] = None
    ) -> List[TaskResult]:
        """
        è¯„ä¼°ä»»åŠ¡é›†ï¼ˆä» YAML é…ç½®ä¸­çš„ harvest_tasks, quick_test, baseline_test ç­‰ï¼‰
        
        Args:
            task_set_name: ä»»åŠ¡é›†åç§° ('harvest_tasks', 'quick_test', 'baseline_test')
            n_trials: è¯•éªŒæ¬¡æ•°
            max_steps: æœ€å¤§æ­¥æ•°
        
        Returns:
            List[TaskResult]: ä»»åŠ¡ç»“æœåˆ—è¡¨
        """
        # ä» YAML åŠ è½½ä»»åŠ¡é›†
        task_ids = self.task_loader.get_task_set(task_set_name)
        
        if not task_ids:
            raise ValueError(f"ä»»åŠ¡é›†ä¸å­˜åœ¨æˆ–ä¸ºç©º: {task_set_name}")
        
        logger.info(f"\n{'='*80}")
        logger.info(f"è¯„ä¼°ä»»åŠ¡é›†: {task_set_name}")
        logger.info(f"ä»»åŠ¡æ•°é‡: {len(task_ids)}")
        logger.info(f"ä»»åŠ¡åˆ—è¡¨: {', '.join(task_ids)}")
        logger.info(f"{'='*80}\n")
        
        return self.evaluate_task_list(
            task_ids=task_ids,
            n_trials=n_trials,
            max_steps=max_steps,
            task_set_name=task_set_name  # ä¼ é€’ä»»åŠ¡é›†åç§°
        )
    
    def print_summary(self, results: Optional[List[TaskResult]] = None):
        """
        æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦
        
        Args:
            results: ä»»åŠ¡ç»“æœåˆ—è¡¨ï¼ˆå¦‚æœNoneåˆ™ä½¿ç”¨self.resultsï¼‰
        """
        if results is None:
            results = self.results
        
        if not results:
            logger.warning("æ²¡æœ‰è¯„ä¼°ç»“æœ")
            return
        
        print(f"\n{'='*80}")
        print("è¯„ä¼°ç»“æœæ±‡æ€»")
        print(f"{'='*80}\n")
        
        # è¡¨å¤´
        print(f"{'ä»»åŠ¡ID':<30} {'æŒ‡ä»¤':<20} {'æˆåŠŸç‡':<10} {'å¹³å‡æ­¥æ•°':<12} {'å¹³å‡æ—¶é—´'}")
        print("-" * 80)
        
        # æ¯ä¸ªä»»åŠ¡çš„ç»“æœ
        for result in results:
            task_id = result.task_id[:28]  # æˆªæ–­è¿‡é•¿çš„ID
            instruction = result.instruction[:18] if result.instruction else "N/A"
            success_rate = f"{result.success_rate * 100:.1f}%"
            avg_steps = f"{result.avg_steps:.1f}"
            avg_time = f"{result.avg_time:.1f}s"
            
            print(f"{task_id:<30} {instruction:<20} {success_rate:<10} {avg_steps:<12} {avg_time}")
        
        # æ€»ä½“ç»Ÿè®¡
        print("\n" + "-" * 80)
        overall_success = sum(r.success_rate for r in results) / len(results)
        overall_steps = sum(r.avg_steps for r in results) / len(results)
        overall_time = sum(r.avg_time for r in results) / len(results)
        total_trials = sum(len(r.trials) for r in results)
        
        print(f"{'æ€»ä½“ç»Ÿè®¡':<30} {'N/A':<20} {overall_success*100:.1f}% {overall_steps:<12.1f} {overall_time:.1f}s")
        print(f"\næ€»ä»»åŠ¡æ•°: {len(results)}")
        print(f"æ€»è¯•éªŒæ•°: {total_trials}")
        print(f"{'='*80}\n")
    
    def generate_report(
        self,
        results: Optional[List[TaskResult]] = None,
        report_name: str = "evaluation_report"
    ):
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            results: ä»»åŠ¡ç»“æœåˆ—è¡¨ï¼ˆå¦‚æœNoneåˆ™ä½¿ç”¨self.resultsï¼‰
            report_name: æŠ¥å‘Šåç§°
            
        Returns:
            Tuple[str, str]: JSONæŠ¥å‘Šè·¯å¾„å’ŒTXTæŠ¥å‘Šè·¯å¾„
        """
        if results is None:
            results = self.results
        
        if not results:
            logger.warning("æ²¡æœ‰è¯„ä¼°ç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return None, None
        
        # æ„å»ºæŠ¥å‘Šæ•°æ®
        report_data = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_tasks": len(results),
                "evaluator": "STEVE-1",
                "framework": "EvaluationFramework"
            },
            "tasks": []
        }
        
        for result in results:
            task_data = {
                "task_id": result.task_id,
                "instruction": result.instruction,
                "language": result.language,
                "success_rate": result.success_rate * 100,  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                "avg_steps": result.avg_steps,
                "avg_time": result.avg_time,
                "trials": [
                    {
                        "success": trial.success,
                        "steps": trial.steps,
                        "time_seconds": trial.time_seconds,
                        "final_inventory": trial.final_inventory
                    }
                    for trial in result.trials
                ]
            }
            report_data["tasks"].append(task_data)
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        report_data["summary"] = {
            "overall_success_rate": np.mean([r.success_rate for r in results]) * 100,
            "total_trials": sum(len(r.trials) for r in results),
            "successful_trials": sum(sum(1 for t in r.trials if t.success) for r in results)
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_filename = f"{report_name}_{timestamp}.json"
        
        # ä¼˜å…ˆçº§ï¼štask-set ç›®å½• > å•ä»»åŠ¡ç›®å½• > å…¨å±€ç›®å½•
        if self.current_task_set_dir:
            # å¤šä»»åŠ¡è¯„ä¼°ï¼ˆtask-setï¼‰ï¼Œä¿å­˜åˆ° task-set ç›®å½•
            json_path = self.current_task_set_dir / json_filename
            logger.info(f"  å°†æŠ¥å‘Šä¿å­˜åˆ° task-set ç›®å½•: {self.current_task_set_dir.name}")
        elif len(results) == 1:
            # å•ä»»åŠ¡è¯„ä¼°ï¼Œä¿å­˜åˆ°ä»»åŠ¡ç›®å½•ä¸‹
            task_id = results[0].task_id
            language = results[0].language
            # æŸ¥æ‰¾åŒ¹é…çš„ç›®å½•ï¼ˆæŒ‰æ—¶é—´å€’åºï¼‰
            pattern = f"{task_id}_{language}_*"
            matching_dirs = sorted(
                Path(self.config.results_dir).glob(pattern),
                key=lambda p: p.stat().st_mtime,
                reverse=True
            )
            if matching_dirs:
                json_path = matching_dirs[0] / json_filename
                #logger.info(f"  å°†æŠ¥å‘Šä¿å­˜åˆ°ä»»åŠ¡ç›®å½•: {matching_dirs[0].name}")
            else:
                json_path = Path(self.report_generator.output_dir) / json_filename
        else:
            # å¤šä»»åŠ¡ä½†æ—  task-setï¼Œä½¿ç”¨å…¨å±€ç›®å½•
            json_path = Path(self.report_generator.output_dir) / json_filename
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        txt_path = json_path.with_suffix('.txt')
        self._generate_text_report(report_data, txt_path)
        
        # ç”Ÿæˆä¸‰ç»´èƒ½åŠ›çŸ©é˜µåˆ†æå’ŒHTMLæŠ¥å‘Š
        matrix_analysis, html_path = self._generate_matrix_report(results, json_path.parent)
        
        #logger.info(f"\n{'='*80}")
        #logger.info(f"æŠ¥å‘Šå·²ç”Ÿæˆ:")
        #logger.info(f"  JSON: {json_path}")
        #logger.info(f"  TXT:  {txt_path}")
        #if html_path:
        #    logger.info(f"  HTML: {html_path}")
        #logger.info(f"{'='*80}\n")
        
        return str(json_path), str(txt_path)
    
    def _generate_matrix_report(
        self, 
        results: List[TaskResult], 
        output_dir: Path
    ) -> Tuple[Optional[Dict], Optional[Path]]:
        """
        ç”Ÿæˆä¸‰ç»´èƒ½åŠ›çŸ©é˜µåˆ†ææŠ¥å‘Šå’ŒHTMLå¯è§†åŒ–æŠ¥å‘Š
        
        Args:
            results: ä»»åŠ¡ç»“æœåˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            (matrix_analysis, html_path): çŸ©é˜µåˆ†æç»“æœå’ŒHTMLè·¯å¾„
        """
        try:
            # å°†TaskResultè½¬æ¢ä¸ºçŸ©é˜µåˆ†æå™¨éœ€è¦çš„æ ¼å¼
            analysis_data = []
            for result in results:
                # ä»task_loaderä¸­è·å–åŸå§‹ä»»åŠ¡é…ç½®
                task_config = self.task_loader.get_task(result.task_id)
                if not task_config:
                    logger.warning(f"æ— æ³•æ‰¾åˆ°ä»»åŠ¡é…ç½®: {result.task_id}")
                    continue
                
                task_data = {
                    'task_config': task_config,
                    'success_rate': result.success_rate,
                    'avg_steps': result.avg_steps,
                    'avg_time': result.avg_time,
                }
                analysis_data.append(task_data)
            
            if not analysis_data:
                logger.warning("æ²¡æœ‰å¯åˆ†æçš„ä»»åŠ¡æ•°æ®")
                return None, None
            
            # æ‰§è¡ŒçŸ©é˜µåˆ†æ
            matrix_analysis = self.matrix_analyzer.analyze_results(analysis_data)
            
            # ä¿å­˜JSONåˆ†æç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            analysis_json_path = output_dir / f"matrix_analysis_{timestamp}.json"
            self.matrix_analyzer.save_analysis(matrix_analysis, analysis_json_path)
            logger.info(f"âœ“ çŸ©é˜µåˆ†æå·²ä¿å­˜: {analysis_json_path.name}")
            
            # æ‰“å°åˆ†ææ‘˜è¦
            self.matrix_analyzer.print_summary(matrix_analysis)
            
            # ç”ŸæˆHTMLæŠ¥å‘Š
            config_filename = Path(self.config.task_config_path).name
            html_path = self.html_generator.generate(
                analysis=matrix_analysis,
                config_file=config_filename,
                output_filename=f"evaluation_report_{timestamp}.html"
            )
            
            return matrix_analysis, html_path
            
        except Exception as e:
            logger.error(f"ç”ŸæˆçŸ©é˜µæŠ¥å‘Šå¤±è´¥: {e}", exc_info=True)
            return None, None
    
    def _generate_text_report(self, report_data: Dict[str, Any], output_path: Path):
        """ç”Ÿæˆäººç±»å¯è¯»çš„æ–‡æœ¬æŠ¥å‘Š"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("STEVE-1 è¯„ä¼°æŠ¥å‘Š\n")
            f.write("="*80 + "\n\n")
            
            # å…ƒæ•°æ®
            f.write(f"ç”Ÿæˆæ—¶é—´: {report_data['metadata']['timestamp']}\n")
            f.write(f"ä»»åŠ¡æ•°é‡: {report_data['metadata']['total_tasks']}\n")
            f.write(f"è¯„ä¼°æ¡†æ¶: {report_data['metadata']['framework']}\n\n")
            
            # æ€»ä½“ç»Ÿè®¡
            summary = report_data['summary']
            f.write("æ€»ä½“ç»Ÿè®¡:\n")
            f.write(f"  æ€»æˆåŠŸç‡: {summary['overall_success_rate']:.1f}%\n")
            f.write(f"  æ€»è¯•éªŒæ•°: {summary['total_trials']}\n")
            f.write(f"  æˆåŠŸè¯•éªŒæ•°: {summary['successful_trials']}\n\n")
            
            # ===== æ·»åŠ è¡¨æ ¼æ±‡æ€» =====
            f.write("="*80 + "\n")
            f.write("è¯„ä¼°ç»“æœæ±‡æ€»\n")
            f.write("="*80 + "\n\n")
            
            # è¡¨å¤´
            f.write(f"{'ä»»åŠ¡ID':<30} {'æŒ‡ä»¤':<20} {'æˆåŠŸç‡':<10} {'å¹³å‡æ­¥æ•°':<12} {'å¹³å‡æ—¶é—´'}\n")
            f.write("-" * 80 + "\n")
            
            # æ¯ä¸ªä»»åŠ¡çš„æ±‡æ€»
            for task in report_data['tasks']:
                task_id = task['task_id'][:28]  # æˆªæ–­è¿‡é•¿çš„ID
                instruction = (task['instruction'][:18] if task['instruction'] else "N/A")
                success_rate = f"{task['success_rate']:.1f}%"
                avg_steps = f"{task['avg_steps']:.1f}"
                avg_time = f"{task['avg_time']:.1f}s"
                
                f.write(f"{task_id:<30} {instruction:<20} {success_rate:<10} {avg_steps:<12} {avg_time}\n")
            
            # æ€»ä½“ç»Ÿè®¡è¡Œ
            f.write("\n" + "-" * 80 + "\n")
            f.write(f"{'æ€»ä½“ç»Ÿè®¡':<30} {'N/A':<20} {summary['overall_success_rate']:.1f}% ")
            
            # è®¡ç®—å¹³å‡æ­¥æ•°å’Œæ—¶é—´
            avg_steps_all = sum(task['avg_steps'] for task in report_data['tasks']) / len(report_data['tasks'])
            avg_time_all = sum(task['avg_time'] for task in report_data['tasks']) / len(report_data['tasks'])
            f.write(f"{avg_steps_all:<12.1f} {avg_time_all:.1f}s\n")
            
            f.write(f"\næ€»ä»»åŠ¡æ•°: {report_data['metadata']['total_tasks']}\n")
            f.write(f"æ€»è¯•éªŒæ•°: {summary['total_trials']}\n")
            f.write("="*80 + "\n\n")
            
            # ===== è¯¦ç»†ä»»åŠ¡ä¿¡æ¯ =====
            f.write("="*80 + "\n")
            f.write("ä»»åŠ¡è¯¦æƒ…\n")
            f.write("="*80 + "\n\n")
            
            for task in report_data['tasks']:
                f.write(f"ä»»åŠ¡: {task['task_id']}\n")
                f.write(f"  æŒ‡ä»¤: {task['instruction']}\n")
                f.write(f"  è¯­è¨€: {task['language']}\n")
                f.write(f"  æˆåŠŸç‡: {task['success_rate']:.1f}%\n")
                f.write(f"  å¹³å‡æ­¥æ•°: {task['avg_steps']:.0f}\n")
                f.write(f"  å¹³å‡æ—¶é—´: {task['avg_time']:.1f}s\n")
                f.write(f"  è¯•éªŒè¯¦æƒ…:\n")
                for i, trial in enumerate(task['trials'], 1):
                    status = "âœ… æˆåŠŸ" if trial['success'] else "âŒ å¤±è´¥"
                    f.write(f"    Trial {i}: {status} | æ­¥æ•°: {trial['steps']:4d} | æ—¶é—´: {trial['time_seconds']:.1f}s")
                    
                    # æ·»åŠ æœ€ç»ˆåº“å­˜ä¿¡æ¯
                    if 'final_inventory' in trial and trial['final_inventory']:
                        inventory_items = [f"{item}Ã—{count}" for item, count in trial['final_inventory'].items()]
                        f.write(f" | åº“å­˜: {', '.join(inventory_items)}")
                    
                    f.write("\n")
                f.write("\n")
    
    def close(self):
        """æ¸…ç†èµ„æº"""
        if self.evaluator:
            self.evaluator.close()
        logger.info("è¯„ä¼°æ¡†æ¶å·²å…³é—­")


# å‘½ä»¤è¡Œæ¥å£
if __name__ == "__main__":
    import argparse
    import warnings
    from src.utils.logging_config import setup_evaluation_logging
    
    # é…ç½®æ—¥å¿—ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„æ ¼å¼å’Œè¿‡æ»¤å™¨ï¼‰
    setup_evaluation_logging()
    
    # è¿‡æ»¤ä¸å¿…è¦çš„è­¦å‘Šä¿¡æ¯ï¼ˆåœ¨æ¡†æ¶åˆå§‹åŒ–ä¹‹å‰ï¼‰
    # 1. PyTorch è­¦å‘Š
    warnings.filterwarnings('ignore', category=UserWarning, module='torch')
    warnings.filterwarnings('ignore', message='.*CUDA is not available.*')
    warnings.filterwarnings('ignore', message='.*Implicit dimension choice for softmax.*')
    warnings.filterwarnings('ignore', message='.*has_cuda.*')
    
    # 2. MineRL/Malmo è­¦å‘Šï¼ˆè®¾ç½® logger çº§åˆ«ï¼‰
    minerl_loggers = [
        'minerl.env.malmo.instance',
        'minerl.env._multiagent',
        'minerl.env.malmo',
    ]
    for logger_name in minerl_loggers:
        minerl_logger = logging.getLogger(logger_name)
        minerl_logger.setLevel(logging.ERROR)
    
    # 3. STEVE-1 è­¦å‘Š
    warnings.filterwarnings('ignore', category=UserWarning, module='steve1')
    
    parser = argparse.ArgumentParser(description='STEVE-1 è¯„ä¼°æ¡†æ¶')
    parser.add_argument(
        '--config',
        type=str,
        default='config/eval_tasks.yaml',
        help='ä»»åŠ¡é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: config/eval_tasks.yamlï¼‰'
    )
    parser.add_argument(
        '--task',
        type=str,
        help='è¯„ä¼°å•ä¸ªä»»åŠ¡ï¼ˆä»»åŠ¡IDï¼‰'
    )
    parser.add_argument(
        '--task-set',
        type=str,
        help='è¯„ä¼°ä»»åŠ¡é›†ï¼ˆå¦‚ harvest_tasks, quick_test, baseline_testï¼‰'
    )
    parser.add_argument(
        '--task-list',
        type=str,
        nargs='+',
        help='è¯„ä¼°ä»»åŠ¡åˆ—è¡¨ï¼ˆå¤šä¸ªä»»åŠ¡IDï¼‰'
    )
    parser.add_argument(
        '--n-trials',
        type=int,
        default=3,
        help='æ¯ä¸ªä»»åŠ¡çš„è¯•éªŒæ¬¡æ•°ï¼ˆé»˜è®¤3æ¬¡ï¼‰'
    )
    parser.add_argument(
        '--max-steps',
        type=int,
        default=2000,
        help='æ¯ä¸ªè¯•éªŒçš„æœ€å¤§æ­¥æ•°ï¼ˆé»˜è®¤2000ï¼‰'
    )
    parser.add_argument(
        '--render',
        action='store_true',
        help='å¯ç”¨æ¸¸æˆçª—å£æ¸²æŸ“ï¼ˆæ˜¾ç¤ºç”»é¢ï¼‰'
    )
    parser.add_argument(
        '--enable_video',
        action='store_true',
        help='å¯ç”¨è§†é¢‘å½•åˆ¶ï¼ˆå›ºå®šå°ºå¯¸ 640x360ï¼‰'
    )
    parser.add_argument(
        '--enable_report',
        action='store_true',
        help='ç”Ÿæˆ HTML æŠ¥å‘Š'
    )
    
    args = parser.parse_args()
    
    # è§†é¢‘å½•åˆ¶ï¼šå¦‚æœå¯ç”¨ï¼Œä½¿ç”¨å›ºå®šå°ºå¯¸ 640x360
    video_size = (640, 360) if args.enable_video else None
    
    # åˆ›å»ºé…ç½®
    config = EvaluationConfig(
        task_config_path=args.config,
        n_trials=args.n_trials,
        max_steps=args.max_steps,
        enable_render=args.render,
        enable_report=args.enable_report,
        video_size=video_size
    )
    
    # åˆ›å»ºè¯„ä¼°æ¡†æ¶
    framework = EvaluationFramework(config=config)
    
    try:
        results = []
        
        # æ ¹æ®å‚æ•°é€‰æ‹©è¯„ä¼°æ¨¡å¼
        if args.task:
            # å•ä¸ªä»»åŠ¡
            result = framework.evaluate_single_task(args.task)
            results = [result]
        
        elif args.task_set:
            # ä»»åŠ¡é›†
            results = framework.evaluate_task_set(args.task_set)
        
        elif args.task_list:
            # ä»»åŠ¡åˆ—è¡¨
            results = framework.evaluate_task_list(args.task_list)
        
        else:
            # é»˜è®¤ï¼šå¿«é€Ÿæµ‹è¯•
            logger.info("æœªæŒ‡å®šä»»åŠ¡ï¼Œè¿è¡Œå¿«é€Ÿæµ‹è¯•...")
            results = framework.evaluate_task_set('quick_test')
        
        # æ‰“å°æ‘˜è¦
        framework.print_summary(results)
        
        # ç”ŸæˆæŠ¥å‘Š
        framework.generate_report(results)
        
        # é‡ç½® task-set ç›®å½•ï¼ˆé¿å…å½±å“åç»­è¯„ä¼°ï¼‰
        framework.current_task_set_dir = None
        
    except KeyboardInterrupt:
        logger.info("\nç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    finally:
        framework.close()
