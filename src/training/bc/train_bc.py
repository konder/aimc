#!/usr/bin/env python3
"""
è¡Œä¸ºå…‹éš†(Behavioral Cloning)è®­ç»ƒè„šæœ¬

ä»ä¸“å®¶æ¼”ç¤ºæ•°æ®è®­ç»ƒç­–ç•¥ã€‚è¿™æ˜¯DAggerçš„åŸºç¡€ï¼Œä¹Ÿå¯ä»¥å•ç‹¬ä½¿ç”¨ã€‚

Usage:
    # ä»æ‰‹åŠ¨å½•åˆ¶çš„æ¼”ç¤ºè®­ç»ƒ
    python src/training/train_bc.py \
        --data data/expert_demos/round_0/ \
        --output checkpoints/bc_round_0.zip \
        --epochs 30

    # ä»DAggerèšåˆæ•°æ®è®­ç»ƒ
    python src/training/train_bc.py \
        --data data/dagger_combined/iter_1.pkl \
        --output checkpoints/dagger_iter_1.zip
"""

import os
import sys
import argparse
import numpy as np
import pickle
from pathlib import Path
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import minedojo
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from src.envs import make_minedojo_env


class ExpertDataset(Dataset):
    """ä¸“å®¶æ¼”ç¤ºæ•°æ®é›†"""
    
    def __init__(self, observations, actions):
        """
        Args:
            observations: numpy array of shape (N, C, H, W), uint8 [0, 255] æˆ– float32
            actions: numpy array of shape (N, 8) - MineDojo MultiDiscrete
        """
        # å½’ä¸€åŒ–å›¾åƒåˆ°[0, 1]
        if observations.dtype == np.uint8:
            # uint8 ç±»å‹ï¼Œéœ€è¦å½’ä¸€åŒ–
            observations = observations.astype(np.float32) / 255.0
        elif observations.dtype in [np.float32, np.float64]:
            # float ç±»å‹ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦å½’ä¸€åŒ–
            if observations.max() > 1.5:
                # å€¼åŸŸåœ¨ [0, 255]ï¼Œéœ€è¦å½’ä¸€åŒ–
                print(f"  âš ï¸  æ£€æµ‹åˆ°æœªå½’ä¸€åŒ–çš„ float æ•°æ® (èŒƒå›´: [{observations.min():.1f}, {observations.max():.1f}])ï¼Œæ­£åœ¨å½’ä¸€åŒ–...")
                observations = observations.astype(np.float32) / 255.0
            # å¦åˆ™å‡è®¾å·²ç»å½’ä¸€åŒ–åˆ° [0, 1]
        
        self.observations = torch.FloatTensor(observations)
        self.actions = torch.LongTensor(actions)
    
    def __len__(self):
        return len(self.observations)
    
    def __getitem__(self, idx):
        return self.observations[idx], self.actions[idx]


def load_expert_demonstrations(data_path):
    """
    åŠ è½½ä¸“å®¶æ¼”ç¤ºæ•°æ®
    
    æ”¯æŒä¸¤ç§æ ¼å¼:
    1. ç›®å½•: åŒ…å«å¤šä¸ª.npyæ–‡ä»¶ï¼ˆæ‰‹åŠ¨å½•åˆ¶ï¼‰
    2. .pklæ–‡ä»¶: å•ä¸ªpickleæ–‡ä»¶ï¼ˆDAggeræ ‡æ³¨ï¼‰
    
    Returns:
        observations: (N, C, H, W)
        actions: (N, 8)
    """
    observations = []
    actions = []
    
    data_path = Path(data_path)
    
    if data_path.is_dir():
        # ä»ç›®å½•åŠ è½½å¤šä¸ª.npyæ–‡ä»¶
        print(f"ä»ç›®å½•åŠ è½½: {data_path}")
        
        # 1. é¦–å…ˆæŸ¥æ‰¾episodeå­ç›®å½•ï¼ˆrecord_manual_chopping.pyæ ¼å¼ï¼‰
        episode_dirs = sorted(data_path.glob("episode_*/"))
        if episode_dirs:
            print(f"  æ‰¾åˆ° {len(episode_dirs)} ä¸ªepisodeç›®å½•")
            for ep_dir in episode_dirs:
                frame_files = sorted(ep_dir.glob("frame_*.npy"))
                if frame_files:
                    print(f"  [{ep_dir.name}] åŠ è½½ {len(frame_files)} ä¸ªå¸§...")
                    for file in frame_files:
                        try:
                            frame_data = np.load(file, allow_pickle=True).item()
                            obs = frame_data['observation']
                            action = frame_data['action']
                            observations.append(obs)
                            actions.append(action)
                        except Exception as e:
                            print(f"    âš ï¸  {file.name}: åŠ è½½å¤±è´¥ - {e}")
                    print(f"    âœ“ {ep_dir.name}: æˆåŠŸåŠ è½½ {len(frame_files)} å¸§")
        
        # 2. å¦‚æœæ²¡æœ‰episodeå­ç›®å½•ï¼ŒæŸ¥æ‰¾å½“å‰ç›®å½•çš„episode_*.npyæ–‡ä»¶
        else:
            episode_files = sorted(data_path.glob("episode_*.npy"))
            frame_files = sorted(data_path.glob("frame_*.npy"))
            
            if episode_files:
                # run_policy_collect_states.py æ ¼å¼
                for file in episode_files:
                    try:
                        episode_data = np.load(file, allow_pickle=True).item()
                        states = episode_data['states']
                        episode_actions = episode_data.get('actions', [])
                        
                        for state, action in zip(states, episode_actions):
                            obs = state['observation']
                            observations.append(obs)
                            actions.append(action)
                        
                        print(f"  âœ“ {file.name}: {len(states)} å¸§")
                    except Exception as e:
                        print(f"  âš ï¸  {file.name}: åŠ è½½å¤±è´¥ - {e}")
            
            elif frame_files:
                # record_manual_chopping.py æ ¼å¼ï¼ˆå•episodeç›®å½•ï¼‰
                for file in frame_files:
                    try:
                        frame_data = np.load(file, allow_pickle=True).item()
                        obs = frame_data['observation']
                        action = frame_data['action']
                        observations.append(obs)
                        actions.append(action)
                    except Exception as e:
                        print(f"  âš ï¸  {file.name}: åŠ è½½å¤±è´¥ - {e}")
                
                if observations:
                    print(f"  âœ“ åŠ è½½äº† {len(frame_files)} ä¸ªå¸§æ–‡ä»¶")
            
            else:
                print(f"  âš ï¸  ç›®å½•ä¸­æœªæ‰¾åˆ°episodeç›®å½•æˆ–frameæ–‡ä»¶")
    
    elif data_path.suffix == '.pkl':
        # ä»pickleæ–‡ä»¶åŠ è½½ï¼ˆDAggeræ ‡æ³¨æ ¼å¼ï¼‰
        print(f"ä»pickleæ–‡ä»¶åŠ è½½: {data_path}")
        
        with open(data_path, 'rb') as f:
            labeled_data = pickle.load(f)
        
        for item in labeled_data:
            observations.append(item['observation'])
            # ä½¿ç”¨ä¸“å®¶æ ‡æ³¨çš„åŠ¨ä½œï¼Œè€Œéç­–ç•¥åŠ¨ä½œ
            actions.append(item['expert_action'])
        
        print(f"  âœ“ åŠ è½½äº† {len(labeled_data)} ä¸ªæ ‡æ³¨")
    
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {data_path}")
    
    if not observations:
        raise ValueError("æœªåŠ è½½åˆ°ä»»ä½•æ•°æ®ï¼")
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    observations = np.array(observations)
    actions = np.array(actions)
    
    # è½¬ç½®è§‚å¯Ÿæ•°æ®: (N, H, W, C) -> (N, C, H, W)
    # å› ä¸ºPyTorch CNNæœŸæœ›channel-firstæ ¼å¼
    if len(observations.shape) == 4 and observations.shape[-1] == 3:
        print(f"  è½¬ç½®å›¾åƒ: (N, H, W, C) -> (N, C, H, W)")
        observations = np.transpose(observations, (0, 3, 1, 2))
    
    print(f"\næ€»è®¡:")
    print(f"  è§‚å¯Ÿ: {observations.shape}")
    print(f"  åŠ¨ä½œ: {actions.shape}")
    
    return observations, actions


def train_bc_with_ppo(
    observations,
    actions,
    output_path,
    task_id="harvest_1_log",
    learning_rate=3e-4,
    n_epochs=30,
    batch_size=64,
    device="auto"
):
    """
    ä½¿ç”¨PPOæ¡†æ¶è¿›è¡Œè¡Œä¸ºå…‹éš†è®­ç»ƒ
    
    ç­–ç•¥: é€šè¿‡é¢„è®­ç»ƒç­–ç•¥ç½‘ç»œï¼Œç„¶åç”¨PPOå¾®è°ƒ
    
    Args:
        observations: ä¸“å®¶è§‚å¯Ÿ
        actions: ä¸“å®¶åŠ¨ä½œ
        output_path: è¾“å‡ºæ¨¡å‹è·¯å¾„
        task_id: MineDojoä»»åŠ¡ID
        learning_rate: å­¦ä¹ ç‡
        n_epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        device: è®¡ç®—è®¾å¤‡
    """
    
    # MPSè®¾å¤‡ç¨³å®šæ€§è°ƒæ•´
    if device == "mps":
        print(f"\nâš ï¸  MPSè®¾å¤‡æ£€æµ‹åˆ°ï¼Œåº”ç”¨ç¨³å®šæ€§ä¼˜åŒ–:")
        print(f"  - é™ä½å­¦ä¹ ç‡: 3e-4 -> 1e-4")
        print(f"  - å¯ç”¨æ¢¯åº¦è£å‰ª")
        print(f"  - æ·»åŠ æ•°å€¼ç¨³å®šæ€§ä¿æŠ¤\n")
        learning_rate = 1e-4  # MPSä¸Šä½¿ç”¨æ›´ä½çš„å­¦ä¹ ç‡
    
    print(f"\n{'='*60}")
    print(f"è¡Œä¸ºå…‹éš†è®­ç»ƒ")
    print(f"{'='*60}")
    print(f"æ•°æ®é‡: {len(observations)} æ ·æœ¬")
    print(f"å­¦ä¹ ç‡: {learning_rate}")
    print(f"è®­ç»ƒè½®æ•°: {n_epochs}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"è®¾å¤‡: {device}")
    print(f"{'='*60}\n")
    
    # åˆ›å»ºç¯å¢ƒï¼ˆç”¨äºè·å–observation_spaceå’Œaction_spaceï¼‰
    print("åˆ›å»ºç¯å¢ƒ...")
    def make_env():
        return make_minedojo_env(
            task_id=task_id,
            max_episode_steps=1000
        )
    
    env = DummyVecEnv([make_env])
    
    # åˆ›å»ºPPOæ¨¡å‹
    print("åˆ›å»ºPPOæ¨¡å‹...")
    # æ³¨æ„: normalize_images=False å› ä¸ºMinedojoWrapperå·²ç»å½’ä¸€åŒ–åˆ°[0,1]
    model = PPO(
        "CnnPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=2048,
        batch_size=batch_size,
        n_epochs=10,  # PPOå†…éƒ¨epoch
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        vf_coef=0.5,
        max_grad_norm=0.5,
        device=device,
        verbose=1,
        policy_kwargs=dict(normalize_images=False)  # å›¾åƒå·²åœ¨ç¯å¢ƒä¸­å½’ä¸€åŒ–
    )
    
    # é¢„è®­ç»ƒç­–ç•¥ç½‘ç»œï¼ˆè¡Œä¸ºå…‹éš†ï¼‰
    print(f"\nå¼€å§‹è¡Œä¸ºå…‹éš†é¢„è®­ç»ƒ...")
    print(f"{'='*60}\n")
    
    dataset = ExpertDataset(observations, actions)
    
    # éªŒè¯æ•°æ®å½’ä¸€åŒ–ï¼ˆè°ƒè¯•ä¿¡æ¯ï¼‰
    sample_obs = dataset.observations[:4]
    print(f"æ•°æ®é›†æ ·æœ¬æ£€æŸ¥:")
    print(f"  å½¢çŠ¶: {sample_obs.shape}")
    print(f"  ç±»å‹: {sample_obs.dtype}")
    print(f"  èŒƒå›´: [{sample_obs.min().item():.3f}, {sample_obs.max().item():.3f}]")
    print(f"  å‡å€¼: {sample_obs.mean().item():.3f}")
    print(f"  æ ‡å‡†å·®: {sample_obs.std().item():.3f}")
    
    if sample_obs.max() > 1.5:
        print(f"  ğŸ”´ é”™è¯¯: æ•°æ®æœªæ­£ç¡®å½’ä¸€åŒ–ï¼åº”è¯¥åœ¨[0,1]èŒƒå›´å†…")
        print(f"  â†’ è¿™ä¼šå¯¼è‡´è®­ç»ƒå¤±è´¥ï¼Œç‰¹åˆ«æ˜¯åœ¨ MPS/CUDA è®¾å¤‡ä¸Š")
        raise ValueError("æ•°æ®å½’ä¸€åŒ–å¤±è´¥ï¼è¯·æ£€æŸ¥æ•°æ®åŠ è½½æµç¨‹")
    elif sample_obs.max() < 0.01:
        print(f"  âš ï¸  è­¦å‘Š: æ•°æ®å¯èƒ½å…¨ä¸º0æˆ–è¿‡æš—")
    else:
        print(f"  âœ“ æ•°æ®å½’ä¸€åŒ–æ­£ç¡®\n")
    
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=0  # macOSä¸Šé¿å…å¤šè¿›ç¨‹é—®é¢˜
    )
    
    # è·å–ç­–ç•¥ç½‘ç»œ
    policy_net = model.policy
    optimizer = torch.optim.Adam(
        policy_net.parameters(), 
        lr=learning_rate
    )
    criterion = nn.CrossEntropyLoss()
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(n_epochs):
        epoch_loss = 0.0
        epoch_acc = 0.0
        num_batches = 0
        num_skipped = 0
        
        for batch_obs, batch_actions in dataloader:
            batch_obs = batch_obs.to(model.device)
            batch_actions = batch_actions.to(model.device)
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            
            # PPOç­–ç•¥è¾“å‡º
            features = policy_net.extract_features(batch_obs)
            latent_pi = policy_net.mlp_extractor.forward_actor(features)
            action_logits = policy_net.action_net(latent_pi)
            
            # MineDojo MultiDiscrete: 8ä¸ªç¦»æ•£åŠ¨ä½œ
            # action_logits shape: (batch_size, sum of action dimensions)
            # æˆ‘ä»¬éœ€è¦åˆ†åˆ«è®¡ç®—æ¯ä¸ªç»´åº¦çš„loss
            
            # ç®€åŒ–ç‰ˆæœ¬: å‡è®¾æ‰€æœ‰åŠ¨ä½œç»´åº¦ç›¸åŒï¼ˆå®é™…ä¸ŠMineDojoä¸æ˜¯è¿™æ ·ï¼‰
            # æ›´ç²¾ç¡®çš„å®ç°éœ€è¦æ ¹æ®MultiDiscreteçš„nvecåˆ†å‰²logits
            
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ç”¨æ•´ä½“actioné¢„æµ‹
            # æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…DAggerå¯èƒ½éœ€è¦æ›´ç²¾ç¡®çš„å®ç°
            
            loss = 0.0
            acc = 0.0
            
            # è¿™é‡Œä½¿ç”¨ç®€åŒ–çš„æŸå¤±è®¡ç®—
            # å®é™…åº”è¯¥æ ¹æ®MultiDiscreteçš„å„ä¸ªç»´åº¦åˆ†åˆ«è®¡ç®—
            # ä½†å¯¹äºåˆå§‹BCè®­ç»ƒï¼Œè¿™ä¸ªç®€åŒ–ç‰ˆæœ¬é€šå¸¸å¤Ÿç”¨
            
            # ä½¿ç”¨ç­–ç•¥çš„distributionæ¥è®¡ç®—log_prob
            actions_tensor = batch_actions
            distribution = policy_net.get_distribution(batch_obs)
            log_prob = distribution.log_prob(actions_tensor)
            
            # BC loss = -log_prob (æœ€å¤§åŒ–ä¸“å®¶åŠ¨ä½œçš„æ¦‚ç‡)
            loss = -log_prob.mean()
            
            # æ£€æŸ¥lossæœ‰æ•ˆæ€§ï¼ˆMPSç¨³å®šæ€§ä¿æŠ¤ï¼‰
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"  è­¦å‘Š: Batch {num_batches} lossæ— æ•ˆ (NaN/Inf)ï¼Œè·³è¿‡")
                num_skipped += 1
                continue
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªï¼ˆå…³é”®ï¼MPSè®¾å¤‡å¿…éœ€ï¼‰
            torch.nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=0.5)
            
            optimizer.step()
            
            epoch_loss += loss.item()
            num_batches += 1
        
        avg_loss = epoch_loss / num_batches if num_batches > 0 else float('inf')
        
        status_msg = f"Epoch {epoch+1}/{n_epochs} | Loss: {avg_loss:.4f}"
        if num_skipped > 0:
            status_msg += f" | è·³è¿‡: {num_skipped} batches"
        print(status_msg)
    
    print(f"\n{'='*60}")
    print(f"é¢„è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*60}\n")
    
    # ä¿å­˜æ¨¡å‹
    print(f"ä¿å­˜æ¨¡å‹åˆ°: {output_path}")
    os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else ".", exist_ok=True)
    model.save(output_path)
    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜\n")
    
    env.close()
    
    return model


def main():
    parser = argparse.ArgumentParser(
        description="è¡Œä¸ºå…‹éš†(BC)è®­ç»ƒ - ä»ä¸“å®¶æ¼”ç¤ºå­¦ä¹ ç­–ç•¥"
    )
    
    parser.add_argument(
        "--data",
        type=str,
        required=True,
        help="ä¸“å®¶æ¼”ç¤ºæ•°æ®è·¯å¾„ï¼ˆç›®å½•æˆ–.pklæ–‡ä»¶ï¼‰"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="è¾“å‡ºæ¨¡å‹è·¯å¾„ï¼ˆ.zipæ–‡ä»¶ï¼‰"
    )
    
    parser.add_argument(
        "--task-id",
        type=str,
        default="harvest_1_log",
        help="MineDojoä»»åŠ¡IDï¼ˆé»˜è®¤: harvest_1_logï¼‰"
    )
    
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=3e-4,
        help="å­¦ä¹ ç‡ï¼ˆé»˜è®¤: 3e-4ï¼‰"
    )
    
    parser.add_argument(
        "--epochs",
        type=int,
        default=30,
        help="è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤: 30ï¼‰"
    )
    
    parser.add_argument(
        "--batch-size",
        type=int,
        default=64,
        help="æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤: 64ï¼‰"
    )
    
    parser.add_argument(
        "--device",
        type=str,
        default="auto",
        help="è®¡ç®—è®¾å¤‡ (auto/cpu/cuda/mpsï¼Œé»˜è®¤: auto)"
    )
    
    args = parser.parse_args()
    
    # éªŒè¯æ•°æ®è·¯å¾„å­˜åœ¨
    if not os.path.exists(args.data):
        print(f"âœ— é”™è¯¯: æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.data}")
        sys.exit(1)
    
    # åŠ è½½ä¸“å®¶æ¼”ç¤º
    try:
        observations, actions = load_expert_demonstrations(args.data)
    except Exception as e:
        print(f"âœ— åŠ è½½æ•°æ®å¤±è´¥: {e}")
        sys.exit(1)
    
    # è®­ç»ƒBC
    train_bc_with_ppo(
        observations=observations,
        actions=actions,
        output_path=args.output,
        task_id=args.task_id,
        learning_rate=args.learning_rate,
        n_epochs=args.epochs,
        batch_size=args.batch_size,
        device=args.device
    )
    
    print(f"âœ“ BCè®­ç»ƒå®Œæˆï¼")
    print(f"\nä¸‹ä¸€æ­¥:")
    print(f"  1. è¯„ä¼°æ¨¡å‹: python tools/evaluate_policy.py --model {args.output}")
    print(f"  2. æ”¶é›†çŠ¶æ€: python tools/run_policy_collect_states.py --model {args.output}\n")


if __name__ == "__main__":
    main()

