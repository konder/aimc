"""
é›¶æ ·æœ¬è¯„ä¼°å®Œæ•´ç‰ˆVPT Agent

åœ¨harvest_logä»»åŠ¡ä¸Šè¯„ä¼°å®Œæ•´ç‰ˆVPTçš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œå¯¹æ¯”ç®€åŒ–ç‰ˆ
"""

import os
import sys
import argparse
import numpy as np
from tqdm import tqdm

PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
sys.path.insert(0, PROJECT_ROOT)

from src.training.vpt import VPTAgent
from src.envs.task_wrappers import HarvestLogWrapper


def create_env():
    """åˆ›å»ºMineDojoç¯å¢ƒ"""
    import minedojo
    from src.envs.task_wrappers import HarvestLogWrapper
    
    base_env = minedojo.make(
        task_id="harvest_1_log",
        image_size=(160, 256),
        world_seed=42,
        seed=42,
    )
    env = HarvestLogWrapper(base_env, required_logs=1, verbose=False)
    return env


def evaluate_agent(agent, num_episodes=10, max_steps=1200, verbose=True, debug_actions=False):
    """
    è¯„ä¼°agentåœ¨ç¯å¢ƒä¸­çš„è¡¨ç°
    
    æ¯ä¸ªepisodeåˆ›å»ºæ–°ç¯å¢ƒï¼Œé¿å…çŠ¶æ€æ±¡æŸ“
    
    Args:
        agent: Agentå®ä¾‹
        num_episodes: è¯„ä¼°è½®æ•°
        max_steps: æ¯è½®æœ€å¤§æ­¥æ•°
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        debug_actions: æ˜¯å¦æ‰“å°åŠ¨ä½œè¯¦æƒ…ï¼ˆè°ƒè¯•ç”¨ï¼‰
    
    Returns:
        stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
    """
    success_count = 0
    total_reward = 0
    episode_lengths = []
    episode_rewards = []
    
    for episode_idx in range(num_episodes):
        # æ¯ä¸ªepisodeåˆ›å»ºæ–°ç¯å¢ƒ
        env = None
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                env = create_env()
                obs = env.reset()
                break
            except Exception as e:
                retry_count += 1
                if env:
                    try:
                        env.close()
                    except:
                        pass
                    env = None
                if retry_count < max_retries:
                    if verbose:
                        print(f"  âš ï¸  ç¯å¢ƒé‡ç½®å¤±è´¥ï¼Œé‡è¯• {retry_count}/{max_retries}...")
                        print(f"      é”™è¯¯: {str(e)[:100]}")
                    import time
                    time.sleep(2)  # å¢åŠ ç­‰å¾…æ—¶é—´åˆ°2ç§’
                else:
                    if verbose:
                        print(f"  âŒ Episode {episode_idx + 1} ç¯å¢ƒåˆ›å»ºå¤±è´¥ï¼Œè·³è¿‡")
                        print(f"      æœ€åé”™è¯¯: {str(e)[:100]}")
                    break  # è·³å‡ºwhileå¾ªç¯
        
        if env is None:
            continue  # è·³è¿‡æ­¤episode
        
        # ä¸´æ—¶å¯ç”¨actionè°ƒè¯•
        original_debug = agent.debug_actions
        agent.debug_actions = debug_actions
        
        try:
            agent.reset()
            
            episode_reward = 0
            done = False
            step_count = 0
            
            if verbose:
                print(f"\nğŸ“ Episode {episode_idx + 1}/{num_episodes}")
            
            while not done and step_count < max_steps:
                # é¢„æµ‹åŠ¨ä½œ
                action = agent.predict(obs, deterministic=True)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                obs, reward, done, info = env.step(action)
                episode_reward += reward
                step_count += 1
                
                # æ‰“å°è¿›åº¦
                if verbose and step_count % 100 == 0:
                    print(f"  Step {step_count}/{max_steps}, Reward={episode_reward:.2f}")
                
                # æ£€æŸ¥æˆåŠŸ
                if reward > 0:
                    success_count += 1
                    if verbose:
                        print(f"  âœ… æˆåŠŸï¼ç¬¬{step_count}æ­¥è·å¾—å¥–åŠ±: {reward}")
                    break
            
            episode_lengths.append(step_count)
            episode_rewards.append(episode_reward)
            total_reward += episode_reward
            
            if verbose:
                status = "âœ… æˆåŠŸ" if episode_reward > 0 else "âŒ å¤±è´¥"
                print(f"  {status} - æ­¥æ•°: {step_count}, ç´¯ç§¯å¥–åŠ±: {episode_reward:.2f}")
        
        finally:
            # æ¢å¤debugè®¾ç½®
            agent.debug_actions = original_debug
            # å…³é—­ç¯å¢ƒ
            if env:
                try:
                    env.close()
                except Exception as e:
                    if verbose:
                        print(f"  âš ï¸  ç¯å¢ƒå…³é—­å‡ºé”™: {e}")
    
    stats = {
        'success_count': success_count,
        'success_rate': success_count / num_episodes,
        'avg_reward': total_reward / num_episodes,
        'total_reward': total_reward,
        'avg_steps': np.mean(episode_lengths),
        'episode_lengths': episode_lengths,
        'episode_rewards': episode_rewards
    }
    
    return stats


def main():
    parser = argparse.ArgumentParser(description='é›¶æ ·æœ¬è¯„ä¼°VPT Agent')
    parser.add_argument('--episodes', type=int, default=10,
                        help='è¯„ä¼°è½®æ•°')
    parser.add_argument('--max_steps', type=int, default=1200,
                        help='æ¯è½®æœ€å¤§æ­¥æ•°')
    parser.add_argument('--device', type=str, default='auto',
                        help='è®¾å¤‡: cpu/cuda/mps/auto')
    parser.add_argument('--weights', type=str, 
                        default='data/pretrained/vpt/rl-from-early-game-2x.weights',
                        help='VPTæƒé‡è·¯å¾„')
    parser.add_argument('--debug-actions', action='store_true',
                        help='æ‰“å°è¯¦ç»†çš„åŠ¨ä½œè½¬æ¢æ—¥å¿—ï¼ˆè°ƒè¯•ç”¨ï¼‰')
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("ğŸ¯ VPTé›¶æ ·æœ¬è¯„ä¼°")
    print("="*70)
    print(f"ä»»åŠ¡: harvest_1_log")
    print(f"è¯„ä¼°è½®æ•°: {args.episodes}")
    print(f"æœ€å¤§æ­¥æ•°: {args.max_steps}")
    print(f"è®¾å¤‡: {args.device}")
    if args.debug_actions:
        print(f"è°ƒè¯•æ¨¡å¼: å¯ç”¨åŠ¨ä½œæ—¥å¿—")
    print("="*70 + "\n")
    
    # è¯„ä¼°VPT Agent
    print("="*70)
    print("ğŸ“Š è¯„ä¼°VPT Agent (å®Œæ•´ç‰ˆ: pi_head + hidden state)")
    print("="*70)
    
    agent = VPTAgent(
        vpt_weights_path=args.weights,
        device=args.device,
        verbose=True,
        debug_actions=False  # é€šè¿‡evaluate_agentå‚æ•°æ§åˆ¶
    )
    agent.eval()
    
    print("\nğŸ’¡ æç¤º: æ¯ä¸ªepisodeä¼šåˆ›å»ºæ–°ç¯å¢ƒï¼Œé¿å…çŠ¶æ€æ±¡æŸ“")
    print("="*70 + "\n")
    
    stats = evaluate_agent(
        agent,
        num_episodes=args.episodes,
        max_steps=args.max_steps,
        verbose=True,
        debug_actions=args.debug_actions
    )
    
    print("\n" + "-"*70)
    print("ğŸ“ˆ è¯„ä¼°ç»Ÿè®¡ç»“æœ:")
    print("-"*70)
    print(f"æˆåŠŸç‡: {stats['success_rate']*100:.1f}% ({stats['success_count']}/{args.episodes})")
    print(f"å¹³å‡å¥–åŠ±: {stats['avg_reward']:.3f}")
    print(f"å¹³å‡æ­¥æ•°: {stats['avg_steps']:.1f}")
    print("-"*70 + "\n")
    
    print("âœ… è¯„ä¼°å®Œæˆï¼")


if __name__ == '__main__':
    main()

