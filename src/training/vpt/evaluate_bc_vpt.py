#!/usr/bin/env python3
"""
è¯„ä¼°VPT BCå¾®è°ƒåçš„æ¨¡å‹

åœ¨MineDojoç¯å¢ƒä¸­è¿è¡Œå¤šä¸ªepisodesï¼Œç»Ÿè®¡æˆåŠŸç‡
"""

import os
import sys
import argparse
import numpy as np
import torch as th
import torch.nn as nn
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
sys.path.insert(0, PROJECT_ROOT)

# æ·»åŠ externalè·¯å¾„
EXTERNAL_PATH = os.path.join(PROJECT_ROOT, 'external')
sys.path.insert(0, EXTERNAL_PATH)

from src.models.vpt import load_vpt_policy
from src.models.vpt.lib.policy import MinecraftPolicy
from src.envs import make_minedojo_env
import minedojo


class MinedojoActionAdapter(nn.Module):
    """
    é€‚é…VPTçš„è¾“å‡ºåˆ°MineDojo action space
    """
    
    def __init__(self, vpt_policy: MinecraftPolicy):
        super().__init__()
        self.vpt_policy = vpt_policy
        
        # MineDojo action dimensions
        self.minedojo_action_dim = [3, 3, 4, 25, 25, 8, 244, 36]
        
        # åˆ›å»ºaction heads
        hidden_dim = 2048
        self.action_heads = nn.ModuleList([
            nn.Linear(hidden_dim, dim) for dim in self.minedojo_action_dim
        ])
    
    def forward(self, obs):
        """
        Args:
            obs: (B, H, W, C) numpy array or tensor
        Returns:
            action_logits: list of tensors, one per action dimension
        """
        batch_size = obs.shape[0]
        
        # è½¬æ¢ä¸ºtensor (å¦‚æœè¿˜ä¸æ˜¯)
        if isinstance(obs, np.ndarray):
            obs = th.from_numpy(obs).float()
        
        # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        obs = obs.to(next(self.vpt_policy.parameters()).device)
        
        # æ·»åŠ æ—¶é—´ç»´åº¦ï¼š(B, H, W, C) -> (B, T=1, H, W, C)
        obs_vpt = obs.unsqueeze(1)  # (B, 1, H, W, C)
        
        # VPTå‰å‘ä¼ æ’­
        x = self.vpt_policy.img_preprocess(obs_vpt)
        x = self.vpt_policy.img_process(x)
        x = x.squeeze(1)  # ç§»é™¤æ—¶é—´ç»´åº¦
        latent = self.vpt_policy.lastlayer(x)
        
        # é€šè¿‡action heads
        action_logits = [head(latent) for head in self.action_heads]
        
        return action_logits
    
    def predict(self, obs, deterministic=True):
        """
        é¢„æµ‹åŠ¨ä½œ
        
        Args:
            obs: (H, W, C) æˆ– (C, H, W) å•ä¸ªè§‚å¯Ÿ
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§åŠ¨ä½œ
        
        Returns:
            action: MineDojo action (list of ints)
        """
        # æ£€æŸ¥å¹¶è½¬æ¢ä¸ºHWCæ ¼å¼
        if len(obs.shape) == 3:
            # æ£€æµ‹CHWæ ¼å¼: (C, H, W)
            if obs.shape[0] == 3 or obs.shape[0] == 1:
                if obs.shape[0] < obs.shape[1] and obs.shape[0] < obs.shape[2]:
                    # (C, H, W) -> (H, W, C)
                    obs = np.transpose(obs, (1, 2, 0))
        
        # å¤„ç†æ•°æ®ç±»å‹å’ŒèŒƒå›´
        is_normalized = obs.dtype in [np.float32, np.float64] and obs.max() <= 1.0
        
        if not is_normalized:
            # uint8 [0,255] -> float32 [0,1]
            obs = obs.astype(np.float32) / 255.0
        else:
            # ç¡®ä¿æ˜¯float32
            obs = obs.astype(np.float32)
        
        # Resizeåˆ°128x128ï¼ˆVPTè®­ç»ƒå°ºå¯¸ï¼‰
        import cv2
        if obs.shape[:2] != (128, 128):
            obs = cv2.resize(obs, (128, 128), interpolation=cv2.INTER_LINEAR)
        
        # æ·»åŠ batchç»´åº¦
        obs = obs[np.newaxis, ...]  # (1, H, W, C)
        
        with th.no_grad():
            action_logits = self.forward(obs)
            
            # é‡‡æ ·æˆ–å–argmax
            actions = []
            for logits in action_logits:
                if deterministic:
                    action = th.argmax(logits, dim=-1)
                else:
                    probs = th.softmax(logits, dim=-1)
                    action = th.multinomial(probs, 1).squeeze(-1)
                actions.append(action.item())
        
        return actions


def evaluate_policy(
    adapter: MinedojoActionAdapter,
    task_id: str = "harvest_1_log",
    num_episodes: int = 20,
    max_steps: int = 1000,
    image_size: tuple = (160, 256),  # ä½¿ç”¨ä¸å½•åˆ¶ä¸€è‡´çš„å°ºå¯¸(160, 256)ï¼Œåœ¨predictä¸­resizeåˆ°VPTçš„128x128
    device: str = "cpu",
    render: bool = False
):
    """
    è¯„ä¼°policyæ€§èƒ½
    
    Args:
        adapter: MinedojoActionAdapter
        task_id: ä»»åŠ¡ID
        num_episodes: è¯„ä¼°episodesæ•°
        max_steps: æ¯ä¸ªepisodeæœ€å¤§æ­¥æ•°
        image_size: å›¾åƒå°ºå¯¸ï¼ˆä¸å½•åˆ¶æ—¶ä¸€è‡´ï¼‰
        device: è®¾å¤‡
        render: æ˜¯å¦æ¸²æŸ“
    
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    # è¯„ä¼°é…ç½®
    print(f"è¯„ä¼°é…ç½®:")
    print(f"  ä»»åŠ¡: {task_id}")
    print(f"  å›¾åƒå°ºå¯¸: {image_size}")
    print(f"  æœ€å¤§æ­¥æ•°: {max_steps}")
    print(f"  Episodes: {num_episodes}")
    
    # è¯„ä¼°ç»“æœ
    results = {
        'episodes': [],
        'success_count': 0,
        'total_episodes': num_episodes
    }
    
    print(f"\nå¼€å§‹è¯„ä¼° ({num_episodes} episodes)...")
    print("  æ¯ä¸ªepisodeç‹¬ç«‹åˆ›å»ºç¯å¢ƒï¼Œç»“æŸåå…³é—­")
    
    adapter.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    
    for episode in tqdm(range(num_episodes), desc="Evaluating"):
        # ä¸ºæ¯ä¸ªepisodeåˆ›å»ºæ–°ç¯å¢ƒ
        env = make_minedojo_env(
            task_id=task_id,
            image_size=image_size,
            max_episode_steps=max_steps
        )
        
        try:
            obs = env.reset()
            episode_reward = 0
            episode_steps = 0
            done = False
            
            for step in range(max_steps):
                # é¢„æµ‹åŠ¨ä½œ
                rgb_obs = obs['rgb'] if isinstance(obs, dict) else obs
                action = adapter.predict(rgb_obs, deterministic=True)
                
                # æ‰§è¡Œ
                obs, reward, done, info = env.step(action)
                episode_reward += reward
                episode_steps += 1
                
                if done:
                    break
            
            # åˆ¤æ–­æˆåŠŸ
            success = done and episode_reward > 0
            
            if success:
                results['success_count'] += 1
            
            results['episodes'].append({
                'episode': episode,
                'reward': episode_reward,
                'steps': episode_steps,
                'success': success,
                'done': done
            })
            
            # å®æ—¶æ˜¾ç¤º
            if (episode + 1) % 5 == 0 or episode == 0:
                current_success_rate = results['success_count'] / (episode + 1)
                print(f"  Episode {episode+1}/{num_episodes}: "
                      f"Success Rate = {current_success_rate:.2%}")
        
        finally:
            # ç¡®ä¿ç¯å¢ƒè¢«å…³é—­
            env.close()
    
    # è®¡ç®—ç»Ÿè®¡
    results['success_rate'] = results['success_count'] / num_episodes
    results['avg_reward'] = np.mean([ep['reward'] for ep in results['episodes']])
    results['avg_steps'] = np.mean([ep['steps'] for ep in results['episodes']])
    
    return results


def load_trained_model(checkpoint_path: str, device: str = 'cpu'):
    """
    åŠ è½½è®­ç»ƒå¥½çš„VPTæ¨¡å‹
    
    Args:
        checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡
    
    Returns:
        adapter: MinedojoActionAdapter
    """
    print(f"åŠ è½½æ¨¡å‹: {checkpoint_path}")
    
    # åŠ è½½checkpoint
    checkpoint = th.load(checkpoint_path, map_location=device)
    
    # æ˜¾ç¤ºcheckpointä¿¡æ¯
    print(f"\nğŸ“¦ Checkpointä¿¡æ¯:")
    print(f"  è®­ç»ƒEpoch: {checkpoint.get('epoch', 'unknown')}")
    print(f"  è®­ç»ƒLoss: {checkpoint.get('loss', 'unknown'):.4f}" if isinstance(checkpoint.get('loss'), (int, float)) else f"  è®­ç»ƒLoss: {checkpoint.get('loss', 'unknown')}")
    print(f"  VPTæƒé‡è·¯å¾„: {checkpoint.get('vpt_weights_path', 'unknown')}")
    
    # åˆ›å»ºVPT policyï¼ˆæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼‰
    print(f"\nğŸ¤– åŠ è½½VPTé¢„è®­ç»ƒæƒé‡...")
    vpt_weights_path = checkpoint.get('vpt_weights_path', 'data/pretrained/vpt/rl-from-early-game-2x.weights')
    policy, load_result = load_vpt_policy(
        vpt_weights_path,
        device=device,
        verbose=True  # æ˜¾ç¤ºè¯¦ç»†åŠ è½½ä¿¡æ¯
    )
    
    # åˆ›å»ºadapter
    adapter = MinedojoActionAdapter(policy)
    
    # åŠ è½½adapteræƒé‡ï¼ˆè¿™æ˜¯å¾®è°ƒåçš„æƒé‡ï¼‰
    print(f"\nğŸ“¥ åŠ è½½å¾®è°ƒåçš„æƒé‡...")
    adapter.load_state_dict(checkpoint['model_state_dict'])
    adapter = adapter.to(device)
    adapter.eval()
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in adapter.parameters())
    trainable_params = sum(p.numel() for p in adapter.parameters() if p.requires_grad)
    
    print(f"\nâœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    return adapter


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°VPT BCæ¨¡å‹")
    parser.add_argument('--model', type=str, required=True,
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--task-id', type=str, default='harvest_1_log',
                        help='MineDojoä»»åŠ¡ID')
    parser.add_argument('--episodes', type=int, default=20,
                        help='è¯„ä¼°episodesæ•°')
    parser.add_argument('--max-steps', type=int, default=500,
                        help='æ¯ä¸ªepisodeæœ€å¤§æ­¥æ•°')
    parser.add_argument('--image-size', type=int, nargs=2, default=[160, 256],
                        help='å›¾åƒå°ºå¯¸ (H W) - ä¸å½•åˆ¶æ—¶ä¸€è‡´')
    parser.add_argument('--device', type=str, default='cpu',
                        choices=['cpu', 'cuda', 'mps'],
                        help='è®¾å¤‡')
    parser.add_argument('--render', action='store_true',
                        help='æ¸²æŸ“ç¯å¢ƒï¼ˆæ…ç”¨ï¼‰')
    
    args = parser.parse_args()
    
    print('=' * 70)
    print('VPT BCæ¨¡å‹è¯„ä¼°')
    print('=' * 70)
    print(f'æ¨¡å‹: {args.model}')
    print(f'ä»»åŠ¡: {args.task_id}')
    print(f'Episodes: {args.episodes}')
    print(f'æœ€å¤§æ­¥æ•°: {args.max_steps}')
    print(f'å›¾åƒå°ºå¯¸: {tuple(args.image_size)}')
    print(f'è®¾å¤‡: {args.device}')
    print('=' * 70)
    
    # åŠ è½½æ¨¡å‹
    adapter = load_trained_model(args.model, device=args.device)
    
    # è¯„ä¼°
    results = evaluate_policy(
        adapter=adapter,
        task_id=args.task_id,
        num_episodes=args.episodes,
        max_steps=args.max_steps,
        image_size=tuple(args.image_size),
        device=args.device,
        render=args.render
    )
    
    # æ‰“å°ç»“æœ
    print('\n' + '=' * 70)
    print('è¯„ä¼°ç»“æœ')
    print('=' * 70)
    print(f'æ€»Episodes: {results["total_episodes"]}')
    print(f'æˆåŠŸ: {results["success_count"]}/{results["total_episodes"]} '
          f'({results["success_rate"]:.2%})')
    print(f'å¹³å‡å¥–åŠ±: {results["avg_reward"]:.2f}')
    print(f'å¹³å‡æ­¥æ•°: {results["avg_steps"]:.1f}')
    print('=' * 70)
    
    # æ˜¾ç¤ºæˆåŠŸçš„episodes
    successful_episodes = [ep for ep in results['episodes'] if ep['success']]
    if successful_episodes:
        print('\næˆåŠŸçš„Episodes:')
        for ep in successful_episodes[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
            print(f"  Episode {ep['episode']}: "
                  f"Reward={ep['reward']:.1f}, Steps={ep['steps']}")
    
    return results


if __name__ == '__main__':
    main()
