#!/usr/bin/env python3
"""
è¯„ä¼°VPT BCå¾®è°ƒåçš„æ¨¡å‹

åœ¨MineDojoç¯å¢ƒä¸­è¿è¡Œå¤šä¸ªepisodesï¼Œç»Ÿè®¡æˆåŠŸç‡
"""

import os
import sys
import argparse
import numpy as np
import torch as th
import torch.nn as nn
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..'))
sys.path.insert(0, PROJECT_ROOT)

# æ·»åŠ externalè·¯å¾„
EXTERNAL_PATH = os.path.join(PROJECT_ROOT, 'external')
sys.path.insert(0, EXTERNAL_PATH)

from src.models.vpt import load_vpt_policy
from src.models.vpt.lib.policy import MinecraftPolicy
from src.envs import make_minedojo_env
import minedojo


class MinedojoActionAdapter(nn.Module):
    """
    é€‚é…VPTçš„è¾“å‡ºåˆ°MineDojo action space
    """
    
    def __init__(self, vpt_policy: MinecraftPolicy):
        super().__init__()
        self.vpt_policy = vpt_policy
        
        # MineDojo action dimensions
        self.minedojo_action_dim = [3, 3, 4, 25, 25, 8, 244, 36]
        
        # åˆ›å»ºaction heads
        hidden_dim = 2048
        self.action_heads = nn.ModuleList([
            nn.Linear(hidden_dim, dim) for dim in self.minedojo_action_dim
        ])
    
    def forward(self, obs):
        """
        Args:
            obs: (B, H, W, C) numpy array or tensor
        Returns:
            action_logits: list of tensors, one per action dimension
        """
        batch_size = obs.shape[0]
        
        # è½¬æ¢ä¸ºtensor (å¦‚æœè¿˜ä¸æ˜¯)
        if isinstance(obs, np.ndarray):
            obs = th.from_numpy(obs).float()
        
        # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        obs = obs.to(next(self.vpt_policy.parameters()).device)
        
        # æ·»åŠ æ—¶é—´ç»´åº¦ï¼š(B, H, W, C) -> (B, T=1, H, W, C)
        obs_vpt = obs.unsqueeze(1)  # (B, 1, H, W, C)
        
        # VPTå‰å‘ä¼ æ’­
        x = self.vpt_policy.img_preprocess(obs_vpt)
        x = self.vpt_policy.img_process(x)
        x = x.squeeze(1)  # ç§»é™¤æ—¶é—´ç»´åº¦
        latent = self.vpt_policy.lastlayer(x)
        
        # é€šè¿‡action heads
        action_logits = [head(latent) for head in self.action_heads]
        
        return action_logits
    
    def predict(self, obs, deterministic=True):
        """
        é¢„æµ‹åŠ¨ä½œ
        
        Args:
            obs: (H, W, C) æˆ– (C, H, W) å•ä¸ªè§‚å¯Ÿ
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§åŠ¨ä½œ
        
        Returns:
            action: MineDojo action (list of ints)
        """
        # æ£€æŸ¥å¹¶è½¬æ¢ä¸ºHWCæ ¼å¼
        if len(obs.shape) == 3:
            # æ£€æµ‹CHWæ ¼å¼: (C, H, W)
            if obs.shape[0] == 3 or obs.shape[0] == 1:
                if obs.shape[0] < obs.shape[1] and obs.shape[0] < obs.shape[2]:
                    # (C, H, W) -> (H, W, C)
                    obs = np.transpose(obs, (1, 2, 0))
        
        # å¤„ç†æ•°æ®ç±»å‹å’ŒèŒƒå›´
        is_normalized = obs.dtype in [np.float32, np.float64] and obs.max() <= 1.0
        
        if not is_normalized:
            # uint8 [0,255] -> float32 [0,1]
            obs = obs.astype(np.float32) / 255.0
        else:
            # ç¡®ä¿æ˜¯float32
            obs = obs.astype(np.float32)
        
        # Resizeåˆ°128x128ï¼ˆVPTè®­ç»ƒå°ºå¯¸ï¼‰
        import cv2
        if obs.shape[:2] != (128, 128):
            obs = cv2.resize(obs, (128, 128), interpolation=cv2.INTER_LINEAR)
        
        # æ·»åŠ batchç»´åº¦
        obs = obs[np.newaxis, ...]  # (1, H, W, C)
        
        with th.no_grad():
            action_logits = self.forward(obs)
            
            # é‡‡æ ·æˆ–å–argmax
            actions = []
            for logits in action_logits:
                if deterministic:
                    action = th.argmax(logits, dim=-1)
                else:
                    probs = th.softmax(logits, dim=-1)
                    action = th.multinomial(probs, 1).squeeze(-1)
                actions.append(action.item())
        
        return actions


def evaluate_policy(
    adapter: MinedojoActionAdapter,
    task_id: str = "harvest_1_log",
    num_episodes: int = 20,
    max_steps: int = 1000,
    image_size: tuple = (160, 256),  # ä½¿ç”¨ä¸å½•åˆ¶ä¸€è‡´çš„å°ºå¯¸(160, 256)ï¼Œåœ¨predictä¸­resizeåˆ°VPTçš„128x128
    device: str = "auto",
    render: bool = False
):
    """
    è¯„ä¼°policyæ€§èƒ½
    
    Args:
        adapter: MinedojoActionAdapter
        task_id: ä»»åŠ¡ID
        num_episodes: è¯„ä¼°episodesæ•°
        max_steps: æ¯ä¸ªepisodeæœ€å¤§æ­¥æ•°
        image_size: å›¾åƒå°ºå¯¸ï¼ˆä¸å½•åˆ¶æ—¶ä¸€è‡´ï¼‰
        device: è®¾å¤‡
        render: æ˜¯å¦æ¸²æŸ“
    
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    # è¯„ä¼°é…ç½®
    print(f"è¯„ä¼°é…ç½®:")
    print(f"  ä»»åŠ¡: {task_id}")
    print(f"  å›¾åƒå°ºå¯¸: {image_size}")
    print(f"  æœ€å¤§æ­¥æ•°: {max_steps}")
    print(f"  Episodes: {num_episodes}")
    
    # è¯„ä¼°ç»“æœ
    results = {
        'episodes': [],
        'success_count': 0,
        'total_episodes': num_episodes,
        'action_stats': {
            'dim0_forward_back': [0, 0, 0],  # 3ä¸ªå€¼: [forward, back, noop]
            'dim1_left_right': [0, 0, 0],    # 3ä¸ªå€¼: [left, right, noop]
            'dim2_jump_sneak': [0, 0, 0, 0], # 4ä¸ªå€¼ï¼ˆä¿®æ­£ï¼‰
            'dim3_pitch': [],                 # 25ä¸ªå€¼ï¼ˆè¿ç»­ç»Ÿè®¡ï¼‰
            'dim4_yaw': [],                   # 25ä¸ªå€¼ï¼ˆè¿ç»­ç»Ÿè®¡ï¼‰
            'dim5_functional': [0] * 8,       # 8ä¸ªå€¼ï¼ˆä¿®æ­£ï¼‰
            'dim6': [0] * 244,                # 244ä¸ªå€¼
            'dim7': [0] * 36,                 # 36ä¸ªå€¼
            'total_actions': 0
        }
    }
    
    print(f"\nå¼€å§‹è¯„ä¼° ({num_episodes} episodes)...")
    print("  æ¯ä¸ªepisodeç‹¬ç«‹åˆ›å»ºç¯å¢ƒï¼Œç»“æŸåå…³é—­")
    print("  ğŸ“Š å°†ç»Ÿè®¡åŠ¨ä½œåˆ†å¸ƒä»¥åˆ†æagentè¡Œä¸º")
    
    adapter.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    
    for episode in tqdm(range(num_episodes), desc="Evaluating"):
        # ä¸ºæ¯ä¸ªepisodeåˆ›å»ºæ–°ç¯å¢ƒ
        env = make_minedojo_env(
            task_id=task_id,
            image_size=image_size,
            max_episode_steps=max_steps
        )
        
        try:
            # é‡è¯•resetï¼ˆMineDojoç¯å¢ƒå¯åŠ¨å¯èƒ½å¤±è´¥ï¼‰
            max_reset_retries = 3
            obs = None
            for retry in range(max_reset_retries):
                try:
                    obs = env.reset()
                    break
                except (EOFError, RuntimeError, Exception) as e:
                    if retry < max_reset_retries - 1:
                        print(f"\n  âš ï¸  Episode {episode+1} resetå¤±è´¥ï¼ˆé‡è¯• {retry+1}/{max_reset_retries}ï¼‰: {e}")
                        import time
                        time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
                        # å…³é—­å¹¶é‡æ–°åˆ›å»ºç¯å¢ƒ
                        try:
                            env.close()
                        except:
                            pass
                        env = make_minedojo_env(
                            task_id=task_id,
                            image_size=image_size,
                            max_episode_steps=max_steps
                        )
                    else:
                        print(f"\n  âœ— Episode {episode+1} resetå¤±è´¥ï¼ˆå·²é‡è¯•{max_reset_retries}æ¬¡ï¼‰ï¼Œè·³è¿‡")
                        raise
            
            if obs is None:
                continue
            
            episode_reward = 0
            episode_steps = 0
            done = False
            episode_actions = []  # è®°å½•episodeçš„æ‰€æœ‰åŠ¨ä½œ
            action_unchanged_count = 0  # è¿ç»­ç›¸åŒåŠ¨ä½œè®¡æ•°
            prev_action = None
            
            for step in range(max_steps):
                # é¢„æµ‹åŠ¨ä½œ
                rgb_obs = obs['rgb'] if isinstance(obs, dict) else obs
                action = adapter.predict(rgb_obs, deterministic=True)
                
                # è®°å½•åŠ¨ä½œ
                episode_actions.append(action.copy())
                
                # ç»Ÿè®¡åŠ¨ä½œåˆ†å¸ƒ
                results['action_stats']['total_actions'] += 1
                results['action_stats']['dim0_forward_back'][action[0]] += 1
                results['action_stats']['dim1_left_right'][action[1]] += 1
                results['action_stats']['dim2_jump_sneak'][action[2]] += 1
                results['action_stats']['dim3_pitch'].append(action[3])
                results['action_stats']['dim4_yaw'].append(action[4])
                results['action_stats']['dim5_functional'][action[5]] += 1
                results['action_stats']['dim6'][action[6]] += 1
                results['action_stats']['dim7'][action[7]] += 1
                
                # æ£€æµ‹åŠ¨ä½œæ˜¯å¦æ”¹å˜ï¼ˆæ£€æµ‹"åŸåœ°ä¸åŠ¨"ï¼‰
                if prev_action is not None and np.array_equal(action, prev_action):
                    action_unchanged_count += 1
                else:
                    action_unchanged_count = 0
                prev_action = action.copy()
                
                # æ‰§è¡Œ
                obs, reward, done, info = env.step(action)
                episode_reward += reward
                episode_steps += 1
                
                if done:
                    break
            
            # åˆ¤æ–­æˆåŠŸ
            success = done and episode_reward > 0
            
            if success:
                results['success_count'] += 1
            
            # è®¡ç®—episodeçš„åŠ¨ä½œç»Ÿè®¡
            episode_action_array = np.array(episode_actions)
            unique_actions = len(np.unique(episode_action_array, axis=0))
            action_diversity = unique_actions / max(episode_steps, 1)
            
            # æ£€æµ‹æ˜¯å¦å¤§éƒ¨åˆ†æ—¶é—´é‡å¤ç›¸åŒåŠ¨ä½œï¼ˆ"å¡ä½"ï¼‰
            max_repeated = 1
            if episode_steps > 0:
                for i in range(episode_steps):
                    repeat_count = 1
                    for j in range(i + 1, episode_steps):
                        if np.array_equal(episode_actions[i], episode_actions[j-1]):
                            repeat_count += 1
                        else:
                            break
                    max_repeated = max(max_repeated, repeat_count)
            
            stuck_ratio = max_repeated / max(episode_steps, 1)
            is_stuck = stuck_ratio > 0.5  # è¶…è¿‡50%æ—¶é—´é‡å¤åŒä¸€åŠ¨ä½œ
            
            results['episodes'].append({
                'episode': episode,
                'reward': episode_reward,
                'steps': episode_steps,
                'success': success,
                'done': done,
                'unique_actions': unique_actions,
                'action_diversity': action_diversity,
                'max_repeated': max_repeated,
                'stuck_ratio': stuck_ratio,
                'is_stuck': is_stuck
            })
            
            # å®æ—¶æ˜¾ç¤º
            if (episode + 1) % 5 == 0 or episode == 0:
                current_success_rate = results['success_count'] / (episode + 1)
                print(f"  Episode {episode+1}/{num_episodes}: "
                      f"Success Rate = {current_success_rate:.2%}")
        
        finally:
            # ç¡®ä¿ç¯å¢ƒè¢«å…³é—­
            env.close()
    
    # è®¡ç®—ç»Ÿè®¡
    results['success_rate'] = results['success_count'] / num_episodes
    results['avg_reward'] = np.mean([ep['reward'] for ep in results['episodes']])
    results['avg_steps'] = np.mean([ep['steps'] for ep in results['episodes']])
    
    return results


def load_trained_model(checkpoint_path: str, device: str = 'cpu'):
    """
    åŠ è½½è®­ç»ƒå¥½çš„VPTæ¨¡å‹
    
    Args:
        checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡
    
    Returns:
        adapter: MinedojoActionAdapter
    """
    print(f"åŠ è½½æ¨¡å‹: {checkpoint_path}")
    
    # åŠ è½½checkpoint
    checkpoint = th.load(checkpoint_path, map_location=device)
    
    # æ˜¾ç¤ºcheckpointä¿¡æ¯
    print(f"\nğŸ“¦ Checkpointä¿¡æ¯:")
    print(f"  è®­ç»ƒEpoch: {checkpoint.get('epoch', 'unknown')}")
    print(f"  è®­ç»ƒLoss: {checkpoint.get('loss', 'unknown'):.4f}" if isinstance(checkpoint.get('loss'), (int, float)) else f"  è®­ç»ƒLoss: {checkpoint.get('loss', 'unknown')}")
    print(f"  VPTæƒé‡è·¯å¾„: {checkpoint.get('vpt_weights_path', 'unknown')}")
    
    # åˆ›å»ºVPT policyï¼ˆæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼‰
    print(f"\nğŸ¤– åŠ è½½VPTé¢„è®­ç»ƒæƒé‡...")
    vpt_weights_path = checkpoint.get('vpt_weights_path', 'data/pretrained/vpt/rl-from-early-game-2x.weights')
    policy, load_result = load_vpt_policy(
        vpt_weights_path,
        device=device,
        verbose=True  # æ˜¾ç¤ºè¯¦ç»†åŠ è½½ä¿¡æ¯
    )
    
    # åˆ›å»ºadapter
    adapter = MinedojoActionAdapter(policy)
    
    # åŠ è½½adapteræƒé‡ï¼ˆè¿™æ˜¯å¾®è°ƒåçš„æƒé‡ï¼‰
    print(f"\nğŸ“¥ åŠ è½½å¾®è°ƒåçš„æƒé‡...")
    adapter.load_state_dict(checkpoint['model_state_dict'])
    adapter = adapter.to(device)
    adapter.eval()
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in adapter.parameters())
    trainable_params = sum(p.numel() for p in adapter.parameters() if p.requires_grad)
    
    print(f"\nâœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    print(f"  æ€»å‚æ•°: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    
    return adapter


def main():
    parser = argparse.ArgumentParser(description="è¯„ä¼°VPT BCæ¨¡å‹")
    parser.add_argument('--model', type=str, required=True,
                        help='è®­ç»ƒå¥½çš„æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--task-id', type=str, default='harvest_1_log',
                        help='MineDojoä»»åŠ¡ID')
    parser.add_argument('--episodes', type=int, default=20,
                        help='è¯„ä¼°episodesæ•°')
    parser.add_argument('--max-steps', type=int, default=500,
                        help='æ¯ä¸ªepisodeæœ€å¤§æ­¥æ•°')
    parser.add_argument('--image-size', type=int, nargs=2, default=[160, 256],
                        help='å›¾åƒå°ºå¯¸ (H W) - ä¸å½•åˆ¶æ—¶ä¸€è‡´')
    parser.add_argument('--device', type=str, default='cpu',
                        choices=['cpu', 'cuda', 'mps'],
                        help='è®¾å¤‡')
    parser.add_argument('--render', action='store_true',
                        help='æ¸²æŸ“ç¯å¢ƒï¼ˆæ…ç”¨ï¼‰')
    
    args = parser.parse_args()
    
    print('=' * 70)
    print('VPT BCæ¨¡å‹è¯„ä¼°')
    print('=' * 70)
    print(f'æ¨¡å‹: {args.model}')
    print(f'ä»»åŠ¡: {args.task_id}')
    print(f'Episodes: {args.episodes}')
    print(f'æœ€å¤§æ­¥æ•°: {args.max_steps}')
    print(f'å›¾åƒå°ºå¯¸: {tuple(args.image_size)}')
    print(f'è®¾å¤‡: {args.device}')
    print('=' * 70)
    
    # åŠ è½½æ¨¡å‹
    adapter = load_trained_model(args.model, device=args.device)
    
    # è¯„ä¼°
    results = evaluate_policy(
        adapter=adapter,
        task_id=args.task_id,
        num_episodes=args.episodes,
        max_steps=args.max_steps,
        image_size=tuple(args.image_size),
        device=args.device,
        render=args.render
    )
    
    # æ‰“å°è¯¦ç»†åˆ†ææŠ¥å‘Š
    print('\n' + '=' * 70)
    print('ğŸ“Š è¯„ä¼°ç»“æœä¸åŠ¨ä½œåˆ†æ')
    print('=' * 70)
    
    # åŸºæœ¬ç»“æœ
    print(f'\nã€åŸºæœ¬ç»Ÿè®¡ã€‘')
    print(f'æ€»Episodes: {results["total_episodes"]}')
    print(f'æˆåŠŸ: {results["success_count"]}/{results["total_episodes"]} '
          f'({results["success_rate"]:.2%})')
    print(f'å¹³å‡å¥–åŠ±: {results["avg_reward"]:.2f}')
    print(f'å¹³å‡æ­¥æ•°: {results["avg_steps"]:.1f}')
    
    # åŠ¨ä½œåˆ†å¸ƒåˆ†æ
    stats = results['action_stats']
    total = stats['total_actions']
    
    print(f'\nã€åŠ¨ä½œåˆ†å¸ƒç»Ÿè®¡ã€‘(æ€»åŠ¨ä½œæ•°: {total})')
    print('-' * 70)
    
    # ç»´åº¦0: å‰è¿›/åé€€
    dim0 = stats['dim0_forward_back']
    print(f'ç§»åŠ¨ï¼ˆå‰/åï¼‰:')
    print(f'  å‰è¿›: {dim0[0]:6d} ({dim0[0]/total*100:5.1f}%)')
    print(f'  åé€€: {dim0[1]:6d} ({dim0[1]/total*100:5.1f}%)')
    print(f'  ä¸åŠ¨: {dim0[2]:6d} ({dim0[2]/total*100:5.1f}%)  {"âš ï¸ è¿‡é«˜ï¼" if dim0[2]/total > 0.5 else ""}')
    
    # ç»´åº¦1: å·¦å³ç§»åŠ¨
    dim1 = stats['dim1_left_right']
    print(f'\nå¹³ç§»ï¼ˆå·¦/å³ï¼‰:')
    print(f'  å·¦ç§»: {dim1[0]:6d} ({dim1[0]/total*100:5.1f}%)')
    print(f'  å³ç§»: {dim1[1]:6d} ({dim1[1]/total*100:5.1f}%)')
    print(f'  ä¸åŠ¨: {dim1[2]:6d} ({dim1[2]/total*100:5.1f}%)  {"âš ï¸ è¿‡é«˜ï¼" if dim1[2]/total > 0.5 else ""}')
    
    # ç»´åº¦2: è·³è·ƒ/æ½œè¡Œç­‰ï¼ˆ4ä¸ªå€¼ï¼‰
    dim2 = stats['dim2_jump_sneak']
    print(f'\nè·³è·ƒ/æ½œè¡Œ/å…¶ä»–ï¼ˆç»´åº¦2ï¼Œ4ä¸ªå€¼ï¼‰:')
    for i, count in enumerate(dim2):
        percentage = count/total*100 if total > 0 else 0
        warning = "  âš ï¸ è¿‡ä½ï¼" if i == 0 and percentage < 10 else ""  # å‡è®¾åŠ¨ä½œ0æ˜¯è·³è·ƒ
        print(f'  åŠ¨ä½œ{i}: {count:6d} ({percentage:5.1f}%){warning}')
    
    # ç»´åº¦3-4: ç›¸æœº
    dim3_mean = np.mean(stats['dim3_pitch']) if stats['dim3_pitch'] else 0
    dim3_std = np.std(stats['dim3_pitch']) if stats['dim3_pitch'] else 0
    dim4_mean = np.mean(stats['dim4_yaw']) if stats['dim4_yaw'] else 0
    dim4_std = np.std(stats['dim4_yaw']) if stats['dim4_yaw'] else 0
    print(f'\nç›¸æœºç§»åŠ¨:')
    print(f'  Pitch (ä¸Šä¸‹): mean={dim3_mean:6.2f}, std={dim3_std:6.2f}  {"âš ï¸ è¿‡ä½ï¼" if dim3_std < 0.01 else ""}')
    print(f'  Yaw (å·¦å³):   mean={dim4_mean:6.2f}, std={dim4_std:6.2f}  {"âš ï¸ è¿‡ä½ï¼" if dim4_std < 0.01 else ""}')
    
    # ç»´åº¦5: åŠŸèƒ½é”®ï¼ˆ8ä¸ªå€¼ï¼‰
    dim5 = stats['dim5_functional']
    print(f'\nåŠŸèƒ½é”®ï¼ˆç»´åº¦5ï¼Œ8ä¸ªå€¼ï¼‰:')
    for i, count in enumerate(dim5):
        if count > 0:  # åªæ‰“å°æœ‰ä½¿ç”¨çš„
            percentage = count/total*100 if total > 0 else 0
            print(f'  åŠŸèƒ½{i}: {count:6d} ({percentage:5.1f}%)')
    
    # Episodeè¡Œä¸ºåˆ†æ
    print(f'\nã€Episodeè¡Œä¸ºåˆ†æã€‘')
    print('-' * 70)
    
    stuck_episodes = [ep for ep in results['episodes'] if ep['is_stuck']]
    low_diversity = [ep for ep in results['episodes'] if ep['action_diversity'] < 0.1]
    
    print(f'å¡ä½çš„Episodes: {len(stuck_episodes)}/{results["total_episodes"]} '
          f'({len(stuck_episodes)/results["total_episodes"]*100:.1f}%)')
    print(f'  å®šä¹‰: è¶…è¿‡50%æ—¶é—´é‡å¤ç›¸åŒåŠ¨ä½œ')
    
    print(f'ä½åŠ¨ä½œå¤šæ ·æ€§: {len(low_diversity)}/{results["total_episodes"]} '
          f'({len(low_diversity)/results["total_episodes"]*100:.1f}%)')
    print(f'  å®šä¹‰: åŠ¨ä½œå¤šæ ·æ€§<0.1 (10%ç‹¬ç‰¹åŠ¨ä½œ)')
    
    avg_diversity = np.mean([ep['action_diversity'] for ep in results['episodes']])
    avg_stuck_ratio = np.mean([ep['stuck_ratio'] for ep in results['episodes']])
    print(f'\nå¹³å‡åŠ¨ä½œå¤šæ ·æ€§: {avg_diversity:.3f}  {"âš ï¸ è¿‡ä½ï¼" if avg_diversity < 0.2 else ""}')
    print(f'å¹³å‡å¡ä½æ¯”ä¾‹: {avg_stuck_ratio:.3f}  {"âš ï¸ è¿‡é«˜ï¼" if avg_stuck_ratio > 0.3 else ""}')
    
    # è¯Šæ–­å’Œå»ºè®®
    print(f'\nã€ğŸ” é—®é¢˜è¯Šæ–­ã€‘')
    print('-' * 70)
    
    issues = []
    
    # æ£€æŸ¥å„ç§é—®é¢˜
    if dim0[2]/total > 0.5:  # ä¸å‰è¿›/åé€€
        issues.append("âŒ ç§»åŠ¨ä¸è¶³ï¼šè¶…è¿‡50%æ—¶é—´ä¸å‰è¿›/åé€€")
    
    if dim1[2]/total > 0.5:  # ä¸å·¦å³ç§»åŠ¨
        issues.append("âŒ å¹³ç§»ä¸è¶³ï¼šè¶…è¿‡50%æ—¶é—´ä¸å·¦å³ç§»åŠ¨")
    
    if dim2[0]/total < 0.1:  # è·³è·ƒå¤ªå°‘
        issues.append("âŒ è·³è·ƒç¼ºå¤±ï¼šè·³è·ƒåŠ¨ä½œ<10%ï¼ˆä¸“å®¶æ•°æ®æ˜¯85%ï¼ï¼‰")
    
    if dim3_std < 0.01 or dim4_std < 0.01:  # ç›¸æœºä¸åŠ¨
        issues.append("âŒ ç›¸æœºé™æ­¢ï¼šç›¸æœºå‡ ä¹ä¸ç§»åŠ¨")
    
    if avg_diversity < 0.2:
        issues.append("âŒ åŠ¨ä½œå•ä¸€ï¼šåŠ¨ä½œå¤šæ ·æ€§è¿‡ä½")
    
    if len(stuck_episodes) > results["total_episodes"] * 0.3:
        issues.append(f"âŒ é¢‘ç¹å¡ä½ï¼š{len(stuck_episodes)}ä¸ªepisodeså¡ä½")
    
    if results["success_rate"] < 0.2:
        issues.append(f"âŒ æˆåŠŸç‡æä½ï¼šåªæœ‰{results['success_rate']:.1%}")
    
    if issues:
        for issue in issues:
            print(f'  {issue}')
    else:
        print('  âœ“ æœªæ£€æµ‹åˆ°æ˜æ˜¾é—®é¢˜')
    
    # æ˜¾ç¤ºæˆåŠŸå’Œå¤±è´¥çš„episodes
    successful_episodes = [ep for ep in results['episodes'] if ep['success']]
    failed_episodes = [ep for ep in results['episodes'] if not ep['success']]
    
    if successful_episodes:
        print(f'\nã€âœ“ æˆåŠŸçš„Episodesã€‘')
        for ep in successful_episodes[:5]:
            print(f"  Episode {ep['episode']+1}: "
                  f"Reward={ep['reward']:.1f}, Steps={ep['steps']}, "
                  f"Diversity={ep['action_diversity']:.3f}")
    
    if failed_episodes:
        print(f'\nã€âœ— å¤±è´¥çš„Episodesï¼ˆå‰5ä¸ªï¼‰ã€‘')
        for ep in failed_episodes[:5]:
            status = "ğŸ”’ å¡ä½" if ep['is_stuck'] else "â“ å…¶ä»–"
            print(f"  Episode {ep['episode']+1}: "
                  f"{status}, Steps={ep['steps']}, "
                  f"Diversity={ep['action_diversity']:.3f}, "
                  f"Stuck={ep['stuck_ratio']:.1%}")
    
    print('\n' + '=' * 70)
    
    return results


if __name__ == '__main__':
    main()
