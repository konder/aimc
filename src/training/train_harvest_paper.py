#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
harvest_1_paper ä»»åŠ¡è®­ç»ƒè„šæœ¬ (MVPç‰ˆæœ¬)

ä½¿ç”¨PPOç®—æ³•ä»å¤´å¼€å§‹è®­ç»ƒMineDojoæ™ºèƒ½ä½“å®Œæˆæ”¶é›†çº¸çš„ä»»åŠ¡

ç”¨æ³•:
    python src/training/train_harvest_paper.py
    python src/training/train_harvest_paper.py --total-timesteps 1000000
"""

import os
import sys
import argparse
from datetime import datetime
import yaml

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import (
    CheckpointCallback, 
    EvalCallback,
    CallbackList
)

from src.utils.env_wrappers import make_minedojo_env


def _detect_device(device_arg):
    """
    æ£€æµ‹å¹¶è¿”å›å¯ç”¨çš„è®¾å¤‡
    
    Args:
        device_arg: ç”¨æˆ·æŒ‡å®šçš„è®¾å¤‡å‚æ•°
        
    Returns:
        str: å®é™…ä½¿ç”¨çš„è®¾å¤‡
    """
    if device_arg != 'auto':
        return device_arg
    
    # è‡ªåŠ¨æ£€æµ‹
    if torch.cuda.is_available():
        print("ğŸš€ æ£€æµ‹åˆ° CUDA GPUï¼Œä½¿ç”¨ cuda")
        return 'cuda'
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("ğŸ æ£€æµ‹åˆ° Apple Siliconï¼Œä½¿ç”¨ MPS åŠ é€Ÿ")
        return 'mps'
    else:
        print("ğŸ’» ä½¿ç”¨ CPU")
        return 'cpu'


class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, log_dir):
        """
        åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        
        Args:
            log_dir: æ—¥å¿—ç›®å½•
        """
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_file = os.path.join(
            log_dir, 
            f"training_{timestamp}.log"
        )
        
        self.log("=" * 70)
        self.log("MineDojo harvest_1_paper è®­ç»ƒå¼€å§‹")
        self.log("=" * 70)
    
    def log(self, message):
        """
        è®°å½•æ—¥å¿—
        
        Args:
            message: æ—¥å¿—æ¶ˆæ¯
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_line = f"[{timestamp}] {message}"
        print(log_line)
        
        with open(self.log_file, 'a') as f:
            f.write(log_line + '\n')


def create_env(task_id, image_size, rank=0):
    """
    åˆ›å»ºç¯å¢ƒçš„å·¥å‚å‡½æ•°
    
    Args:
        task_id: ä»»åŠ¡ID
        image_size: å›¾åƒå°ºå¯¸
        rank: è¿›ç¨‹ç¼–å·ï¼ˆç”¨äºå¹¶è¡Œç¯å¢ƒï¼‰
        
    Returns:
        callable: è¿”å›ç¯å¢ƒçš„å‡½æ•°
    """
    def _init():
        env = make_minedojo_env(
            task_id=task_id,
            image_size=image_size,
            use_frame_stack=False,  # MVPç‰ˆæœ¬ä¸ä½¿ç”¨å¸§å †å 
            use_discrete_actions=False  # ä½¿ç”¨åŸå§‹MultiDiscreteç©ºé—´
        )
        # æ³¨æ„: ä¸ä½¿ç”¨Monitorä»¥é¿å…gym/gymnasiumå…¼å®¹æ€§é—®é¢˜
        # PPOå·²ç»æœ‰å†…ç½®çš„æ—¥å¿—è®°å½•åŠŸèƒ½
        return env
    return _init


def train(args):
    """
    ä¸»è®­ç»ƒå‡½æ•°
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    # æ£€æµ‹å¹¶è®¾ç½®è®¾å¤‡
    device = _detect_device(args.device)
    args.device = device
    
    # åˆå§‹åŒ–æ—¥å¿—
    logger = TrainingLogger(args.log_dir)
    logger.log(f"è®­ç»ƒé…ç½®:")
    logger.log(f"  ä»»åŠ¡: {args.task_id}")
    logger.log(f"  æ€»æ­¥æ•°: {args.total_timesteps:,}")
    logger.log(f"  å¹¶è¡Œç¯å¢ƒæ•°: {args.n_envs}")
    logger.log(f"  å­¦ä¹ ç‡: {args.learning_rate}")
    logger.log(f"  å›¾åƒå°ºå¯¸: {args.image_size}")
    logger.log(f"  è®¾å¤‡: {args.device}")
    logger.log("")
    
    # åˆ›å»ºç›®å½•
    os.makedirs(args.checkpoint_dir, exist_ok=True)
    os.makedirs(args.tensorboard_dir, exist_ok=True)
    
    # ========================================================================
    # 1. åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    # ========================================================================
    logger.log("[1/5] åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    
    if args.n_envs > 1:
        # å¤šä¸ªå¹¶è¡Œç¯å¢ƒ
        logger.log(f"  ä½¿ç”¨ {args.n_envs} ä¸ªå¹¶è¡Œç¯å¢ƒåŠ é€Ÿè®­ç»ƒ")
        env = SubprocVecEnv([
            create_env(args.task_id, args.image_size, i) 
            for i in range(args.n_envs)
        ])
    else:
        # å•ä¸ªç¯å¢ƒ
        env = DummyVecEnv([create_env(args.task_id, args.image_size)])
    
    logger.log("  âœ“ è®­ç»ƒç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    
    # ========================================================================
    # 2. åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    # ========================================================================
    # æ³¨æ„: MineDojoåœ¨åˆ›å»ºå¤šä¸ªç›¸åŒä»»åŠ¡çš„ç¯å¢ƒæ—¶æœ‰bugï¼Œæš‚æ—¶ç¦ç”¨ç‹¬ç«‹è¯„ä¼°ç¯å¢ƒ
    # logger.log("[2/5] åˆ›å»ºè¯„ä¼°ç¯å¢ƒ...")
    # eval_env = DummyVecEnv([create_env(args.task_id, args.image_size)])
    # logger.log("  âœ“ è¯„ä¼°ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    eval_env = None  # æš‚æ—¶ç¦ç”¨
    logger.log("[2/5] è·³è¿‡è¯„ä¼°ç¯å¢ƒåˆ›å»ºï¼ˆé¿å…MineDojoå¤šç¯å¢ƒbugï¼‰")
    
    # ========================================================================
    # 3. åˆ›å»ºPPOæ¨¡å‹
    # ========================================================================
    logger.log("[3/5] åˆ›å»ºPPOæ¨¡å‹...")
    logger.log("  æ³¨æ„: æ¨¡å‹ä»éšæœºåˆå§‹åŒ–å¼€å§‹ï¼ˆæ— é¢„è®­ç»ƒæƒé‡ï¼‰")
    
    # PPOè¶…å‚æ•°
    # æ³¨æ„: normalize_images=False å› ä¸ºæˆ‘ä»¬å·²ç»åœ¨wrapperä¸­å½’ä¸€åŒ–äº†
    policy_kwargs = dict(
        normalize_images=False  # é‡è¦ï¼æˆ‘ä»¬å·²ç»å½’ä¸€åŒ–äº†å›¾åƒ
    )
    
    model = PPO(
        policy="CnnPolicy",
        env=env,
        learning_rate=args.learning_rate,
        n_steps=args.n_steps,
        batch_size=args.batch_size,
        n_epochs=args.n_epochs,
        gamma=args.gamma,
        gae_lambda=args.gae_lambda,
        clip_range=args.clip_range,
        ent_coef=args.ent_coef,
        vf_coef=args.vf_coef,
        max_grad_norm=args.max_grad_norm,
        verbose=1,
        tensorboard_log=args.tensorboard_dir,
        device=args.device,
        policy_kwargs=policy_kwargs,
    )
    
    # è®¡ç®—å‚æ•°é‡
    n_params = sum(p.numel() for p in model.policy.parameters())
    logger.log(f"  âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ (å‚æ•°é‡: {n_params:,})")
    
    # ========================================================================
    # 4. è®¾ç½®å›è°ƒå‡½æ•°
    # ========================================================================
    logger.log("[4/5] è®¾ç½®è®­ç»ƒå›è°ƒ...")
    
    # æ£€æŸ¥ç‚¹å›è°ƒï¼šå®šæœŸä¿å­˜æ¨¡å‹
    checkpoint_callback = CheckpointCallback(
        save_freq=args.save_freq,
        save_path=args.checkpoint_dir,
        name_prefix="harvest_paper",
        save_replay_buffer=False,
        save_vecnormalize=True,
    )
    
    # è¯„ä¼°å›è°ƒï¼šå®šæœŸè¯„ä¼°æ¨¡å‹æ€§èƒ½
    # æ³¨æ„: ç”±äºç¦ç”¨äº†ç‹¬ç«‹è¯„ä¼°ç¯å¢ƒï¼Œæš‚æ—¶ä¸ä½¿ç”¨ EvalCallback
    # eval_callback = EvalCallback(
    #     eval_env,
    #     best_model_save_path=args.checkpoint_dir,
    #     log_path=args.log_dir,
    #     eval_freq=args.eval_freq,
    #     n_eval_episodes=args.n_eval_episodes,
    #     deterministic=True,
    #     render=False,
    # )
    
    # ç»„åˆå›è°ƒï¼ˆæš‚æ—¶åªä½¿ç”¨æ£€æŸ¥ç‚¹å›è°ƒï¼‰
    callback = checkpoint_callback
    
    logger.log("  âœ“ å›è°ƒè®¾ç½®å®Œæˆ")
    logger.log(f"    - æ¯ {args.save_freq} æ­¥ä¿å­˜æ£€æŸ¥ç‚¹")
    # logger.log(f"    - æ¯ {args.eval_freq} æ­¥è¯„ä¼°æ¨¡å‹")
    logger.log("")
    
    # ========================================================================
    # 5. å¼€å§‹è®­ç»ƒ
    # ========================================================================
    logger.log("[5/5] å¼€å§‹è®­ç»ƒ...")
    logger.log("=" * 70)
    logger.log("")
    
    try:
        model.learn(
            total_timesteps=args.total_timesteps,
            callback=callback,
            log_interval=10,  # æ¯10æ¬¡rolloutæ‰“å°ä¸€æ¬¡
            tb_log_name="ppo_harvest_paper",
            reset_num_timesteps=True,
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = os.path.join(
            args.checkpoint_dir, 
            "harvest_paper_final.zip"
        )
        model.save(final_model_path)
        
        logger.log("")
        logger.log("=" * 70)
        logger.log("âœ“ è®­ç»ƒå®Œæˆ!")
        logger.log(f"âœ“ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")
        logger.log("=" * 70)
        
    except KeyboardInterrupt:
        logger.log("")
        logger.log("=" * 70)
        logger.log("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        logger.log("=" * 70)
        
        # ä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹
        interrupted_path = os.path.join(
            args.checkpoint_dir, 
            "harvest_paper_interrupted.zip"
        )
        model.save(interrupted_path)
        logger.log(f"å½“å‰æ¨¡å‹å·²ä¿å­˜åˆ°: {interrupted_path}")
    
    finally:
        # æ¸…ç†èµ„æº
        env.close()
        if eval_env is not None:
            eval_env.close()
        logger.log("ç¯å¢ƒå·²å…³é—­")


def evaluate(args):
    """
    è¯„ä¼°å·²è®­ç»ƒçš„æ¨¡å‹
    
    Args:
        args: å‘½ä»¤è¡Œå‚æ•°
    """
    print("=" * 70)
    print("æ¨¡å‹è¯„ä¼°")
    print("=" * 70)
    
    # åŠ è½½æ¨¡å‹
    print(f"\n[1/3] åŠ è½½æ¨¡å‹: {args.model_path}")
    model = PPO.load(args.model_path)
    print("  âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # åˆ›å»ºç¯å¢ƒ
    print("\n[2/3] åˆ›å»ºç¯å¢ƒ...")
    env = make_minedojo_env(
        task_id=args.task_id,
        image_size=args.image_size,
        use_discrete_actions=False  # ä½¿ç”¨åŸå§‹MultiDiscreteç©ºé—´
    )
    print("  âœ“ ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    
    # è¯„ä¼°
    print(f"\n[3/3] è¿è¡Œ {args.n_eval_episodes} ä¸ªè¯„ä¼°episodes...")
    
    episode_rewards = []
    episode_lengths = []
    success_count = 0
    
    for episode in range(args.n_eval_episodes):
        obs = env.reset()
        done = False
        episode_reward = 0
        episode_length = 0
        
        while not done and episode_length < args.max_episode_steps:
            action, _states = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            episode_reward += reward
            episode_length += 1
            
            if reward > 0:
                success_count += 1
        
        episode_rewards.append(episode_reward)
        episode_lengths.append(episode_length)
        
        print(f"  Episode {episode + 1}/{args.n_eval_episodes}: "
              f"reward={episode_reward:.2f}, steps={episode_length}")
    
    # ç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 70)
    print("è¯„ä¼°ç»“æœ:")
    print("=" * 70)
    print(f"å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f} Â± "
          f"{np.std(episode_rewards):.2f}")
    print(f"å¹³å‡æ­¥æ•°: {np.mean(episode_lengths):.1f} Â± "
          f"{np.std(episode_lengths):.1f}")
    print(f"æˆåŠŸç‡: {success_count}/{args.n_eval_episodes} "
          f"({100*success_count/args.n_eval_episodes:.1f}%)")
    print("=" * 70)
    
    env.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="MineDojo harvest_1_paper è®­ç»ƒè„šæœ¬"
    )
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument(
        '--mode', 
        type=str, 
        default='train',
        choices=['train', 'eval'],
        help='è¿è¡Œæ¨¡å¼: train=è®­ç»ƒ, eval=è¯„ä¼°'
    )
    
    # ä»»åŠ¡é…ç½®
    parser.add_argument(
        '--task-id',
        type=str,
        default='harvest_milk',  # é»˜è®¤ä½¿ç”¨harvest_milkï¼ˆæ›´ç¨³å®šï¼‰
        help='MineDojoä»»åŠ¡ID'
    )
    parser.add_argument(
        '--image-size',
        type=int,
        nargs=2,
        default=[160, 256],
        help='å›¾åƒå°ºå¯¸ (height width)'
    )
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument(
        '--total-timesteps',
        type=int,
        default=500000,
        help='æ€»è®­ç»ƒæ­¥æ•°'
    )
    parser.add_argument(
        '--n-envs',
        type=int,
        default=1,
        help='å¹¶è¡Œç¯å¢ƒæ•°é‡'
    )
    parser.add_argument(
        '--learning-rate',
        type=float,
        default=3e-4,
        help='å­¦ä¹ ç‡'
    )
    parser.add_argument(
        '--n-steps',
        type=int,
        default=2048,
        help='æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•°'
    )
    parser.add_argument(
        '--batch-size',
        type=int,
        default=64,
        help='æ‰¹æ¬¡å¤§å°'
    )
    parser.add_argument(
        '--n-epochs',
        type=int,
        default=10,
        help='æ¯æ¬¡æ›´æ–°çš„epochæ•°'
    )
    parser.add_argument(
        '--gamma',
        type=float,
        default=0.99,
        help='æŠ˜æ‰£å› å­'
    )
    parser.add_argument(
        '--gae-lambda',
        type=float,
        default=0.95,
        help='GAE lambdaå‚æ•°'
    )
    parser.add_argument(
        '--clip-range',
        type=float,
        default=0.2,
        help='PPOè£å‰ªèŒƒå›´'
    )
    parser.add_argument(
        '--ent-coef',
        type=float,
        default=0.01,
        help='ç†µç³»æ•°'
    )
    parser.add_argument(
        '--vf-coef',
        type=float,
        default=0.5,
        help='ä»·å€¼å‡½æ•°ç³»æ•°'
    )
    parser.add_argument(
        '--max-grad-norm',
        type=float,
        default=0.5,
        help='æ¢¯åº¦è£å‰ªé˜ˆå€¼'
    )
    
    # ä¿å­˜å’Œè¯„ä¼°
    parser.add_argument(
        '--save-freq',
        type=int,
        default=10000,
        help='ä¿å­˜æ£€æŸ¥ç‚¹çš„é¢‘ç‡'
    )
    parser.add_argument(
        '--eval-freq',
        type=int,
        default=10000,
        help='è¯„ä¼°æ¨¡å‹çš„é¢‘ç‡'
    )
    parser.add_argument(
        '--n-eval-episodes',
        type=int,
        default=5,
        help='è¯„ä¼°çš„episodeæ•°é‡'
    )
    parser.add_argument(
        '--max-episode-steps',
        type=int,
        default=2000,
        help='æ¯ä¸ªepisodeçš„æœ€å¤§æ­¥æ•°'
    )
    
    # è·¯å¾„é…ç½®
    parser.add_argument(
        '--log-dir',
        type=str,
        default='logs/training',
        help='æ—¥å¿—ç›®å½•'
    )
    parser.add_argument(
        '--checkpoint-dir',
        type=str,
        default='checkpoints/harvest_paper',
        help='æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•'
    )
    parser.add_argument(
        '--tensorboard-dir',
        type=str,
        default='logs/tensorboard',
        help='TensorBoardæ—¥å¿—ç›®å½•'
    )
    parser.add_argument(
        '--model-path',
        type=str,
        default='checkpoints/harvest_paper/best_model.zip',
        help='è¯„ä¼°æ—¶åŠ è½½çš„æ¨¡å‹è·¯å¾„'
    )
    
    # è®¾å¤‡é…ç½®
    parser.add_argument(
        '--device',
        type=str,
        default='auto',
        choices=['auto', 'cpu', 'cuda', 'mps'],
        help='è®­ç»ƒè®¾å¤‡ (auto/cpu/cuda/mps)'
    )
    
    args = parser.parse_args()
    
    # è½¬æ¢image_sizeä¸ºå…ƒç»„
    args.image_size = tuple(args.image_size)
    
    # è¿è¡Œ
    if args.mode == 'train':
        train(args)
    else:
        evaluate(args)


if __name__ == "__main__":
    main()

