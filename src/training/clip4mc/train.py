#!/usr/bin/env python3
"""
CLIP4MC ç®€åŒ–è®­ç»ƒè„šæœ¬ (å• GPU æµ‹è¯•ç‰ˆ)

åŸºäº https://github.com/PKU-RL/CLIP4MC çš„è®­ç»ƒä»£ç ç®€åŒ–ç‰ˆæœ¬ï¼Œ
ç”¨äºéªŒè¯æ•°æ®æ ¼å¼å’Œè®­ç»ƒæµç¨‹ã€‚

ä½¿ç”¨æ–¹æ³•:
    python -m src.training.clip4mc.train \
        --data-log data/training/clip4mc/data_log.json \
        --pretrain-model data/weights/vit-b-16-clip/ViT-B-16.pt \
        --epochs 5 --batch-size 1
"""

import argparse
import json
import os
import pickle
import sys
from pathlib import Path
from typing import Dict, List, Literal
import logging

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class SimpleClip4MCDataset(Dataset):
    """ç®€åŒ–çš„ CLIP4MC æ•°æ®é›†"""
    
    def __init__(self, data_dirs: List[str], dataset_type: str = "train"):
        self.data_dirs = data_dirs
        self.dataset_type = dataset_type
        
        # Minecraft å›¾åƒå½’ä¸€åŒ–å‚æ•°
        self.image_mean = torch.tensor([0.3331, 0.3245, 0.3051]).view(3, 1, 1)
        self.image_std = torch.tensor([0.2439, 0.2493, 0.2873]).view(3, 1, 1)
    
    def __len__(self):
        return len(self.data_dirs)
    
    def __getitem__(self, idx):
        data_dir = Path(self.data_dirs[idx])
        
        # åŠ è½½æ–‡æœ¬
        with open(data_dir / "text_input.pkl", "rb") as f:
            text_data = pickle.load(f)
        
        if isinstance(text_data, dict):
            text_tokens = text_data.get('tokens', text_data)
        else:
            text_tokens = text_data
        
        text_tokens = torch.tensor(text_tokens, dtype=torch.long)
        
        # åŠ è½½è§†é¢‘
        with open(data_dir / "video_input.pkl", "rb") as f:
            video_data = pickle.load(f)
        
        # (N, H, W, C) -> (N, C, H, W) å¹¶å½’ä¸€åŒ–
        video = np.array(video_data)
        if video.ndim == 4 and video.shape[-1] == 3:
            video = video.transpose(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)
        
        # è½¬æ¢ä¸º float å¹¶å½’ä¸€åŒ–åˆ° [0, 1]
        video = torch.tensor(video, dtype=torch.float32)
        if video.max() > 1.0:
            video = video / 255.0
        
        # æ ‡å‡†åŒ–
        video = (video - self.image_mean) / self.image_std
        
        # ç¡®ä¿æ˜¯ 16 å¸§
        if video.shape[0] != 16:
            indices = np.linspace(0, video.shape[0] - 1, 16, dtype=int)
            video = video[indices]
        
        return text_tokens, video


class SimpleCLIP4MC(nn.Module):
    """ç®€åŒ–çš„ CLIP4MC æ¨¡å‹ (ç”¨äºéªŒè¯)"""
    
    def __init__(self, pretrain_path: str = None):
        super().__init__()
        
        self.clip_model = None
        self.tokenizer = None
        
        # å°è¯•ä»æœ¬åœ°æ–‡ä»¶åŠ è½½ CLIP
        if pretrain_path and Path(pretrain_path).exists():
            try:
                import open_clip
                # åˆ›å»ºæ¨¡å‹ç»“æ„
                self.clip_model, _, self.preprocess = open_clip.create_model_and_transforms(
                    'ViT-B-16', pretrained=''  # ä¸ä¸‹è½½æƒé‡
                )
                # åŠ è½½æœ¬åœ°æƒé‡
                state_dict = torch.load(pretrain_path, map_location='cpu')
                if 'state_dict' in state_dict:
                    state_dict = state_dict['state_dict']
                self.clip_model.load_state_dict(state_dict, strict=False)
                self.tokenizer = open_clip.get_tokenizer('ViT-B-16')
                logger.info(f"âœ“ ä»æœ¬åœ°åŠ è½½ CLIP æƒé‡: {pretrain_path}")
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½æœ¬åœ°æƒé‡: {e}")
                self.clip_model = None
        
        # å¦‚æœæœ¬åœ°åŠ è½½å¤±è´¥ï¼Œå°è¯•åœ¨çº¿åŠ è½½
        if self.clip_model is None:
            try:
                import open_clip
                self.clip_model, _, self.preprocess = open_clip.create_model_and_transforms(
                    'ViT-B-16', pretrained='openai'
                )
                self.tokenizer = open_clip.get_tokenizer('ViT-B-16')
                logger.info("âœ“ ä½¿ç”¨ OpenAI CLIP é¢„è®­ç»ƒæƒé‡")
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½ open_clip: {e}")
                logger.info("ä½¿ç”¨ç®€åŒ–çš„éšæœºåˆå§‹åŒ–æ¨¡å‹")
        
        # è§†é¢‘æ—¶åºèšåˆ
        self.video_adapter = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
        )
        
        # æ¸©åº¦å‚æ•°
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
        
    def encode_video(self, video: torch.Tensor) -> torch.Tensor:
        """
        ç¼–ç è§†é¢‘å¸§
        
        Args:
            video: (B, T, C, H, W)
        
        Returns:
            è§†é¢‘åµŒå…¥ (B, 512)
        """
        B, T, C, H, W = video.shape
        
        # å±•å¹³ batch å’Œ time
        video_flat = video.view(B * T, C, H, W)
        
        # ä½¿ç”¨ CLIP è§†è§‰ç¼–ç å™¨
        if self.clip_model is not None:
            with torch.no_grad():
                frame_features = self.clip_model.encode_image(video_flat)
        else:
            # Fallback: ç®€å•çº¿æ€§æŠ•å½±
            frame_features = video_flat.view(B * T, -1)
            if not hasattr(self, '_fallback_proj'):
                in_dim = C * H * W
                self._fallback_proj = nn.Linear(in_dim, 512).to(video_flat.device)
            frame_features = self._fallback_proj(frame_features)
        
        # é‡å¡‘ä¸º (B, T, D)
        frame_features = frame_features.view(B, T, -1)
        
        # æ—¶åºèšåˆ (ç®€å•å¹³å‡)
        video_features = frame_features.mean(dim=1)
        
        # é€‚é…å™¨
        video_features = self.video_adapter(video_features)
        
        # L2 å½’ä¸€åŒ–
        video_features = F.normalize(video_features, dim=-1)
        
        return video_features
    
    def encode_text(self, text: torch.Tensor) -> torch.Tensor:
        """
        ç¼–ç æ–‡æœ¬
        
        Args:
            text: (B, L) token ids
        
        Returns:
            æ–‡æœ¬åµŒå…¥ (B, 512)
        """
        if self.clip_model is not None:
            text_features = self.clip_model.encode_text(text)
        else:
            # Fallback: ä½¿ç”¨åµŒå…¥å±‚
            if not hasattr(self, '_text_embed'):
                self._text_embed = nn.Embedding(50000, 512).to(text.device)
            text_embed = self._text_embed(text.clamp(0, 49999))
            text_features = text_embed.mean(dim=1)
        
        text_features = F.normalize(text_features, dim=-1)
        return text_features
    
    def forward(self, text: torch.Tensor, video: torch.Tensor):
        """
        å‰å‘ä¼ æ’­ï¼Œè®¡ç®—å¯¹æ¯”æŸå¤±
        
        Args:
            text: (B, L) token ids
            video: (B, T, C, H, W)
        
        Returns:
            loss, logits_per_video, logits_per_text
        """
        video_features = self.encode_video(video)
        text_features = self.encode_text(text)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        logit_scale = self.logit_scale.exp()
        logits_per_video = logit_scale * video_features @ text_features.t()
        logits_per_text = logits_per_video.t()
        
        # å¯¹æ¯”æŸå¤±
        batch_size = video.shape[0]
        labels = torch.arange(batch_size, device=video.device)
        
        loss_v = F.cross_entropy(logits_per_video, labels)
        loss_t = F.cross_entropy(logits_per_text, labels)
        loss = (loss_v + loss_t) / 2
        
        return loss, logits_per_video, logits_per_text


def train_epoch(model, dataloader, optimizer, device, epoch, pbar=None):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()
    total_loss = 0
    
    for batch_idx, (text, video) in enumerate(dataloader):
        text = text.to(device)
        video = video.to(device)
        
        optimizer.zero_grad()
        
        loss, logits_v, logits_t = model(text, video)
        
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        if pbar:
            pbar.set_postfix_str(f"loss={loss.item():.4f}")
            pbar.update(1)
        elif batch_idx % 10 == 0:
            logger.info(
                f"Epoch {epoch} [{batch_idx}/{len(dataloader)}] "
                f"Loss: {loss.item():.4f}"
            )
    
    return total_loss / len(dataloader)


def main():
    parser = argparse.ArgumentParser(description="CLIP4MC ç®€åŒ–è®­ç»ƒ")
    
    parser.add_argument(
        "--data-log",
        type=Path,
        default=Path("data/training/clip4mc/data_log.json"),
        help="æ•°æ® log æ–‡ä»¶"
    )
    
    parser.add_argument(
        "--pretrain-model",
        type=Path,
        default=Path("data/weights/vit-b-16-clip/ViT-B-16.pt"),
        help="é¢„è®­ç»ƒ CLIP æƒé‡"
    )
    
    parser.add_argument("--epochs", type=int, default=5, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=1, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-5, help="å­¦ä¹ ç‡")
    parser.add_argument("--device", type=str, default="auto", help="è®¾å¤‡")
    parser.add_argument("--output", type=Path, default=None, help="æ¨¡å‹ä¿å­˜è·¯å¾„")
    
    args = parser.parse_args()
    
    # è®¾å¤‡é€‰æ‹©
    if args.device == "auto":
        if torch.cuda.is_available():
            device = torch.device("cuda")
        elif torch.backends.mps.is_available():
            device = torch.device("mps")
        else:
            device = torch.device("cpu")
    else:
        device = torch.device(args.device)
    
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    if not args.data_log.exists():
        logger.error(f"æ•°æ® log æ–‡ä»¶ä¸å­˜åœ¨: {args.data_log}")
        return
    
    with open(args.data_log) as f:
        data_log = json.load(f)
    
    train_dirs = data_log.get("train", [])
    test_dirs = data_log.get("test", [])
    
    logger.info(f"è®­ç»ƒé›†: {len(train_dirs)} ä¸ªæ ·æœ¬")
    logger.info(f"æµ‹è¯•é›†: {len(test_dirs)} ä¸ªæ ·æœ¬")
    
    if not train_dirs:
        logger.error("æ²¡æœ‰è®­ç»ƒæ•°æ®")
        return
    
    # åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
    train_dataset = SimpleClip4MCDataset(train_dirs, "train")
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0,
    )
    
    # åˆ›å»ºæ¨¡å‹
    logger.info("åˆ›å»ºæ¨¡å‹...")
    model = SimpleCLIP4MC(str(args.pretrain_model))
    model = model.to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        weight_decay=0.01,
    )
    
    # è®­ç»ƒ
    logger.info("\n" + "=" * 60)
    logger.info("å¼€å§‹è®­ç»ƒ")
    logger.info("=" * 60)
    
    total_steps = args.epochs * len(train_loader)
    
    if tqdm:
        with tqdm(
            total=total_steps,
            desc="ğŸ¯ è®­ç»ƒè¿›åº¦",
            unit="step",
            position=0,
            leave=True,
            file=sys.stderr,
            dynamic_ncols=True,
            bar_format='{desc}: {percentage:3.0f}%|{bar}| {n}/{total} [{postfix}] [{elapsed}<{remaining}]'
        ) as pbar:
            for epoch in range(1, args.epochs + 1):
                avg_loss = train_epoch(model, train_loader, optimizer, device, epoch, pbar)
                logger.info(f"Epoch {epoch} å¹³å‡æŸå¤±: {avg_loss:.4f}")
    else:
        for epoch in range(1, args.epochs + 1):
            logger.info(f"\n--- Epoch {epoch}/{args.epochs} ---")
            avg_loss = train_epoch(model, train_loader, optimizer, device, epoch)
            logger.info(f"Epoch {epoch} å¹³å‡æŸå¤±: {avg_loss:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    output_path = args.output or args.data_log.parent / "clip4mc_trained.pt"
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'epochs': args.epochs,
    }, output_path)
    
    logger.info(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
    logger.info("âœ“ è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()

