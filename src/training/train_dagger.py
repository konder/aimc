#!/usr/bin/env python3
"""
DAggerä¸»å¾ªç¯ - è‡ªåŠ¨åŒ–è¿­ä»£è®­ç»ƒ

æ•´åˆçŠ¶æ€æ”¶é›†ã€æ•°æ®èšåˆå’Œé‡æ–°è®­ç»ƒçš„å®Œæ•´DAggeræµç¨‹ã€‚
å¯ä»¥æ‰‹åŠ¨é€æ­¥æ‰§è¡Œï¼Œä¹Ÿå¯ä»¥è‡ªåŠ¨åŒ–è¿è¡Œå¤šè½®è¿­ä»£ã€‚

Usage:
    # æ‰‹åŠ¨æ¨¡å¼: å•è½®è¿­ä»£
    python src/training/train_dagger.py \
        --iteration 1 \
        --base-data data/expert_demos/round_0/ \
        --new-data data/expert_labels/iter_1.pkl \
        --output checkpoints/dagger_iter_1.zip

    # è‡ªåŠ¨æ¨¡å¼: å¤šè½®è¿­ä»£ï¼ˆéœ€è¦äººå·¥æ ‡æ³¨ï¼‰
    python src/training/train_dagger.py \
        --auto \
        --initial-model checkpoints/bc_round_0.zip \
        --iterations 3 \
        --output-dir checkpoints/dagger/
"""

import os
import sys
import argparse
import numpy as np
import pickle
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.training.train_bc import load_expert_demonstrations, train_bc_with_ppo
from tools.run_policy_collect_states import collect_policy_states
from tools.evaluate_policy import evaluate_policy


def aggregate_data(base_data_path, new_data_path, output_path):
    """
    èšåˆåŸºç¡€æ•°æ®å’Œæ–°æ ‡æ³¨æ•°æ®
    
    Args:
        base_data_path: åŸºç¡€æ•°æ®è·¯å¾„ï¼ˆç›®å½•æˆ–.pklï¼‰
        new_data_path: æ–°æ ‡æ³¨æ•°æ®è·¯å¾„ï¼ˆ.pklï¼‰
        output_path: è¾“å‡ºèšåˆæ•°æ®è·¯å¾„ï¼ˆ.pklï¼‰
    
    Returns:
        (observations, actions): èšåˆåçš„æ•°æ®
    """
    
    print(f"\n{'='*60}")
    print(f"æ•°æ®èšåˆ")
    print(f"{'='*60}")
    
    # åŠ è½½åŸºç¡€æ•°æ®
    print(f"åŠ è½½åŸºç¡€æ•°æ®: {base_data_path}")
    base_obs, base_actions = load_expert_demonstrations(base_data_path)
    print(f"  åŸºç¡€æ•°æ®: {len(base_obs)} æ ·æœ¬")
    
    # åŠ è½½æ–°æ ‡æ³¨æ•°æ®
    print(f"åŠ è½½æ–°æ ‡æ³¨æ•°æ®: {new_data_path}")
    with open(new_data_path, 'rb') as f:
        new_labeled = pickle.load(f)
    
    new_obs = np.array([item['observation'] for item in new_labeled])
    new_actions = np.array([item['expert_action'] for item in new_labeled])
    print(f"  æ–°æ ‡æ³¨: {len(new_obs)} æ ·æœ¬")
    
    # èšåˆ
    all_obs = np.concatenate([base_obs, new_obs], axis=0)
    all_actions = np.concatenate([base_actions, new_actions], axis=0)
    
    print(f"  èšåˆå: {len(all_obs)} æ ·æœ¬")
    print(f"{'='*60}\n")
    
    # ä¿å­˜èšåˆæ•°æ®
    if output_path:
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else ".", exist_ok=True)
        aggregated_data = []
        for obs, action in zip(all_obs, all_actions):
            aggregated_data.append({
                'observation': obs,
                'expert_action': action
            })
        
        with open(output_path, 'wb') as f:
            pickle.dump(aggregated_data, f)
        
        print(f"âœ“ èšåˆæ•°æ®å·²ä¿å­˜: {output_path}\n")
    
    return all_obs, all_actions


def run_dagger_iteration(
    iteration,
    current_model,
    base_data_path,
    output_dir,
    task_id="harvest_1_log",
    num_episodes=20,
    learning_rate=3e-4,
    epochs=30
):
    """
    è¿è¡Œå•è½®DAggerè¿­ä»£
    
    æµç¨‹:
    1. è¯„ä¼°å½“å‰ç­–ç•¥
    2. è¿è¡Œç­–ç•¥æ”¶é›†å¤±è´¥çŠ¶æ€
    3. ç­‰å¾…äººå·¥æ ‡æ³¨
    4. èšåˆæ•°æ®
    5. é‡æ–°è®­ç»ƒ
    6. è¯„ä¼°æ–°ç­–ç•¥
    
    Args:
        iteration: è¿­ä»£è½®æ¬¡ï¼ˆ1, 2, 3, ...ï¼‰
        current_model: å½“å‰ç­–ç•¥æ¨¡å‹è·¯å¾„
        base_data_path: åŸºç¡€æ•°æ®è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        task_id: MineDojoä»»åŠ¡ID
        num_episodes: æ”¶é›†çš„episodeæ•°é‡
        learning_rate: å­¦ä¹ ç‡
        epochs: è®­ç»ƒè½®æ•°
    """
    
    print(f"\n{'='*70}")
    print(f"DAgger è¿­ä»£ {iteration}")
    print(f"{'='*70}\n")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    states_dir = os.path.join(output_dir, f"states_iter_{iteration}")
    labels_file = os.path.join(output_dir, f"labels_iter_{iteration}.pkl")
    combined_file = os.path.join(output_dir, f"combined_iter_{iteration}.pkl")
    new_model_file = os.path.join(output_dir, f"dagger_iter_{iteration}.zip")
    
    # æ­¥éª¤1: è¯„ä¼°å½“å‰ç­–ç•¥
    print(f"[æ­¥éª¤1/{6}] è¯„ä¼°å½“å‰ç­–ç•¥")
    print(f"-" * 70)
    eval_results = evaluate_policy(
        model_path=current_model,
        num_episodes=10,
        task_id=task_id
    )
    
    if eval_results:
        current_success_rate = eval_results['success_rate']
        print(f"å½“å‰æˆåŠŸç‡: {current_success_rate*100:.1f}%\n")
        
        if current_success_rate >= 0.90:
            print(f"âœ“ æˆåŠŸç‡å·²è¾¾åˆ°90%ï¼Œæ— éœ€ç»§ç»­è¿­ä»£")
            return current_model, True  # è¿”å›Trueè¡¨ç¤ºå·²æ”¶æ•›
    
    # æ­¥éª¤2: è¿è¡Œç­–ç•¥æ”¶é›†çŠ¶æ€
    print(f"[æ­¥éª¤2/6] è¿è¡Œç­–ç•¥æ”¶é›†å¤±è´¥çŠ¶æ€")
    print(f"-" * 70)
    collect_stats = collect_policy_states(
        model_path=current_model,
        num_episodes=num_episodes,
        output_dir=states_dir,
        task_id=task_id,
        save_failures_only=True,  # åªä¿å­˜å¤±è´¥çš„episode
        max_steps=1000
    )
    
    if not collect_stats or collect_stats['saved_episodes'] == 0:
        print(f"âš ï¸  æœªæ”¶é›†åˆ°å¤±è´¥episodeï¼Œè¿­ä»£ç»ˆæ­¢")
        return current_model, True
    
    # æ­¥éª¤3: ç­‰å¾…äººå·¥æ ‡æ³¨
    print(f"[æ­¥éª¤3/6] ç­‰å¾…äººå·¥æ ‡æ³¨")
    print(f"-" * 70)
    print(f"è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤è¿›è¡Œæ ‡æ³¨:")
    print(f"\n  python tools/label_states.py \\")
    print(f"      --states {states_dir} \\")
    print(f"      --output {labels_file} \\")
    print(f"      --smart-sampling\n")
    
    input(f"å®Œæˆæ ‡æ³¨åï¼ŒæŒ‰Enterç»§ç»­...")
    
    # éªŒè¯æ ‡æ³¨æ–‡ä»¶å­˜åœ¨
    if not os.path.exists(labels_file):
        print(f"âœ— é”™è¯¯: æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨: {labels_file}")
        print(f"  è¯·å…ˆå®Œæˆæ ‡æ³¨ï¼")
        return current_model, False
    
    # æ­¥éª¤4: èšåˆæ•°æ®
    print(f"\n[æ­¥éª¤4/6] èšåˆæ•°æ®")
    print(f"-" * 70)
    
    # æ›´æ–°base_data_path: å¦‚æœæ˜¯ç¬¬ä¸€è½®ï¼Œä½¿ç”¨åŸå§‹æ•°æ®ï¼›å¦åˆ™ä½¿ç”¨ä¸Šä¸€è½®çš„èšåˆæ•°æ®
    if iteration > 1:
        prev_combined = os.path.join(output_dir, f"combined_iter_{iteration-1}.pkl")
        if os.path.exists(prev_combined):
            base_data_path = prev_combined
    
    all_obs, all_actions = aggregate_data(
        base_data_path=base_data_path,
        new_data_path=labels_file,
        output_path=combined_file
    )
    
    # æ­¥éª¤5: é‡æ–°è®­ç»ƒ
    print(f"[æ­¥éª¤5/6] é‡æ–°è®­ç»ƒç­–ç•¥")
    print(f"-" * 70)
    new_model = train_bc_with_ppo(
        observations=all_obs,
        actions=all_actions,
        output_path=new_model_file,
        task_id=task_id,
        learning_rate=learning_rate,
        n_epochs=epochs
    )
    
    # æ­¥éª¤6: è¯„ä¼°æ–°ç­–ç•¥
    print(f"[æ­¥éª¤6/6] è¯„ä¼°æ–°ç­–ç•¥")
    print(f"-" * 70)
    new_eval_results = evaluate_policy(
        model_path=new_model_file,
        num_episodes=20,
        task_id=task_id
    )
    
    if new_eval_results:
        new_success_rate = new_eval_results['success_rate']
        improvement = (new_success_rate - current_success_rate) * 100
        
        print(f"\n{'='*70}")
        print(f"è¿­ä»£ {iteration} å®Œæˆ")
        print(f"{'='*70}")
        print(f"æˆåŠŸç‡: {current_success_rate*100:.1f}% â†’ {new_success_rate*100:.1f}% (+{improvement:.1f}%)")
        print(f"æ–°æ¨¡å‹: {new_model_file}")
        print(f"{'='*70}\n")
    
    return new_model_file, False  # è¿”å›Falseè¡¨ç¤ºæœªæ”¶æ•›


def main():
    parser = argparse.ArgumentParser(
        description="DAggerè®­ç»ƒ - Dataset Aggregationè¿­ä»£ä¼˜åŒ–"
    )
    
    # æ‰‹åŠ¨æ¨¡å¼å‚æ•°
    parser.add_argument(
        "--iteration",
        type=int,
        help="è¿­ä»£è½®æ¬¡ï¼ˆæ‰‹åŠ¨æ¨¡å¼ï¼‰"
    )
    
    parser.add_argument(
        "--base-data",
        type=str,
        help="åŸºç¡€æ•°æ®è·¯å¾„ï¼ˆæ‰‹åŠ¨æ¨¡å¼ï¼‰"
    )
    
    parser.add_argument(
        "--new-data",
        type=str,
        help="æ–°æ ‡æ³¨æ•°æ®è·¯å¾„ï¼ˆæ‰‹åŠ¨æ¨¡å¼ï¼Œ.pklæ–‡ä»¶ï¼‰"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        help="è¾“å‡ºæ¨¡å‹è·¯å¾„ï¼ˆæ‰‹åŠ¨æ¨¡å¼ï¼‰"
    )
    
    # è‡ªåŠ¨æ¨¡å¼å‚æ•°
    parser.add_argument(
        "--auto",
        action="store_true",
        help="è‡ªåŠ¨æ¨¡å¼ï¼ˆå¤šè½®è¿­ä»£ï¼‰"
    )
    
    parser.add_argument(
        "--initial-model",
        type=str,
        help="åˆå§‹BCæ¨¡å‹è·¯å¾„ï¼ˆè‡ªåŠ¨æ¨¡å¼ï¼‰"
    )
    
    parser.add_argument(
        "--initial-data",
        type=str,
        help="åˆå§‹ä¸“å®¶æ•°æ®è·¯å¾„ï¼ˆè‡ªåŠ¨æ¨¡å¼ï¼‰"
    )
    
    parser.add_argument(
        "--iterations",
        type=int,
        default=3,
        help="è¿­ä»£æ¬¡æ•°ï¼ˆè‡ªåŠ¨æ¨¡å¼ï¼Œé»˜è®¤: 3ï¼‰"
    )
    
    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/dagger",
        help="è¾“å‡ºç›®å½•ï¼ˆè‡ªåŠ¨æ¨¡å¼ï¼Œé»˜è®¤: data/daggerï¼‰"
    )
    
    # é€šç”¨å‚æ•°
    parser.add_argument(
        "--task-id",
        type=str,
        default="harvest_1_log",
        help="MineDojoä»»åŠ¡IDï¼ˆé»˜è®¤: harvest_1_logï¼‰"
    )
    
    parser.add_argument(
        "--num-episodes",
        type=int,
        default=20,
        help="æ¯è½®æ”¶é›†çš„episodeæ•°ï¼ˆé»˜è®¤: 20ï¼‰"
    )
    
    parser.add_argument(
        "--learning-rate",
        type=float,
        default=3e-4,
        help="å­¦ä¹ ç‡ï¼ˆé»˜è®¤: 3e-4ï¼‰"
    )
    
    parser.add_argument(
        "--epochs",
        type=int,
        default=30,
        help="è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤: 30ï¼‰"
    )
    
    args = parser.parse_args()
    
    if args.auto:
        # è‡ªåŠ¨æ¨¡å¼: å¤šè½®è¿­ä»£
        print(f"\nğŸ”„ DAggerè‡ªåŠ¨æ¨¡å¼")
        print(f"è¿­ä»£æ¬¡æ•°: {args.iterations}")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        
        if not args.initial_model or not args.initial_data:
            print(f"âœ— è‡ªåŠ¨æ¨¡å¼éœ€è¦ --initial-model å’Œ --initial-data")
            sys.exit(1)
        
        current_model = args.initial_model
        
        for i in range(1, args.iterations + 1):
            current_model, converged = run_dagger_iteration(
                iteration=i,
                current_model=current_model,
                base_data_path=args.initial_data,
                output_dir=args.output_dir,
                task_id=args.task_id,
                num_episodes=args.num_episodes,
                learning_rate=args.learning_rate,
                epochs=args.epochs
            )
            
            if converged:
                print(f"âœ“ DAggerå·²æ”¶æ•›ï¼Œåœæ­¢è¿­ä»£")
                break
        
        print(f"\nâœ“ DAggerè®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ç»ˆæ¨¡å‹: {current_model}\n")
    
    else:
        # æ‰‹åŠ¨æ¨¡å¼: å•è½®è¿­ä»£
        if not all([args.iteration, args.base_data, args.new_data, args.output]):
            print(f"âœ— æ‰‹åŠ¨æ¨¡å¼éœ€è¦: --iteration, --base-data, --new-data, --output")
            sys.exit(1)
        
        # èšåˆæ•°æ®
        all_obs, all_actions = aggregate_data(
            base_data_path=args.base_data,
            new_data_path=args.new_data,
            output_path=None  # ä¸ä¿å­˜ä¸­é—´æ–‡ä»¶
        )
        
        # è®­ç»ƒ
        train_bc_with_ppo(
            observations=all_obs,
            actions=all_actions,
            output_path=args.output,
            task_id=args.task_id,
            learning_rate=args.learning_rate,
            n_epochs=args.epochs
        )
        
        print(f"âœ“ è®­ç»ƒå®Œæˆï¼")
        print(f"æ¨¡å‹: {args.output}\n")


if __name__ == "__main__":
    main()

