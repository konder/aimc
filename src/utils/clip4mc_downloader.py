#!/usr/bin/env python3
"""
CLIP4MC YouTube è§†é¢‘ä¸‹è½½å’Œé¢„å¤„ç†å·¥å…·

ä» CLIP4MC æ•°æ®é›†ä¸‹è½½ Minecraft YouTube è§†é¢‘å¹¶è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼ã€‚
åŸºäº https://github.com/PKU-RL/CLIP4MC çš„æ•°æ®é›†ã€‚

ä½¿ç”¨æ–¹æ³•:
    # ä¸‹è½½è§†é¢‘
    python -m src.utils.clip4mc_downloader download \
        --output-dir data/raw_videos/clip4mc_youtube \
        --max-samples 100 --cookies data/www.youtube.com_cookies.txt

    # é¢„å¤„ç†ä¸ºè®­ç»ƒæ ¼å¼
    python -m src.utils.clip4mc_downloader preprocess \
        --samples-json data/raw_videos/clip4mc_youtube/samples.json \
        --output-dir data/training/clip4mc

    # ä¸€é”®æ‰§è¡Œï¼ˆä¸‹è½½+é¢„å¤„ç†ï¼‰
    python -m src.utils.clip4mc_downloader all \
        --max-samples 100 --cookies data/www.youtube.com_cookies.txt
"""

import argparse
import json
import logging
import pickle
import subprocess
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional
import numpy as np

try:
    from tqdm import tqdm
except ImportError:
    # Fallback: ç®€å•çš„è¿›åº¦æ˜¾ç¤º
    class tqdm:
        def __init__(self, iterable=None, total=None, desc=None, **kwargs):
            self.iterable = iterable
            self.total = total
            self.desc = desc or ""
            self.n = 0
        
        def __enter__(self):
            return self
        
        def __exit__(self, *args):
            print("", file=sys.stderr)
        
        def update(self, n=1):
            self.n += n
            if self.total:
                pct = self.n / self.total * 100
                print(f"\r{self.desc}: {pct:.0f}% ({self.n}/{self.total})", end="", file=sys.stderr)
        
        def set_postfix_str(self, s):
            pass

logger = logging.getLogger(__name__)

# ============================================================
# é…ç½®
# ============================================================

# CLIP4MC æ•°æ®é›† URL
CLIP4MC_DATASET_URLS = {
    "test": "https://huggingface.co/datasets/AnonymousUserCLIP4MC/CLIP4MC/raw/main/dataset_test.json",
    "train": "https://huggingface.co/datasets/AnonymousUserCLIP4MC/CLIP4MC/raw/main/dataset_train.json",
}

# è§†é¢‘å¸§é…ç½®
NUM_FRAMES = 16
FRAME_SIZE = (224, 224)

# é»˜è®¤ä¸‹è½½é…ç½®
DEFAULT_RESOLUTION = 360  # 360p
DEFAULT_FPS = 20  # Minecraft é»˜è®¤å¸§ç‡

# é»˜è®¤è·¯å¾„
DEFAULT_RAW_DIR = Path("data/raw_videos/clip4mc_youtube")
DEFAULT_TRAINING_DIR = Path("data/training/clip4mc")


# ============================================================
# YouTube ä¸‹è½½åŠŸèƒ½
# ============================================================

def download_dataset_json(dataset_type: str, cache_dir: Path) -> List[Dict]:
    """ä¸‹è½½ CLIP4MC æ•°æ®é›† JSON æ–‡ä»¶"""
    import requests
    
    cache_dir.mkdir(parents=True, exist_ok=True)
    cache_file = cache_dir / f"dataset_{dataset_type}.json"
    
    if cache_file.exists():
        logger.info(f"ä½¿ç”¨ç¼“å­˜: {cache_file}")
        with open(cache_file) as f:
            return json.load(f)
    
    url = CLIP4MC_DATASET_URLS.get(dataset_type)
    if not url:
        raise ValueError(f"æœªçŸ¥æ•°æ®é›†ç±»å‹: {dataset_type}")
    
    logger.info(f"ä¸‹è½½æ•°æ®é›†: {url}")
    response = requests.get(url, timeout=60)
    response.raise_for_status()
    
    data = response.json()
    
    with open(cache_file, 'w') as f:
        json.dump(data, f)
    
    logger.info(f"ä¸‹è½½å®Œæˆï¼Œå…± {len(data)} æ¡è®°å½•")
    return data


class DownloadAbortError(Exception):
    """403 é”™è¯¯ï¼Œéœ€è¦ç»ˆæ­¢ä¸‹è½½"""
    pass


def download_youtube_video(
    video_id: str, 
    output_path: Path, 
    timeout: int = 300,
    cookies_file: str = None,
    resolution: int = DEFAULT_RESOLUTION,
    fps: int = None,
) -> bool:
    """
    ä¸‹è½½ YouTube è§†é¢‘ (ä½¿ç”¨ yt-dlp)
    
    Args:
        video_id: YouTube è§†é¢‘ ID
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        cookies_file: cookies.txt æ–‡ä»¶è·¯å¾„
        resolution: æœ€å¤§åˆ†è¾¨ç‡ (360, 480, 720, 1080)
        fps: æœ€å¤§å¸§ç‡ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶
    
    Raises:
        DownloadAbortError: é‡åˆ° 403 é”™è¯¯ï¼Œéœ€è¦ç»ˆæ­¢æ•´ä¸ªä¸‹è½½ä»»åŠ¡
    """
    if output_path.exists():
        logger.debug(f"è§†é¢‘å·²å­˜åœ¨: {output_path}")
        return True
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    url = f"https://www.youtube.com/watch?v={video_id}"
    
    try:
        import yt_dlp
        
        # ä½¿ç”¨ HLS æµæ ¼å¼ (91=144p, 92=240p, 93=360p, 94=480p, 95=720p)
        # HLS æ ¼å¼å¯ä»¥ç»•è¿‡ googlevideo.com çš„ 403 é™åˆ¶
        if resolution <= 144:
            hls_format = "91"
        elif resolution <= 240:
            hls_format = "92"
        elif resolution <= 360:
            hls_format = "93"
        elif resolution <= 480:
            hls_format = "94"
        else:
            hls_format = "95"
        
        ydl_opts = {
            'format': f'{hls_format}/91/best',  # HLS æµæ ¼å¼ä¼˜å…ˆ
            'outtmpl': str(output_path.with_suffix('.mp4')),
            'noplaylist': True,
            'quiet': True,
            'no_warnings': True,
            'socket_timeout': timeout,
            'retries': 10,
            'fragment_retries': 10,
            'extractor_retries': 3,
            'proxy': '',  # ç¦ç”¨ä»£ç†
        }
        
        if cookies_file and Path(cookies_file).exists():
            ydl_opts['cookiefile'] = cookies_file
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([url])
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
        if output_path.exists():
            return True
        output_with_ext = output_path.with_suffix('.mp4')
        if output_with_ext != output_path and output_with_ext.exists():
            output_with_ext.rename(output_path)
            return True
        possible_files = list(output_path.parent.glob(f"{video_id}*.mp4"))
        if possible_files:
            possible_files[0].rename(output_path)
            return True
            
        logger.warning(f"ä¸‹è½½å®Œæˆä½†æ–‡ä»¶ä¸å­˜åœ¨: {output_path}")
        return False
            
    except ImportError:
        logger.error("è¯·å…ˆå®‰è£… yt-dlp: pip install yt-dlp")
        return False
    except Exception as e:
        error_msg = str(e)
        if "Video unavailable" in error_msg:
            logger.warning(f"è§†é¢‘ä¸å¯ç”¨: {video_id}")
            return False
        elif "Private video" in error_msg:
            logger.warning(f"ç§å¯†è§†é¢‘: {video_id}")
            return False
        elif "403" in error_msg:
            logger.error(f"âŒ 403 Forbidden: {video_id}")
            logger.error("âš ï¸ YouTube æ‹’ç»è®¿é—®ï¼Œè¯·åˆ·æ–° cookies åé‡è¯•")
            raise DownloadAbortError("403 Forbidden - éœ€è¦åˆ·æ–° cookies")
        else:
            logger.warning(f"ä¸‹è½½å¤±è´¥ {video_id}: {error_msg[:100]}")
            return False


def extract_video_clip(
    input_path: Path, 
    output_path: Path, 
    start_time: float, 
    end_time: float
) -> bool:
    """ä½¿ç”¨ ffmpeg æå–è§†é¢‘ç‰‡æ®µ"""
    if output_path.exists():
        return True
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    duration = end_time - start_time
    
    cmd = [
        "ffmpeg", "-y",
        "-ss", str(start_time),
        "-i", str(input_path),
        "-t", str(duration),
        "-c:v", "libx264",
        "-c:a", "aac",
        "-preset", "fast",
        "-loglevel", "error",
        str(output_path)
    ]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        if result.returncode == 0 and output_path.exists():
            return True
        logger.warning(f"ffmpeg å¤±è´¥: {result.stderr[:200] if result.stderr else 'Unknown'}")
        return False
    except subprocess.TimeoutExpired:
        logger.warning("ffmpeg è¶…æ—¶")
        return False
    except FileNotFoundError:
        logger.error("è¯·å…ˆå®‰è£… ffmpeg: brew install ffmpeg")
        return False
    except Exception as e:
        logger.warning(f"æå–å¤±è´¥: {e}")
        return False


def download_videos(
    output_dir: Path,
    dataset_type: str = "test",
    max_samples: int = None,
    cookies_file: str = None,
    resume: bool = True,
    resolution: int = DEFAULT_RESOLUTION,
    fps: int = None,
    delete_original: bool = True,
) -> List[Dict]:
    """
    ä¸‹è½½ CLIP4MC æ•°æ®é›†çš„è§†é¢‘ç‰‡æ®µ
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        dataset_type: "test" æˆ– "train"
        max_samples: æœ€å¤§æ ·æœ¬æ•°
        cookies_file: cookies.txt æ–‡ä»¶è·¯å¾„
        resume: æ˜¯å¦ä»æ£€æŸ¥ç‚¹æ¢å¤
        resolution: ä¸‹è½½åˆ†è¾¨ç‡ (360, 480, 720, 1080)
        fps: æœ€å¤§å¸§ç‡ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶
        delete_original: åˆ‡ç‰‡æˆåŠŸåæ˜¯å¦åˆ é™¤åŸå§‹å®Œæ•´è§†é¢‘
    
    Returns:
        å¤„ç†æˆåŠŸçš„æ ·æœ¬åˆ—è¡¨
    """
    import datetime
    
    output_dir = Path(output_dir)
    videos_dir = output_dir / "videos"
    clips_dir = output_dir / "clips"
    cache_dir = output_dir / ".cache"
    reports_dir = output_dir / "reports"
    
    # åˆ›å»ºç›®å½•
    for d in [videos_dir, clips_dir, cache_dir, reports_dir]:
        d.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥ç‚¹æ–‡ä»¶
    checkpoint_file = output_dir / ".checkpoint.json"
    samples_file = output_dir / "samples.json"
    
    start_time = time.time()
    
    data = download_dataset_json(dataset_type, cache_dir)
    logger.info(f"æ•°æ®é›†å…± {len(data)} æ¡è®°å½•")
    
    # è¿‡æ»¤æœ‰æ•ˆæ ·æœ¬ (1-30ç§’)
    valid_samples = [
        s for s in data 
        if 1.0 <= (s.get("end position", 0) - s.get("begin position", 0)) <= 30.0
    ]
    logger.info(f"æœ‰æ•ˆæ ·æœ¬ï¼ˆ1-30ç§’ï¼‰: {len(valid_samples)} æ¡")
    
    if max_samples:
        samples_to_process = valid_samples[:max_samples]
    else:
        samples_to_process = valid_samples
    
    total = len(samples_to_process)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    processed_samples = []
    failed_samples = []
    skipped_samples = []
    processed_ids = set()
    start_idx = 0
    
    if resume and checkpoint_file.exists():
        try:
            with open(checkpoint_file) as f:
                checkpoint = json.load(f)
            processed_samples = checkpoint.get("processed_samples", [])
            failed_samples = checkpoint.get("failed_samples", [])
            skipped_samples = checkpoint.get("skipped_samples", [])
            processed_ids = set(checkpoint.get("processed_ids", []))
            start_idx = checkpoint.get("last_idx", 0)
            
            logger.info(f"ğŸ“‚ ä»æ£€æŸ¥ç‚¹æ¢å¤: å·²å¤„ç† {len(processed_ids)} ä¸ª, ä»ç¬¬ {start_idx + 1} ä¸ªç»§ç»­")
        except Exception as e:
            logger.warning(f"æ— æ³•åŠ è½½æ£€æŸ¥ç‚¹: {e}")
    
    remaining = total - len(processed_ids)
    logger.info(f"å°†å¤„ç† {remaining} ä¸ªæ ·æœ¬ (æ€»è®¡ {total})\n")
    
    def save_checkpoint(idx):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            "last_idx": idx,
            "processed_ids": list(processed_ids),
            "processed_samples": processed_samples,
            "failed_samples": failed_samples,
            "skipped_samples": skipped_samples,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        }
        with open(checkpoint_file, 'w') as f:
            json.dump(checkpoint, f, indent=2, ensure_ascii=False)
        # åŒæ—¶æ›´æ–° samples.json
        with open(samples_file, 'w') as f:
            json.dump(processed_samples, f, indent=2, ensure_ascii=False)
    
    # ä½¿ç”¨å›ºå®šåœ¨åº•éƒ¨çš„è¿›åº¦æ¡
    try:
        with tqdm(
            total=total,
            initial=len(processed_ids),
            desc=f"ğŸ“¥ {len(processed_ids)}/{total}",
            unit="video",
            position=0,
            leave=True,
            file=sys.stderr,
            dynamic_ncols=True,
            bar_format='{desc} | {percentage:3.0f}%|{bar}| âœ“{postfix} [{elapsed}<{remaining}]'
        ) as pbar:
            
            for idx, sample in enumerate(samples_to_process):
                video_id = sample.get("vid", "unknown")
                transcript = sample.get("transcript clip", "")
                begin_pos = sample.get("begin position", 0)
                end_pos = sample.get("end position", 0)
                
                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
                sample_id = f"{video_id}_{int(begin_pos)}_{int(end_pos)}"
                
                # è·³è¿‡å·²å¤„ç†çš„
                if sample_id in processed_ids:
                    continue
                
                # æ›´æ–°è¿›åº¦æ¡æè¿°
                pbar.set_description(f"ğŸ“¥ {idx+1}/{total}")
                pbar.set_postfix_str(f"{len(processed_samples)} âœ—{len(failed_samples)}")
                
                if not video_id or not transcript:
                    skipped_samples.append({"video_id": video_id, "reason": "æ— æ•ˆæ•°æ®"})
                    processed_ids.add(sample_id)
                    pbar.update(1)
                    continue
                
                # ä¸‹è½½å®Œæ•´è§†é¢‘
                video_path = videos_dir / f"{video_id}.mp4"
                need_download = not video_path.exists()
                
                if need_download:
                    try:
                        if not download_youtube_video(
                            video_id, video_path, 
                            cookies_file=cookies_file,
                            resolution=resolution,
                            fps=fps
                        ):
                            failed_samples.append({
                                "video_id": video_id,
                                "reason": "ä¸‹è½½å¤±è´¥",
                                "begin": begin_pos,
                                "end": end_pos,
                            })
                            processed_ids.add(sample_id)
                            pbar.update(1)
                            # æ¯10ä¸ªå¤±è´¥ä¿å­˜æ£€æŸ¥ç‚¹
                            if len(failed_samples) % 10 == 0:
                                save_checkpoint(idx)
                            time.sleep(0.5)
                            continue
                    except DownloadAbortError as e:
                        # 403 é”™è¯¯ï¼Œç»ˆæ­¢æ•´ä¸ªä¸‹è½½ä»»åŠ¡
                        pbar.close()
                        save_checkpoint(idx)
                        logger.error(f"\nâŒ ä¸‹è½½ä»»åŠ¡ç»ˆæ­¢: {e}")
                        logger.error(f"å·²å¤„ç†: {len(processed_samples)} æˆåŠŸ, {len(failed_samples)} å¤±è´¥")
                        logger.error("è¯·åˆ·æ–° cookies åä½¿ç”¨ 'status' æŸ¥çœ‹è¿›åº¦ï¼Œ'download' ç»§ç»­ä¸‹è½½")
                        return {
                            "success": False,
                            "aborted": True,
                            "reason": str(e),
                            "processed": len(processed_samples),
                            "failed": len(failed_samples),
                        }
                
                # æå–ç‰‡æ®µ
                clip_name = f"{video_id}_{int(begin_pos)}_{int(end_pos)}.mp4"
                clip_path = clips_dir / clip_name
                
                if not clip_path.exists():
                    if not extract_video_clip(video_path, clip_path, begin_pos, end_pos):
                        failed_samples.append({
                            "video_id": video_id,
                            "reason": "ç‰‡æ®µæå–å¤±è´¥",
                            "begin": begin_pos,
                            "end": end_pos,
                        })
                        processed_ids.add(sample_id)
                        pbar.update(1)
                        continue
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–ç‰‡æ®µéœ€è¦ä»è¿™ä¸ªè§†é¢‘æå–
                # å¦‚æœå¯ç”¨ delete_originalï¼Œåœ¨æ‰€æœ‰ç‰‡æ®µæå–å®Œæˆååˆ é™¤åŸå§‹è§†é¢‘
                if delete_original and video_path.exists():
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¾…å¤„ç†çš„ç‰‡æ®µæ¥è‡ªåŒä¸€è§†é¢‘
                    remaining_from_same_video = any(
                        s.get("vid") == video_id and 
                        f"{video_id}_{int(s.get('begin position', 0))}_{int(s.get('end position', 0))}" not in processed_ids
                        for s in samples_to_process[idx+1:]
                    )
                    if not remaining_from_same_video:
                        try:
                            video_path.unlink()
                            logger.debug(f"å·²åˆ é™¤åŸå§‹è§†é¢‘: {video_path.name}")
                        except Exception as e:
                            logger.debug(f"åˆ é™¤å¤±è´¥: {e}")
                
                processed_samples.append({
                    "video_id": video_id,
                    "text": transcript.strip(),
                    "clip_path": str(clip_path),
                    "begin_time": begin_pos,
                    "end_time": end_pos,
                    "duration": end_pos - begin_pos,
                })
                processed_ids.add(sample_id)
                
                pbar.update(1)
                
                # æ¯20ä¸ªæˆåŠŸä¿å­˜æ£€æŸ¥ç‚¹
                if len(processed_samples) % 20 == 0:
                    save_checkpoint(idx)
                
                time.sleep(0.3)
    
    except KeyboardInterrupt:
        logger.info("\n\nâš ï¸ ä¸‹è½½è¢«ä¸­æ–­ï¼Œä¿å­˜æ£€æŸ¥ç‚¹...")
        save_checkpoint(idx if 'idx' in dir() else 0)
        logger.info(f"âœ“ æ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå°†è‡ªåŠ¨æ¢å¤")
        raise
    
    elapsed_time = time.time() - start_time
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    with open(samples_file, 'w', encoding='utf-8') as f:
        json.dump(processed_samples, f, indent=2, ensure_ascii=False)
    
    # åˆ é™¤æ£€æŸ¥ç‚¹ï¼ˆä¸‹è½½å®Œæˆï¼‰
    if checkpoint_file.exists():
        checkpoint_file.unlink()
        logger.info("âœ“ ä¸‹è½½å®Œæˆï¼Œæ£€æŸ¥ç‚¹å·²æ¸…é™¤")
    
    # ç”Ÿæˆä¸‹è½½æŠ¥å‘Š
    report = {
        "timestamp": datetime.datetime.now().isoformat(),
        "dataset_type": dataset_type,
        "elapsed_seconds": round(elapsed_time, 2),
        "elapsed_human": f"{int(elapsed_time//60)}åˆ†{int(elapsed_time%60)}ç§’",
        "summary": {
            "total": total,
            "success": len(processed_samples),
            "failed": len(failed_samples),
            "skipped": len(skipped_samples),
            "success_rate": f"{len(processed_samples)/total*100:.1f}%" if total > 0 else "0%",
        },
        "output_files": {
            "samples_json": str(samples_file),
            "videos_dir": str(videos_dir),
            "clips_dir": str(clips_dir),
        },
        "failed_samples": failed_samples[:50],  # åªä¿å­˜å‰50ä¸ªå¤±è´¥æ ·æœ¬
        "skipped_samples": skipped_samples[:20],
    }
    
    report_file = reports_dir / f"download_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
    print(f"\n{'='*60}", file=sys.stderr)
    print(f"ğŸ“Š CLIP4MC ä¸‹è½½æŠ¥å‘Š", file=sys.stderr)
    print(f"{'='*60}", file=sys.stderr)
    print(f"  ğŸ“… æ—¶é—´: {report['timestamp'][:19]}", file=sys.stderr)
    print(f"  â±ï¸  è€—æ—¶: {report['elapsed_human']}", file=sys.stderr)
    print(f"  ğŸ“¦ æ•°æ®é›†: {dataset_type}", file=sys.stderr)
    print(f"", file=sys.stderr)
    print(f"  ğŸ“ˆ ç»Ÿè®¡:", file=sys.stderr)
    print(f"     æ€»ä»»åŠ¡: {total}", file=sys.stderr)
    print(f"     âœ“ æˆåŠŸ: {len(processed_samples)} ({report['summary']['success_rate']})", file=sys.stderr)
    print(f"     âœ— å¤±è´¥: {len(failed_samples)}", file=sys.stderr)
    print(f"     âŠ˜ è·³è¿‡: {len(skipped_samples)}", file=sys.stderr)
    print(f"", file=sys.stderr)
    print(f"  ğŸ“ è¾“å‡ºæ–‡ä»¶:", file=sys.stderr)
    print(f"     æ ·æœ¬: {samples_file}", file=sys.stderr)
    print(f"     è§†é¢‘: {videos_dir}", file=sys.stderr)
    print(f"     åˆ‡ç‰‡: {clips_dir}", file=sys.stderr)
    print(f"     æŠ¥å‘Š: {report_file}", file=sys.stderr)
    print(f"{'='*60}", file=sys.stderr)
    
    return processed_samples


# ============================================================
# æ•°æ®é¢„å¤„ç†åŠŸèƒ½
# ============================================================

def extract_video_frames(video_path: Path, num_frames: int = NUM_FRAMES) -> np.ndarray:
    """ä»è§†é¢‘ä¸­å‡åŒ€é‡‡æ ·å¸§"""
    try:
        import cv2
    except ImportError:
        logger.error("è¯·å®‰è£… opencv-python: pip install opencv-python")
        raise
    
    cap = cv2.VideoCapture(str(video_path))
    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if total_frames < num_frames:
        indices = list(range(total_frames)) + [total_frames - 1] * (num_frames - total_frames)
    else:
        indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    
    frames = []
    for idx in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        
        if not ret:
            frame = frames[-1].copy() if frames else np.zeros((FRAME_SIZE[1], FRAME_SIZE[0], 3), dtype=np.uint8)
        
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frame = cv2.resize(frame, FRAME_SIZE)
        frames.append(frame)
    
    cap.release()
    return np.array(frames, dtype=np.uint8)


def get_tokenizer():
    """è·å– CLIP tokenizer"""
    try:
        from transformers import CLIPTokenizer
        return CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch16")
    except Exception:
        try:
            import open_clip
            tokenizer = open_clip.get_tokenizer('ViT-B-16')
            
            class TokenizerWrapper:
                def __init__(self, tok):
                    self._tokenizer = tok
                
                def __call__(self, text, max_length=77, padding='max_length', 
                           truncation=True, return_tensors='np'):
                    tokens = self._tokenizer(text)
                    return {'input_ids': tokens.numpy() if return_tensors == 'np' else tokens}
            
            return TokenizerWrapper(tokenizer)
        except ImportError:
            logger.error("è¯·å®‰è£… transformers æˆ– open-clip-torch")
            raise


def preprocess_samples(samples_json: Path, output_dir: Path) -> List[str]:
    """å°†è§†é¢‘ç‰‡æ®µè½¬æ¢ä¸º CLIP4MC è®­ç»ƒæ ¼å¼"""
    import datetime
    
    start_time = time.time()
    
    logger.info("åŠ è½½ CLIP tokenizer...")
    tokenizer = get_tokenizer()
    logger.info("âœ“ Tokenizer å·²åŠ è½½\n")
    
    with open(samples_json) as f:
        samples = json.load(f)
    
    if not samples:
        logger.error("æ²¡æœ‰å¯å¤„ç†çš„æ ·æœ¬")
        return []
    
    total = len(samples)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    processed_dirs = []
    failed_samples = []
    
    # ä½¿ç”¨å›ºå®šåœ¨åº•éƒ¨çš„è¿›åº¦æ¡
    with tqdm(
        total=total,
        desc=f"ğŸ”„ 0/{total}",
        unit="sample",
        position=0,
        leave=True,
        file=sys.stderr,
        dynamic_ncols=True,
        bar_format='{desc} | {percentage:3.0f}%|{bar}| âœ“{postfix} [{elapsed}<{remaining}]'
    ) as pbar:
        
        for idx, sample in enumerate(samples):
            clip_path = Path(sample['clip_path'])
            text = sample['text']
            video_id = sample.get('video_id', 'unknown')
            
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            pbar.set_description(f"ğŸ”„ {idx+1}/{total}")
            pbar.set_postfix_str(f"{len(processed_dirs)} âœ—{len(failed_samples)}")
            
            if not clip_path.exists():
                failed_samples.append({"video_id": video_id, "reason": "æ–‡ä»¶ä¸å­˜åœ¨"})
                pbar.update(1)
                continue
            
            sample_dir = output_dir / f"data_dir_{idx}"
            sample_dir.mkdir(parents=True, exist_ok=True)
            
            try:
                # æå–è§†é¢‘å¸§
                frames = extract_video_frames(clip_path)
                
                # Tokenize æ–‡æœ¬
                tokens = tokenizer(text, max_length=77, padding='max_length', 
                                 truncation=True, return_tensors='np')
                text_input = {'tokens': tokens['input_ids'][0]}
                
                # ä¿å­˜
                with open(sample_dir / "video_input.pkl", 'wb') as f:
                    pickle.dump(frames, f)
                
                with open(sample_dir / "text_input.pkl", 'wb') as f:
                    pickle.dump(text_input, f)
                
                with open(sample_dir / "size.json", 'w') as f:
                    json.dump({
                        "num_samples": 1,
                        "video_shape": list(frames.shape),
                        "text_length": len(text_input['tokens']),
                        "original_text": text,
                        "video_id": video_id,
                    }, f, indent=2)
                
                processed_dirs.append(str(sample_dir))
                
            except Exception as e:
                failed_samples.append({"video_id": video_id, "reason": str(e)[:100]})
            
            pbar.update(1)
    
    elapsed_time = time.time() - start_time
    
    # åˆ›å»º log æ–‡ä»¶
    log_data = None
    if processed_dirs:
        n_train = max(1, int(len(processed_dirs) * 0.8))
        log_data = {
            "train": processed_dirs[:n_train],
            "test": processed_dirs[n_train:] if n_train < len(processed_dirs) else processed_dirs[:1],
        }
        with open(output_dir / "data_log.json", 'w') as f:
            json.dump(log_data, f, indent=2)
    
    # ç”Ÿæˆé¢„å¤„ç†æŠ¥å‘Š
    report = {
        "timestamp": datetime.datetime.now().isoformat(),
        "elapsed_seconds": round(elapsed_time, 2),
        "elapsed_human": f"{int(elapsed_time//60)}åˆ†{int(elapsed_time%60)}ç§’",
        "summary": {
            "total": total,
            "success": len(processed_dirs),
            "failed": len(failed_samples),
            "success_rate": f"{len(processed_dirs)/total*100:.1f}%" if total > 0 else "0%",
        },
        "training_split": {
            "train": len(log_data['train']) if log_data else 0,
            "test": len(log_data['test']) if log_data else 0,
        },
        "output_dir": str(output_dir),
        "failed_samples": failed_samples[:30],
    }
    
    report_file = output_dir / f"preprocess_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
    print(f"\n{'='*60}", file=sys.stderr)
    print(f"ğŸ“Š CLIP4MC é¢„å¤„ç†æŠ¥å‘Š", file=sys.stderr)
    print(f"{'='*60}", file=sys.stderr)
    print(f"  ğŸ“… æ—¶é—´: {report['timestamp'][:19]}", file=sys.stderr)
    print(f"  â±ï¸  è€—æ—¶: {report['elapsed_human']}", file=sys.stderr)
    print(f"", file=sys.stderr)
    print(f"  ğŸ“ˆ ç»Ÿè®¡:", file=sys.stderr)
    print(f"     æ€»æ ·æœ¬: {total}", file=sys.stderr)
    print(f"     âœ“ æˆåŠŸ: {len(processed_dirs)} ({report['summary']['success_rate']})", file=sys.stderr)
    print(f"     âœ— å¤±è´¥: {len(failed_samples)}", file=sys.stderr)
    print(f"", file=sys.stderr)
    if log_data:
        print(f"  ğŸ“š æ•°æ®é›†åˆ’åˆ†:", file=sys.stderr)
        print(f"     è®­ç»ƒé›†: {len(log_data['train'])}", file=sys.stderr)
        print(f"     æµ‹è¯•é›†: {len(log_data['test'])}", file=sys.stderr)
        print(f"", file=sys.stderr)
    print(f"  ğŸ“ è¾“å‡ºæ–‡ä»¶:", file=sys.stderr)
    print(f"     æ•°æ®ç›®å½•: {output_dir}", file=sys.stderr)
    print(f"     æŠ¥å‘Š: {report_file}", file=sys.stderr)
    print(f"{'='*60}", file=sys.stderr)
    
    return processed_dirs


# ============================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description="CLIP4MC YouTube è§†é¢‘ä¸‹è½½å’Œé¢„å¤„ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä¸‹è½½ test é›†å…¨éƒ¨è§†é¢‘
  python -m src.utils.clip4mc_downloader download --dataset test --cookies data/www.youtube.com_cookies.txt

  # ä¸‹è½½ 100 ä¸ªæ ·æœ¬
  python -m src.utils.clip4mc_downloader download --max-samples 100 --cookies data/www.youtube.com_cookies.txt

  # é¢„å¤„ç†å·²ä¸‹è½½çš„è§†é¢‘
  python -m src.utils.clip4mc_downloader preprocess

  # ä¸€é”®æ‰§è¡Œï¼ˆä¸‹è½½+é¢„å¤„ç†ï¼‰
  python -m src.utils.clip4mc_downloader all --max-samples 100 --cookies data/www.youtube.com_cookies.txt
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å‘½ä»¤')
    
    # download å‘½ä»¤
    dl_parser = subparsers.add_parser('download', help='ä¸‹è½½ YouTube è§†é¢‘')
    dl_parser.add_argument("--output-dir", "-o", type=Path, default=DEFAULT_RAW_DIR, help="è¾“å‡ºç›®å½•")
    dl_parser.add_argument("--dataset", "-d", choices=["test", "train"], default="test", help="æ•°æ®é›†ç±»å‹")
    dl_parser.add_argument("--max-samples", "-n", type=int, default=None, help="æœ€å¤§æ ·æœ¬æ•°ï¼Œé»˜è®¤å…¨éƒ¨")
    dl_parser.add_argument("--cookies", "-c", type=str, default=None, help="cookies.txt æ–‡ä»¶è·¯å¾„")
    dl_parser.add_argument("--no-resume", action="store_true", help="ä¸ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œé‡æ–°å¼€å§‹")
    dl_parser.add_argument("--resolution", "-r", type=int, default=DEFAULT_RESOLUTION, 
                          choices=[360, 480, 720, 1080], help="ä¸‹è½½åˆ†è¾¨ç‡ (é»˜è®¤: 360)")
    dl_parser.add_argument("--fps", type=int, default=None, help="æœ€å¤§å¸§ç‡ï¼Œé»˜è®¤ä¸é™åˆ¶")
    dl_parser.add_argument("--keep-original", action="store_true", 
                          help="ä¿ç•™åŸå§‹å®Œæ•´è§†é¢‘ï¼ˆé»˜è®¤ï¼šåˆ‡ç‰‡ååˆ é™¤ï¼‰")
    
    # preprocess å‘½ä»¤
    pp_parser = subparsers.add_parser('preprocess', help='é¢„å¤„ç†ä¸ºè®­ç»ƒæ ¼å¼')
    pp_parser.add_argument("--samples-json", type=Path, default=DEFAULT_RAW_DIR / "samples.json")
    pp_parser.add_argument("--output-dir", type=Path, default=DEFAULT_TRAINING_DIR)
    
    # all å‘½ä»¤
    all_parser = subparsers.add_parser('all', help='ä¸‹è½½å¹¶é¢„å¤„ç†')
    all_parser.add_argument("--raw-dir", type=Path, default=DEFAULT_RAW_DIR, help="åŸå§‹è§†é¢‘ç›®å½•")
    all_parser.add_argument("--training-dir", type=Path, default=DEFAULT_TRAINING_DIR, help="è®­ç»ƒæ•°æ®ç›®å½•")
    all_parser.add_argument("--dataset", "-d", choices=["test", "train"], default="test")
    all_parser.add_argument("--max-samples", "-n", type=int, default=None)
    all_parser.add_argument("--cookies", "-c", type=str, default=None)
    all_parser.add_argument("--no-resume", action="store_true", help="ä¸ä»æ£€æŸ¥ç‚¹æ¢å¤")
    all_parser.add_argument("--resolution", "-r", type=int, default=DEFAULT_RESOLUTION,
                          choices=[360, 480, 720, 1080], help="ä¸‹è½½åˆ†è¾¨ç‡ (é»˜è®¤: 360)")
    all_parser.add_argument("--fps", type=int, default=None, help="æœ€å¤§å¸§ç‡")
    all_parser.add_argument("--keep-original", action="store_true", help="ä¿ç•™åŸå§‹å®Œæ•´è§†é¢‘")
    
    args = parser.parse_args()
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    if args.command == 'download':
        download_videos(
            output_dir=args.output_dir,
            dataset_type=args.dataset,
            max_samples=args.max_samples,
            cookies_file=args.cookies,
            resume=not getattr(args, 'no_resume', False),
            resolution=args.resolution,
            fps=args.fps,
            delete_original=not getattr(args, 'keep_original', False),
        )
    
    elif args.command == 'preprocess':
        preprocess_samples(
            samples_json=args.samples_json,
            output_dir=args.output_dir,
        )
    
    elif args.command == 'all':
        logger.info("=" * 60)
        logger.info("æ­¥éª¤ 1: ä¸‹è½½è§†é¢‘")
        logger.info("=" * 60)
        download_videos(
            output_dir=args.raw_dir,
            dataset_type=args.dataset,
            max_samples=args.max_samples,
            cookies_file=args.cookies,
            resume=not getattr(args, 'no_resume', False),
            resolution=args.resolution,
            fps=args.fps,
            delete_original=not getattr(args, 'keep_original', False),
        )
        
        logger.info("\n" + "=" * 60)
        logger.info("æ­¥éª¤ 2: é¢„å¤„ç†ä¸ºè®­ç»ƒæ ¼å¼")
        logger.info("=" * 60)
        preprocess_samples(
            samples_json=args.raw_dir / "samples.json",
            output_dir=args.training_dir,
        )
        
        logger.info("\n" + "=" * 60)
        logger.info("å…¨éƒ¨å®Œæˆï¼")
        logger.info("=" * 60)
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()

