#!/usr/bin/env python3
"""
Decord Video Processing Pipeline - DALI-like Architecture
"""

import json
import pickle
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Iterator, Any
from dataclasses import dataclass
from collections import OrderedDict
import time
from multiprocessing import Pool
from functools import partial

import numpy as np
import decord
from decord import VideoReader, cpu, gpu
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(processName)s] - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ============================================================
# Data Structures
# ============================================================

@dataclass
class VideoSegment:
    """è§†é¢‘ç‰‡æ®µå…ƒæ•°æ®"""
    vid: str                    # è§†é¢‘ ID
    video_path: Path            # è§†é¢‘æ–‡ä»¶è·¯å¾„
    start_time: float           # å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
    end_time: float             # ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    transcript: str             # æ–‡æœ¬æè¿°
    size: List[float] = None    # ç›®æ ‡ç‰©ä½“å¤§å°ï¼ˆ16 ä¸ªå€¼ï¼‰
    data_type: str = 'train'    # æ•°æ®ç±»å‹ï¼š'train', 'val', 'test'
    text_input_path: str = None # text_input.pkl æ–‡ä»¶è·¯å¾„ï¼ˆé¢„ç”Ÿæˆï¼‰
    
    @property
    def duration(self) -> float:
        """ç‰‡æ®µæ—¶é•¿"""
        return self.end_time - self.start_time
    
    def __repr__(self):
        return f"VideoSegment(vid={self.vid}, {self.start_time:.1f}s-{self.end_time:.1f}s, type={self.data_type})"


@dataclass
class ProcessedSample:
    """å¤„ç†åçš„æ ·æœ¬"""
    index: int
    vid: str
    frames: np.ndarray          # (N, H, W, 3) uint8
    tokens: np.ndarray          # (77,) int64
    size: List[float]           # [16] float
    sample_dir: Path
    success: bool
    data_type: str = 'train'    # æ•°æ®ç±»å‹ï¼š'train', 'val', 'test'
    error_msg: Optional[str] = None
    
    @property
    def num_frames(self) -> int:
        return self.frames.shape[0] if self.frames is not None else 0


# ============================================================
# Pipeline Components
# ============================================================

class VideoDataSource:
    """
    è§†é¢‘æ•°æ®æº (Data Source)
    
    è´Ÿè´£ï¼š
    1. åŠ è½½é¢„å¤„ç†å¥½çš„å…ƒæ•°æ® JSON
    2. ç”Ÿæˆ VideoSegment åˆ—è¡¨
    
    å…ƒæ•°æ®æ ¼å¼ï¼š
    [
        {
            "vid": "abc123",
            "video_path": "/path/to/video.mp4",
            "start_time": 10.5,
            "end_time": 15.3,
            "transcript": "player mining diamond",
            "size": [0.3, 0.4, 0.5, ...],  # 16 ä¸ªå€¼
            "data_type": "train",  # 'train', 'val', 'test'
            "text_input_path": "/path/to/abc123_text_input.pkl"  # é¢„ç”Ÿæˆçš„ text_input.pkl
        },
        ...
    ]
    """
    
    def __init__(self, metadata_path: Path):
        """
        Args:
            metadata_path: é¢„å¤„ç†å¥½çš„å…ƒæ•°æ® JSON æ–‡ä»¶
                          åŒ…å« vid, video_path, start_time, end_time, transcript, size
        """
        self.metadata_path = Path(metadata_path)
        self.segments: List[VideoSegment] = []
        
        self._load_data()
    
    def _load_data(self):
        """åŠ è½½å…ƒæ•°æ®å¹¶ç”Ÿæˆ VideoSegment åˆ—è¡¨"""
        with open(self.metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        logger.info(f"åŠ è½½äº† {len(metadata)} æ¡å…ƒæ•°æ®")
        
        # ç›´æ¥ä»å…ƒæ•°æ®åˆ›å»º VideoSegment
        success_count = 0
        for item in metadata:
            vid = item.get('vid', '')
            video_path_str = item.get('video_path', '')
            
            if not vid or not video_path_str:
                logger.warning(f"è·³è¿‡æ— æ•ˆæ¡ç›®: vid={vid}, video_path={video_path_str}")
                continue
            
            video_path = Path(video_path_str)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not video_path.exists():
                logger.warning(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
                continue
            
            segment = VideoSegment(
                vid=vid,
                video_path=video_path,
                start_time=item.get('start_time', 0),
                end_time=item.get('end_time', 0),
                transcript=item.get('transcript', ''),
                size=item.get('size', []),
                data_type=item.get('data_type', 'train'),  # é»˜è®¤ä¸º train
                text_input_path=item.get('text_input_path', None)
            )
            self.segments.append(segment)
            success_count += 1
        
        logger.info(f"åŠ è½½æˆåŠŸ: {success_count}/{len(metadata)} ä¸ªè§†é¢‘ç‰‡æ®µ")
    
    def __len__(self) -> int:
        return len(self.segments)
    
    def __getitem__(self, index: int) -> VideoSegment:
        return self.segments[index]
    
    def __iter__(self) -> Iterator[VideoSegment]:
        return iter(self.segments)


class DecordProcessor:
    """
    Decord è§†é¢‘å¤„ç†å™¨ (Processor)
    
    è´Ÿè´£ï¼š
    1. ä½¿ç”¨ Decord æå–è§†é¢‘å¸§
    2. Resize åˆ°ç›®æ ‡å°ºå¯¸
    3. æ–‡æœ¬ Tokenization
    """
    
    def __init__(
        self,
        frame_height: int = 160,
        frame_width: int = 256,
        device_id: int = 0,
        use_gpu: bool = True
    ):
        """
        Args:
            frame_height: ç›®æ ‡å¸§é«˜åº¦
            frame_width: ç›®æ ‡å¸§å®½åº¦
            device_id: GPU ID
            use_gpu: æ˜¯å¦ä½¿ç”¨ GPU
        """
        self.frame_height = frame_height
        self.frame_width = frame_width
        self.device_id = device_id
        self.use_gpu = use_gpu
        
        self.ctx = gpu(device_id) if use_gpu else cpu(0)
        
        logger.info(f"DecordProcessor åˆå§‹åŒ–: {'GPU' if use_gpu else 'CPU'} (device={device_id})")
    
    def extract_frames(self, segment: VideoSegment) -> Optional[np.ndarray]:
        """
        æå–è§†é¢‘å¸§ (æ ¸å¿ƒæ–¹æ³•)
        
        Args:
            segment: è§†é¢‘ç‰‡æ®µä¿¡æ¯
        
        Returns:
            frames: (N, H, W, 3) uint8, RGB æ ¼å¼
        """
        try:
            # åˆ›å»º VideoReader (Decord æ”¯æŒåˆå§‹åŒ–æ—¶æŒ‡å®š resize)
            vr = VideoReader(
                str(segment.video_path),
                ctx=self.ctx,
                width=self.frame_width,
                height=self.frame_height
            )
            
            # è·å–è§†é¢‘ä¿¡æ¯
            fps = vr.get_avg_fps()
            total_frames = len(vr)
            
            if total_frames == 0:
                return None
            
            # è®¡ç®—å¸§èŒƒå›´
            start_frame = int(segment.start_time * fps)
            end_frame = int(segment.end_time * fps)
            
            # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
            start_frame = max(0, min(start_frame, total_frames - 1))
            end_frame = max(start_frame + 1, min(end_frame, total_frames))
            
            # æ‰¹é‡è¯»å–å¸§ï¼ˆDecord çš„æ ¸å¿ƒä¼˜åŠ¿ï¼‰
            frame_indices = list(range(start_frame, end_frame))
            frames = vr.get_batch(frame_indices).asnumpy()
            
            # Decord è¾“å‡º: (N, H, W, 3) RGB uint8
            return frames.astype(np.uint8)
        
        except Exception as e:
            logger.error(f"{segment.vid}: Decord è§£ç å¤±è´¥ - {str(e)}")
            return None
    
    def process_segment(self, index: int, segment: VideoSegment) -> ProcessedSample:
        """
        å¤„ç†å•ä¸ªè§†é¢‘ç‰‡æ®µ
        
        Returns:
            ProcessedSample: åŒ…å«å¤„ç†ç»“æœçš„æ•°æ®ç»“æ„
        """
        try:
            # 1. æå–å¸§
            frames = self.extract_frames(segment)
            if frames is None or len(frames) == 0:
                return ProcessedSample(
                    index=index,
                    vid=segment.vid,
                    frames=None,
                    tokens=None,
                    size=None,
                    sample_dir=None,
                    success=False,
                    data_type=segment.data_type,
                    error_msg="å¸§æå–å¤±è´¥"
                )
            
            # 2. æ£€æŸ¥ text_input_path
            if not segment.text_input_path:
                return ProcessedSample(
                    index=index,
                    vid=segment.vid,
                    frames=None,
                    tokens=None,
                    size=None,
                    sample_dir=None,
                    success=False,
                    data_type=segment.data_type,
                    error_msg="å…ƒæ•°æ®ç¼ºå°‘ text_input_path"
                )
            
            text_input_path = Path(segment.text_input_path)
            if not text_input_path.exists():
                return ProcessedSample(
                    index=index,
                    vid=segment.vid,
                    frames=None,
                    tokens=None,
                    size=None,
                    sample_dir=None,
                    success=False,
                    data_type=segment.data_type,
                    error_msg=f"text_input.pkl æ–‡ä»¶ä¸å­˜åœ¨: {text_input_path}"
                )
            
            # 3. å¤„ç† size æ•°æ®
            # æ³¨æ„ï¼šå®˜æ–¹ CLIP4MC çš„ size æ•°ç»„å›ºå®šä¸º 16 ä¸ªå€¼
            # è¿™äº›å€¼å¯¹åº” DataLoader åŠ¨æ€é‡‡æ ·çš„ 16 å¸§ï¼Œè€Œéå®é™…æå–çš„æ‰€æœ‰å¸§
            # å› æ­¤ç›´æ¥ä½¿ç”¨å…ƒæ•°æ®ä¸­çš„ sizeï¼Œä¸åšä»»ä½•å¤„ç†
            size_values = segment.size if segment.size else []
            
            if not size_values or len(size_values) == 0:
                # å…ƒæ•°æ®æœªæä¾› sizeï¼Œä½¿ç”¨å ä½ç¬¦ï¼ˆ16 ä¸ªå€¼ï¼‰
                size_values = [0] * 16
            
            return ProcessedSample(
                index=index,
                vid=segment.vid,
                frames=frames,
                tokens=text_input_path,  # å­˜å‚¨ text_input.pkl è·¯å¾„ï¼Œç¨åæ‹·è´
                size=size_values,
                sample_dir=None,  # ç¨åç”± Saver å¡«å……
                success=True,
                data_type=segment.data_type
            )
        
        except Exception as e:
            return ProcessedSample(
                index=index,
                vid=segment.vid,
                frames=None,
                tokens=None,
                size=None,
                sample_dir=None,
                success=False,
                data_type=segment.data_type,
                error_msg=f"å¤„ç†å¼‚å¸¸: {str(e)}"
            )


class SampleSaver:
    """
    æ ·æœ¬ä¿å­˜å™¨ (Saver)
    
    è´Ÿè´£ï¼š
    1. ä¿å­˜ video_input.pkl
    2. æ‹·è´é¢„ç”Ÿæˆçš„ text_input.pkl
    3. ä¿å­˜ size.json
    """
    
    def __init__(self, output_dir: Path):
        """
        Args:
            output_dir: è¾“å‡ºç›®å½•
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def save_sample(self, sample: ProcessedSample) -> bool:
        """
        ä¿å­˜å•ä¸ªæ ·æœ¬
        
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not sample.success:
            return False
        
        try:
            import shutil
            
            # åˆ›å»ºæ ·æœ¬ç›®å½•
            sample_dir = self.output_dir / f"sample_{sample.index:06d}_{sample.vid}"
            sample_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ video_input.pkl
            with open(sample_dir / "video_input.pkl", "wb") as f:
                pickle.dump(sample.frames, f, protocol=pickle.HIGHEST_PROTOCOL)
            
            # æ‹·è´é¢„ç”Ÿæˆçš„ text_input.pkl
            # sample.tokens ç°åœ¨å­˜å‚¨çš„æ˜¯ text_input.pkl çš„è·¯å¾„
            text_input_src = sample.tokens  # è¿™æ˜¯ä¸€ä¸ª Path å¯¹è±¡
            if isinstance(text_input_src, (str, Path)):
                text_input_src = Path(text_input_src)
                if text_input_src.exists():
                    shutil.copy2(text_input_src, sample_dir / "text_input.pkl")
                else:
                    logger.error(f"text_input.pkl æºæ–‡ä»¶ä¸å­˜åœ¨: {text_input_src}")
                    return False
            else:
                logger.error(f"text_input.pkl è·¯å¾„æ ¼å¼é”™è¯¯: {type(text_input_src)}")
                return False
            
            # ä¿å­˜ size.json
            with open(sample_dir / "size.json", "w") as f:
                json.dump(sample.size, f)
            
            # æ›´æ–° sample_dir
            sample.sample_dir = sample_dir
            
            return True
        
        except Exception as e:
            logger.error(f"ä¿å­˜å¤±è´¥ {sample.vid}: {str(e)}")
            sample.success = False
            sample.error_msg = f"ä¿å­˜å¤±è´¥: {str(e)}"
            return False


def _process_single_segment_worker(
    args: Tuple[int, VideoSegment, Path, Tuple[int, int], int, bool]
) -> Dict[str, Any]:
    """
    å¤šè¿›ç¨‹ worker å‡½æ•°ï¼šå¤„ç†å•ä¸ªè§†é¢‘ç‰‡æ®µ
    
    Args:
        args: (index, segment, output_dir, frame_size, device_id, use_gpu)
    
    Returns:
        dict: å¤„ç†ç»“æœ
    """
    index, segment, output_dir, frame_size, device_id, use_gpu = args
    
    # æ¯ä¸ª worker åˆå§‹åŒ–è‡ªå·±çš„ processor å’Œ saver
    processor = DecordProcessor(
        frame_height=frame_size[0],
        frame_width=frame_size[1],
        device_id=device_id,
        use_gpu=use_gpu
    )
    
    saver = SampleSaver(output_dir=output_dir)
    
    # å¤„ç†
    sample = processor.process_segment(index, segment)
    
    # ä¿å­˜
    if sample.success:
        success = saver.save_sample(sample)
        if not success:
            sample.success = False
    
    # è¿”å›ç»“æœ
    return {
        'index': index,
        'vid': segment.vid,
        'success': sample.success,
        'error_msg': sample.error_msg,
        'sample_dir': str(sample.sample_dir) if sample.sample_dir else None,
        'data_type': segment.data_type
    }


# ============================================================
# Pipeline & Iterator
# ============================================================

class DecordPipeline:
    """
    Decord è§†é¢‘å¤„ç† Pipeline (ç±»ä¼¼ DALI Pipeline)
    
    æ¶æ„ï¼š
        VideoDataSource -> DecordProcessor -> SampleSaver -> Iterator
    
    ç‰¹ç‚¹ï¼š
        - Pipeline æ¨¡å¼è®¾è®¡
        - æ”¯æŒæ‰¹é‡è¿­ä»£
        - è¿›åº¦è·Ÿè¸ª
        - ç»Ÿè®¡ä¿¡æ¯
    """
    
    def __init__(
        self,
        metadata_path: Path,
        output_dir: Path,
        batch_size: int = 1,
        frame_size: Tuple[int, int] = (160, 256),
        device_id: int = 0,
        use_gpu: bool = True,
        num_workers: int = 1,
        show_progress: bool = True
    ):
        """
        Args:
            metadata_path: é¢„å¤„ç†å¥½çš„å…ƒæ•°æ® JSONï¼ˆåŒ…å« vid, video_path, start_time, end_time, size, data_type, text_input_pathï¼‰
            output_dir: è¾“å‡ºç›®å½•
            batch_size: æ‰¹é‡å¤§å°ï¼ˆç”¨äºè¿›åº¦æ˜¾ç¤ºï¼Œå®é™…ä»æ˜¯é€ä¸ªå¤„ç†ï¼‰
            frame_size: (height, width) ç›®æ ‡å¸§å°ºå¯¸
            device_id: GPU ID
            use_gpu: æ˜¯å¦ä½¿ç”¨ GPU
            num_workers: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤: 1ï¼Œå•çº¿ç¨‹ï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        """
        self.batch_size = batch_size
        self.show_progress = show_progress
        self.num_workers = num_workers
        self.frame_size = frame_size
        self.device_id = device_id
        self.use_gpu = use_gpu
        
        # 1. åˆå§‹åŒ–ç»„ä»¶
        logger.info("=" * 60)
        logger.info("åˆå§‹åŒ– Decord Pipeline")
        logger.info("=" * 60)
        
        self.data_source = VideoDataSource(metadata_path=metadata_path)
        
        # å•è¿›ç¨‹æ¨¡å¼ï¼šåˆå§‹åŒ– processor
        if num_workers == 1:
            self.processor = DecordProcessor(
                frame_height=frame_size[0],
                frame_width=frame_size[1],
                device_id=device_id,
                use_gpu=use_gpu
            )
        else:
            # å¤šè¿›ç¨‹æ¨¡å¼ï¼šä¸åœ¨ä¸»è¿›ç¨‹åˆå§‹åŒ– processorï¼ˆæ¯ä¸ªå­è¿›ç¨‹ä¼šåˆå§‹åŒ–è‡ªå·±çš„ï¼‰
            self.processor = None
        
        self.saver = SampleSaver(output_dir=output_dir)
        
        # 2. ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total': len(self.data_source),
            'processed': 0,
            'success': 0,
            'failed': 0,
            'failed_samples': [],
            'successful_samples': [],  # å­˜å‚¨æˆåŠŸçš„æ ·æœ¬ä¿¡æ¯ï¼ˆåŒ…å« data_typeï¼‰
            'start_time': None,
            'end_time': None
        }
        
        logger.info(f"å¾…å¤„ç†: {self.stats['total']} ä¸ªè§†é¢‘ç‰‡æ®µ")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"è®¾å¤‡: {'GPU' if use_gpu else 'CPU'} (ID={device_id})")
        logger.info(f"å¸§å°ºå¯¸: {frame_size[0]}x{frame_size[1]}")
        logger.info(f"å¹¶è¡Œè¿›ç¨‹: {num_workers}")
        logger.info("=" * 60)
    
    def __len__(self) -> int:
        """Pipeline ä¸­çš„æ ·æœ¬æ€»æ•°"""
        return len(self.data_source)
    
    def __iter__(self) -> Iterator[Dict[str, Any]]:
        """
        è¿­ä»£å¤„ç†å™¨ï¼ˆæ”¯æŒå¤šè¿›ç¨‹ï¼‰
        
        Yields:
            batch_result: åŒ…å«æ‰¹æ¬¡å¤„ç†ç»“æœçš„å­—å…¸
        """
        self.stats['start_time'] = time.time()
        
        if self.num_workers == 1:
            # å•è¿›ç¨‹æ¨¡å¼ï¼šåŸæœ‰é€»è¾‘
            yield from self._iter_single_process()
        else:
            # å¤šè¿›ç¨‹æ¨¡å¼ï¼šä½¿ç”¨ Pool
            yield from self._iter_multi_process()
        
        self.stats['end_time'] = time.time()
    
    def _iter_single_process(self) -> Iterator[Dict[str, Any]]:
        """å•è¿›ç¨‹è¿­ä»£å™¨"""
        # åˆ›å»ºè¿›åº¦æ¡
        if self.show_progress:
            pbar = tqdm(
                total=len(self.data_source),
                desc="ğŸ¬ Decord Pipeline (å•è¿›ç¨‹)",
                unit="video"
            )
        else:
            pbar = None
        
        # é€ä¸ªå¤„ç†
        for index, segment in enumerate(self.data_source):
            # 1. å¤„ç†
            sample = self.processor.process_segment(index, segment)
            
            # 2. ä¿å­˜
            if sample.success:
                save_success = self.saver.save_sample(sample)
                if save_success:
                    self.stats['success'] += 1
                    self.stats['successful_samples'].append({
                        'sample_dir': str(sample.sample_dir),
                        'data_type': sample.data_type,
                        'vid': sample.vid
                    })
                else:
                    self.stats['failed'] += 1
                    self.stats['failed_samples'].append({
                        'vid': sample.vid,
                        'error': sample.error_msg
                    })
            else:
                self.stats['failed'] += 1
                self.stats['failed_samples'].append({
                    'vid': sample.vid,
                    'error': sample.error_msg
                })
                if self.stats['failed'] <= 10:
                    logger.warning(f"å¤„ç†å¤±è´¥: {sample.vid} - {sample.error_msg}")
            
            self.stats['processed'] += 1
            
            # 3. æ›´æ–°è¿›åº¦
            if pbar:
                pbar.update(1)
                pbar.set_postfix({
                    'success': self.stats['success'],
                    'failed': self.stats['failed']
                })
            
            # 4. Yield æ‰¹æ¬¡ç»“æœ
            yield {
                'index': index,
                'vid': segment.vid,
                'success': sample.success,
                'num_frames': sample.frames.shape[0] if sample.success and sample.frames is not None else 0,
                'sample_dir': str(sample.sample_dir) if sample.sample_dir else None,
                'error_msg': sample.error_msg,
                'batch_size': 1,
                'num_success': self.stats['success'],
                'num_failed': self.stats['failed']
            }
        
        if pbar:
            pbar.close()
    
    def _iter_multi_process(self) -> Iterator[Dict[str, Any]]:
        """å¤šè¿›ç¨‹è¿­ä»£å™¨"""
        # åˆ›å»ºè¿›åº¦æ¡
        if self.show_progress:
            pbar = tqdm(
                total=len(self.data_source),
                desc=f"ğŸ¬ Decord Pipeline ({self.num_workers}è¿›ç¨‹)",
                unit="video"
            )
        else:
            pbar = None
        
        # å‡†å¤‡å‚æ•°åˆ—è¡¨
        args_list = [
            (
                index,
                segment,
                self.saver.output_dir,
                self.frame_size,
                self.device_id,
                self.use_gpu
            )
            for index, segment in enumerate(self.data_source)
        ]
        
        # ä½¿ç”¨ Pool.imap è¿›è¡Œå¹¶è¡Œå¤„ç†ï¼ˆä¿æŒé¡ºåºï¼‰
        with Pool(processes=self.num_workers) as pool:
            for result in pool.imap(_process_single_segment_worker, args_list):
                # ç»Ÿè®¡
                if result['success']:
                    self.stats['success'] += 1
                    self.stats['successful_samples'].append({
                        'sample_dir': result['sample_dir'],
                        'data_type': result['data_type'],
                        'vid': result['vid']
                    })
                else:
                    self.stats['failed'] += 1
                    self.stats['failed_samples'].append({
                        'vid': result['vid'],
                        'error': result['error_msg']
                    })
                    if self.stats['failed'] <= 10:
                        logger.warning(f"å¤„ç†å¤±è´¥: {result['vid']} - {result['error_msg']}")
                
                self.stats['processed'] += 1
                
                # æ›´æ–°è¿›åº¦
                if pbar:
                    pbar.update(1)
                    pbar.set_postfix({
                        'success': self.stats['success'],
                        'failed': self.stats['failed']
                    })
                
                # Yield ç»“æœï¼ˆæ·»åŠ é¢å¤–å­—æ®µä¿æŒå…¼å®¹ï¼‰
                result['batch_size'] = 1
                result['num_success'] = self.stats['success']
                result['num_failed'] = self.stats['failed']
                yield result
        
        if pbar:
            pbar.close()
    
    def run(self) -> Dict[str, Any]:
        """
        è¿è¡Œæ•´ä¸ª pipelineï¼ˆä¾¿æ·æ–¹æ³•ï¼‰
        
        Returns:
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        for _ in self:
            pass  # è¿­ä»£å®Œæˆ
        
        return self.get_stats()
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        
        if stats['start_time'] and stats['end_time']:
            elapsed = stats['end_time'] - stats['start_time']
            stats['elapsed_time'] = elapsed
            stats['speed'] = stats['success'] / elapsed if elapsed > 0 else 0
        
        return stats
    
    def summary(self):
        """æ‰“å°æ‘˜è¦"""
        stats = self.get_stats()
        
        logger.info("\n" + "=" * 60)
        logger.info("Pipeline å¤„ç†å®Œæˆ")
        logger.info("=" * 60)
        logger.info(f"æ€»æ•°: {stats['total']}")
        logger.info(f"æˆåŠŸ: {stats['success']}")
        logger.info(f"å¤±è´¥: {stats['failed']}")
        
        if 'elapsed_time' in stats:
            logger.info(f"è€—æ—¶: {stats['elapsed_time']/60:.2f} åˆ†é’Ÿ")
            logger.info(f"é€Ÿåº¦: {stats['speed']:.2f} è§†é¢‘/ç§’")
        
        # ä¿å­˜å¤±è´¥åˆ—è¡¨
        if stats['failed_samples']:
            failed_json = self.saver.output_dir / "failed_samples.json"
            with open(failed_json, 'w', encoding='utf-8') as f:
                json.dump(stats['failed_samples'], f, indent=2, ensure_ascii=False)
            logger.info(f"å¤±è´¥åˆ—è¡¨: {failed_json}")
        
        # ç”Ÿæˆ dataset_info.json
        self._generate_dataset_info(stats['successful_samples'])
        
        logger.info("=" * 60)
    
    def _generate_dataset_info(self, successful_samples: List[Dict[str, str]]):
        """
        ç”Ÿæˆ dataset_info.json
        
        æ ¹æ®å…ƒæ•°æ®ä¸­çš„ data_type å­—æ®µåˆ†é…æ ·æœ¬åˆ° train/val/test
        
        Args:
            successful_samples: æˆåŠŸå¤„ç†çš„æ ·æœ¬åˆ—è¡¨
                [
                    {
                        'sample_dir': '/path/to/sample_000000_xxx',
                        'data_type': 'train',
                        'vid': 'xxx'
                    },
                    ...
                ]
        """
        dataset_info_path = self.saver.output_dir / "dataset_info.json"
        
        # æ ¹æ® data_type åˆ†ç»„
        dataset_info = {
            "train": [],
            "val": [],
            "test": []
        }
        
        for sample in successful_samples:
            sample_dir = sample['sample_dir']
            data_type = sample.get('data_type', 'train')
            
            # ç¡®ä¿ data_type æœ‰æ•ˆ
            if data_type not in ['train', 'val', 'test']:
                logger.warning(f"æ— æ•ˆçš„ data_type: {data_type}ï¼Œé»˜è®¤ä¸º train")
                data_type = 'train'
            
            dataset_info[data_type].append(sample_dir)
        
        # ä¿å­˜
        with open(dataset_info_path, "w") as f:
            json.dump(dataset_info, f, indent=2)
        
        # æ‰“å°ç»Ÿè®¡
        logger.info(f"dataset_info.json ç”ŸæˆæˆåŠŸ:")
        logger.info(f"  train: {len(dataset_info['train'])} ä¸ªæ ·æœ¬")
        logger.info(f"  val:   {len(dataset_info['val'])} ä¸ªæ ·æœ¬")
        logger.info(f"  test:  {len(dataset_info['test'])} ä¸ªæ ·æœ¬")


# ============================================================
# Command Line Interface
# ============================================================

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Decord Video Processing Pipeline (DALI-like Architecture)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
    # åŸºæœ¬ä½¿ç”¨
    python src/utils/generate_clip4mc_training_datas.py \\
        --metadata preprocessed_metadata.json \\
        --output-dir /path/to/output

    # GPU åŠ é€Ÿ
    python src/utils/decord_pipeline.py \\
        --metadata preprocessed_metadata.json \\
        --output-dir /path/to/output \\
        --use-gpu \\
        --device-id 0

    # å¤šè¿›ç¨‹åŠ é€Ÿï¼ˆ10 ä¸ªè¿›ç¨‹ï¼‰
    python src/utils/generate_clip4mc_training_datas.py \\
        --metadata preprocessed_metadata.json \\
        --output-dir /path/to/output \\
        --num-workers 10

    # è‡ªå®šä¹‰å‚æ•°
    python src/utils/generate_clip4mc_training_datas.py \\
        --metadata preprocessed_metadata.json \\
        --output-dir /path/to/output \\
        --use-gpu \\
        --device-id 0 \\
        --num-workers 4 \\
        --frame-height 160 \\
        --frame-width 256

å…ƒæ•°æ® JSON æ ¼å¼:
    [
        {
            "vid": "abc123",
            "video_path": "/path/to/video.mp4",
            "start_time": 10.5,
            "end_time": 15.3,
            "transcript": "player mining diamond",
            "size": [0.3, 0.4, 0.5, ...],  # 16 ä¸ªå€¼
            "data_type": "train",  # 'train', 'val', 'test'
            "text_input_path": "/path/to/abc123_text_input.pkl"  # é¢„ç”Ÿæˆçš„ text_input.pkl
        },
        ...
    ]
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--metadata", type=Path, required=True,
                       help="é¢„å¤„ç†å¥½çš„å…ƒæ•°æ® JSON æ–‡ä»¶ï¼ˆåŒ…å« vid, video_path, start_time, end_time, transcript, sizeï¼‰")
    parser.add_argument("--output-dir", type=Path, required=True,
                       help="è¾“å‡ºç›®å½•")
    
    # å¯é€‰å‚æ•°
    parser.add_argument("--frame-height", type=int, default=160,
                       help="ç›®æ ‡å¸§é«˜åº¦ (é»˜è®¤: 160)")
    parser.add_argument("--frame-width", type=int, default=256,
                       help="ç›®æ ‡å¸§å®½åº¦ (é»˜è®¤: 256)")
    parser.add_argument("--device-id", type=int, default=0,
                       help="GPU ID (é»˜è®¤: 0)")
    parser.add_argument("--use-gpu", action='store_true',
                       help="ä½¿ç”¨ GPU åŠ é€Ÿ")
    parser.add_argument("--num-workers", type=int, default=1,
                       help="å¹¶è¡Œè¿›ç¨‹æ•° (é»˜è®¤: 1ï¼Œå•çº¿ç¨‹)")
    parser.add_argument("--batch-size", type=int, default=1,
                       help="æ‰¹é‡å¤§å°ï¼ˆæš‚ä»…ç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰")
    parser.add_argument("--no-progress", action='store_true',
                       help="ç¦ç”¨è¿›åº¦æ¡")
    
    args = parser.parse_args()
    
    # åˆ›å»º pipeline
    pipeline = DecordPipeline(
        metadata_path=args.metadata,
        output_dir=args.output_dir,
        batch_size=args.batch_size,
        frame_size=(args.frame_height, args.frame_width),
        device_id=args.device_id,
        use_gpu=args.use_gpu,
        num_workers=args.num_workers,
        show_progress=not args.no_progress
    )
    
    # è¿è¡Œ pipeline
    pipeline.run()
    
    # æ‰“å°æ‘˜è¦
    pipeline.summary()
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())

