#!/usr/bin/env python3
"""
ç”Ÿæˆ Decord Pipeline æ‰€éœ€çš„å…ƒæ•°æ®æ–‡ä»¶

åŠŸèƒ½ï¼š
1. è¯»å– dataset_test.json å’Œ dataset_train_LocalCorrelationFilter.json
2. è¯»å– youtube_download_log.csvï¼ˆvid åˆ°æ–‡ä»¶åæ˜ å°„ï¼‰
3. åŒ¹é…è§†é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒç½‘ç›˜æ–‡ä»¶åå…¼å®¹ï¼‰
4. ç”Ÿæˆ text_input.pklï¼ˆä½¿ç”¨ transformers AutoTokenizerï¼Œä¸ CLIP4MC å®˜æ–¹ä¸€è‡´ï¼‰
5. è¾“å‡ºå…ƒæ•°æ® JSON æ–‡ä»¶

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # å•è¿›ç¨‹æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
    python src/utils/generate_clip4mc_metadata.py \\
        --test-json data/training/dataset_test.json \\
        --download-log data/training/youtube_download_log.csv \\
        --videos-dir /path/to/videos \\
        --text-inputs-dir /path/to/text_inputs \\
        --output metadata.json \\
        --loose-match
    
    # å¤šè¿›ç¨‹æ¨¡å¼ï¼ˆæ¨èï¼Œå¤„ç†å¤§é‡æ•°æ®æ—¶ï¼‰
    python src/utils/generate_clip4mc_metadata.py \\
        --test-json data/training/dataset_test.json \\
        --train-json data/training/dataset_train_LocalCorrelationFilter.json \\
        --download-log data/training/youtube_download_log.csv \\
        --videos-dir /path/to/videos \\
        --text-inputs-dir /path/to/text_inputs \\
        --output metadata.json \\
        --loose-match \\
        --num-workers 8 \\
        --unmatched-output unmatched.json
    
    # æ€§èƒ½æµ‹è¯•æ¨¡å¼ï¼ˆè·³è¿‡ text token ç”Ÿæˆï¼‰
    python src/utils/generate_clip4mc_metadata.py \\
        --test-json data/training/dataset_test.json \\
        --download-log data/training/youtube_download_log.csv \\
        --videos-dir /path/to/videos \\
        --text-inputs-dir /tmp/text_inputs \\
        --output /tmp/metadata.json \\
        --loose-match \\
        --num-workers 16 \\
        --skip-text-generation

ä¾èµ–ï¼š
    - transformers: pip install transformers
"""

import json
import csv
import pickle
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from urllib.parse import urlparse, parse_qs
import argparse
from multiprocessing import Pool
from tqdm import tqdm

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


# ============================================================
# æ•°æ®ç»“æ„
# ============================================================

@dataclass
class VideoClip:
    """è§†é¢‘ç‰‡æ®µå…ƒæ•°æ®"""
    vid: str
    transcript: str
    start_time: float
    end_time: float
    size: List[float]
    data_type: str  # 'train', 'test'


# ============================================================
# å…¨å±€å˜é‡ï¼ˆç”¨äºè¿›ç¨‹æ± åˆå§‹åŒ–ï¼‰
# ============================================================
_worker_vid_to_title = None
_worker_video_files = None
_worker_text_inputs_dir = None
_worker_use_loose_match = None
_worker_video_prefix = None
_worker_text_prefix = None
_worker_skip_text_generation = None
_worker_video_index = None  # é¢„è®¡ç®—çš„è§†é¢‘æ–‡ä»¶ç´¢å¼•ï¼ˆæ€§èƒ½ä¼˜åŒ–ï¼‰


def _init_metadata_worker(
    vid_to_title: Dict[str, str],
    video_files: List[Path],
    text_inputs_dir: Path,
    use_loose_match: bool,
    video_prefix: str,
    text_prefix: str,
    skip_text_generation: bool
):
    """
    åˆå§‹åŒ– worker è¿›ç¨‹ï¼ˆæ¯ä¸ªè¿›ç¨‹åªè°ƒç”¨ä¸€æ¬¡ï¼‰
    
    Args:
        vid_to_title: vid åˆ° title çš„æ˜ å°„
        video_files: è§†é¢‘æ–‡ä»¶åˆ—è¡¨
        text_inputs_dir: text_input.pkl è¾“å‡ºç›®å½•
        use_loose_match: æ˜¯å¦ä½¿ç”¨å®½æ¾åŒ¹é…
        video_prefix: è§†é¢‘è·¯å¾„å‰ç¼€
        text_prefix: text_input.pkl è·¯å¾„å‰ç¼€
        skip_text_generation: æ˜¯å¦è·³è¿‡ text token ç”Ÿæˆ
    """
    global _worker_vid_to_title, _worker_video_files, _worker_text_inputs_dir
    global _worker_use_loose_match, _worker_video_prefix, _worker_text_prefix
    global _worker_skip_text_generation, _worker_video_index
    
    _worker_vid_to_title = vid_to_title
    _worker_video_files = video_files
    _worker_text_inputs_dir = text_inputs_dir
    _worker_use_loose_match = use_loose_match
    _worker_video_prefix = video_prefix
    _worker_text_prefix = text_prefix
    _worker_skip_text_generation = skip_text_generation
    
    # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šé¢„è®¡ç®—è§†é¢‘æ–‡ä»¶ç´¢å¼•ï¼ˆO(n) â†’ O(1) æŸ¥æ‰¾ï¼‰
    # è¿™å°†æŸ¥æ‰¾ä»çº¿æ€§éå†ï¼ˆ2092æ¬¡ï¼‰å˜ä¸ºå­—å…¸æŸ¥æ‰¾ï¼ˆ1æ¬¡ï¼‰
    _worker_video_index = _build_video_index(video_files, use_loose_match)


def _build_video_index(video_files: List[Path], use_loose_match: bool) -> Dict:
    """
    æ„å»ºè§†é¢‘æ–‡ä»¶ç´¢å¼•ï¼ˆé¢„è®¡ç®—æ‰€æœ‰ normalized ç‰ˆæœ¬ï¼‰
    
    è¿™æ˜¯æ€§èƒ½ä¼˜åŒ–çš„å…³é”®ï¼š
    - å°† O(n) çº¿æ€§æŸ¥æ‰¾ä¼˜åŒ–ä¸º O(1) å­—å…¸æŸ¥æ‰¾
    - é¿å…é‡å¤è®¡ç®— normalized ç‰ˆæœ¬
    - åœ¨ worker åˆå§‹åŒ–æ—¶åªè®¡ç®—ä¸€æ¬¡
    
    é‡è¦ï¼šå¯¹æ‰€æœ‰æ–‡ä»¶åå…ˆåº”ç”¨ normalize_netdisk_filenameï¼Œ
    ä»¥å¤„ç†äº‘å­˜å‚¨/ç½‘ç›˜çš„ç‰¹æ®Šå­—ç¬¦æ›¿æ¢ï¼ˆå…¨è§’ç¬¦å·ã€HTMLå®ä½“ç­‰ï¼‰
    
    Args:
        video_files: è§†é¢‘æ–‡ä»¶åˆ—è¡¨
        use_loose_match: æ˜¯å¦ä½¿ç”¨å®½æ¾åŒ¹é…
    
    Returns:
        ç´¢å¼•å­—å…¸ï¼ŒåŒ…å«å¤šä¸ªåŒ¹é…ç­–ç•¥çš„ç´¢å¼•
    """
    index = {
        'direct': {},           # ç­–ç•¥ 1: ç›´æ¥åŒ¹é…
        'normalized': {},       # ç­–ç•¥ 2: è§„èŒƒåŒ–åŒ¹é…
        'vid_contains': [],     # æœ€åç­–ç•¥: vid åŒ…å«åŒ¹é…ï¼ˆä»éœ€éå†ï¼‰
    }
    
    # å¦‚æœå¯ç”¨å®½æ¾åŒ¹é…ï¼Œæ·»åŠ é¢å¤–ç´¢å¼•
    if use_loose_match:
        index['loose'] = {}           # ç­–ç•¥ 3: å®½æ¾åŒ¹é…
        index['ultra_loose'] = {}     # ç­–ç•¥ 4: è¶…å®½æ¾åŒ¹é…
    
    # é¢„è®¡ç®—æ‰€æœ‰æ–‡ä»¶çš„ normalized ç‰ˆæœ¬ï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰
    for video_file in tqdm(video_files, desc="ğŸ”¨ æ„å»ºè§†é¢‘ç´¢å¼•", unit="file", leave=False):
        stem = video_file.stem
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šå…ˆåº”ç”¨ç½‘ç›˜å­—ç¬¦è§„èŒƒåŒ–
        # å°†å…¨è§’ç¬¦å·ã€HTMLå®ä½“ç­‰è½¬æ¢ä¸ºæ ‡å‡†å­—ç¬¦
        stem_normalized_netdisk = normalize_netdisk_filename(stem)
        
        # ç­–ç•¥ 1: ç›´æ¥åŒ¹é…ç´¢å¼•ï¼ˆä½¿ç”¨è§„èŒƒåŒ–åçš„æ–‡ä»¶åï¼‰
        index['direct'][stem_normalized_netdisk] = video_file
        
        # ç­–ç•¥ 2: è§„èŒƒåŒ–åŒ¹é…ç´¢å¼•
        normalized = normalize_title_for_filename(stem_normalized_netdisk, remove_punctuation=False)
        if normalized not in index['normalized']:  # é¿å…è¦†ç›–ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªåŒ¹é…ï¼‰
            index['normalized'][normalized] = video_file
        
        # å®½æ¾åŒ¹é…ç´¢å¼•ï¼ˆä»…åœ¨å¯ç”¨æ—¶ï¼‰
        if use_loose_match:
            # ç­–ç•¥ 3: å®½æ¾åŒ¹é…ï¼ˆç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼‰
            loose = normalize_title_for_filename(stem_normalized_netdisk, remove_punctuation=True)
            if loose not in index['loose']:
                index['loose'][loose] = video_file
            
            # ç­–ç•¥ 4: è¶…å®½æ¾åŒ¹é…ï¼ˆåªä¿ç•™å­—æ¯å’Œæ•°å­—ï¼‰
            ultra_loose = normalize_for_ultra_loose_match(stem_normalized_netdisk)
            if ultra_loose not in index['ultra_loose']:
                index['ultra_loose'][ultra_loose] = video_file
        
        # vid åŒ…å«åŒ¹é…ï¼ˆéœ€è¦éå†ï¼Œä½¿ç”¨åŸå§‹ stem ä»¥ä¿ç•™ vidï¼‰
        index['vid_contains'].append((video_file, stem))
    
    return index


def _process_clip_worker(clip: VideoClip) -> Tuple[Optional[Dict], Optional[Dict]]:
    """
    Worker å‡½æ•°ï¼šå¤„ç†å•ä¸ª clipï¼ˆä½¿ç”¨å…¨å±€å˜é‡ï¼‰
    
    Args:
        clip: è§†é¢‘ç‰‡æ®µ
    
    Returns:
        (metadata_item, unmatched_item)
    """
    global _worker_vid_to_title, _worker_video_files, _worker_text_inputs_dir
    global _worker_use_loose_match, _worker_video_prefix, _worker_text_prefix
    global _worker_skip_text_generation
    
    return process_single_clip(
        clip,
        _worker_vid_to_title,
        _worker_video_files,
        _worker_text_inputs_dir,
        _worker_use_loose_match,
        _worker_video_prefix,
        _worker_text_prefix,
        _worker_skip_text_generation
    )


# ============================================================
# è¾…åŠ©å‡½æ•°
# ============================================================


def normalize_title_for_filename(title: str, remove_punctuation: bool = False) -> str:
    """
    è§„èŒƒåŒ–æ ‡é¢˜ä»¥åŒ¹é…æ–‡ä»¶å
    
    æ–‡ä»¶ç³»ç»Ÿä¼šç§»é™¤æˆ–æ›¿æ¢æŸäº›å­—ç¬¦ï¼š
    - ç§»é™¤ emoji å’Œå…¶ä»–éASCIIå­—ç¬¦
    - ç‚¹å·å¯èƒ½è¢«ç©ºæ ¼æ›¿æ¢
    - ç½‘ç›˜ç‰¹æ®Šå­—ç¬¦æ›¿æ¢
    
    Args:
        title: æ ‡é¢˜
        remove_punctuation: æ˜¯å¦å°†æ ‡ç‚¹ç¬¦å·æ›¿æ¢ä¸ºç©ºæ ¼ï¼ˆç”¨äºå®½æ¾åŒ¹é…ï¼‰
    """
    import re
    
    # ç§»é™¤ emoji å’Œå…¶ä»–éASCIIå¯æ‰“å°å­—ç¬¦ï¼ˆä¿ç•™å¸¸è§ç¬¦å·ï¼‰
    # åªä¿ç•™ ASCII å¯æ‰“å°å­—ç¬¦ + ç©ºæ ¼
    normalized = ''.join(
        c if (32 <= ord(c) < 127) else ' ' 
        for c in title
    )
    
    # å¦‚æœéœ€è¦ï¼Œå°†æ ‡ç‚¹ç¬¦å·æ›¿æ¢ä¸ºç©ºæ ¼ï¼ˆç”¨äºå®½æ¾åŒ¹é…ï¼‰
    if remove_punctuation:
        # å°†å¸¸è§æ ‡ç‚¹ç¬¦å·æ›¿æ¢ä¸ºç©ºæ ¼
        normalized = re.sub(r'[\.,:;!?\'"()\[\]{}\-_/\\|]', ' ', normalized)
    
    # åˆå¹¶å¤šä¸ªç©ºæ ¼
    normalized = re.sub(r'\s+', ' ', normalized).strip()
    
    return normalized


def normalize_for_ultra_loose_match(text: str) -> str:
    """
    è¶…å®½æ¾åŒ¹é…ï¼šåªä¿ç•™å­—æ¯å’Œæ•°å­—
    
    ç”¨äºå¤„ç†æ–‡ä»¶åä¸­ç§»é™¤äº†æ‰€æœ‰ç‰¹æ®Šå­—ç¬¦çš„æƒ…å†µï¼š
    - CASA *ALEX* â†’ casa alex
    - $1,000 â†’ 1 000
    - EXPÃ‰RIENCE: (NFD) â†’ experience (ç§»é™¤é‡éŸ³ç¬¦å·)
    
    Args:
        text: åŸå§‹æ–‡æœ¬
    
    Returns:
        åªåŒ…å«å­—æ¯ã€æ•°å­—å’Œç©ºæ ¼çš„å°å†™æ–‡æœ¬
    """
    import re
    import unicodedata
    
    # 1. Unicode è§„èŒƒåŒ– (NFD) + ç§»é™¤é‡éŸ³ç¬¦å·
    nfd = unicodedata.normalize('NFD', text)
    without_accents = ''.join(c for c in nfd if unicodedata.category(c) != 'Mn')
    
    # 2. åªä¿ç•™å­—æ¯ã€æ•°å­—å’Œç©ºæ ¼
    normalized = re.sub(r'[^a-zA-Z0-9\s]', ' ', without_accents)
    
    # 3. åˆå¹¶å¤šä¸ªç©ºæ ¼
    normalized = re.sub(r'\s+', ' ', normalized).strip()
    
    return normalized.lower()


def normalize_netdisk_filename(name: str) -> str:
    """
    è§„èŒƒåŒ–ç½‘ç›˜è½¬å­˜åçš„æ–‡ä»¶åï¼ˆåå‘è½¬æ¢ç‰¹æ®Šå­—ç¬¦ï¼‰
    
    ç½‘ç›˜ï¼ˆå¦‚ç™¾åº¦ç½‘ç›˜ï¼‰å’Œäº‘å­˜å‚¨åœ¨è½¬å­˜æ–‡ä»¶æ—¶ä¼šæ›¿æ¢æ–‡ä»¶ç³»ç»Ÿéæ³•å­—ç¬¦ã€‚
    è¿™ä¸ªå‡½æ•°å°†è¿™äº›æ›¿æ¢å­—ç¬¦è½¬å›åŸå§‹å­—ç¬¦ï¼Œä»¥ä¾¿åŒ¹é…ã€‚
    
    æ”¯æŒçš„è½¬æ¢ï¼š
    - å…¨è§’æ ‡ç‚¹ç¬¦å· â†’ åŠè§’
    - ç‰¹æ®ŠUnicodeå­—ç¬¦ â†’ æ ‡å‡†å­—ç¬¦
    - HTMLå®ä½“ â†’ åŸå§‹å­—ç¬¦
    """
    replacements = [
        # ç‰¹æ®ŠUnicodeå­—ç¬¦ï¼ˆä¼˜å…ˆå¤„ç†ï¼‰
        ('â§¸', '/'),    # BIG SOLIDUS (U+29F8) â†’ SOLIDUS
        ('â„', '/'),    # FRACTION SLASH (U+2044) â†’ SOLIDUS
        ('âˆ•', '/'),    # DIVISION SLASH (U+2215) â†’ SOLIDUS
        
        # å…¨è§’æ ‡ç‚¹ç¬¦å· â†’ åŠè§’ï¼ˆæœ€å¸¸è§ï¼‰
        ('ï¼', '!'),   # FULLWIDTH EXCLAMATION MARK
        ('ï¼‚', '"'),   # FULLWIDTH QUOTATION MARK
        ('ï¼ƒ', '#'),   # FULLWIDTH NUMBER SIGN
        ('ï¼„', '$'),   # FULLWIDTH DOLLAR SIGN
        ('ï¼…', '%'),   # FULLWIDTH PERCENT SIGN
        ('ï¼†', '&'),   # FULLWIDTH AMPERSAND
        ('ï¼‡', "'"),   # FULLWIDTH APOSTROPHE
        ('ï¼ˆ', '('),   # FULLWIDTH LEFT PARENTHESIS
        ('ï¼‰', ')'),   # FULLWIDTH RIGHT PARENTHESIS
        ('ï¼Š', '*'),   # FULLWIDTH ASTERISK
        ('ï¼‹', '+'),   # FULLWIDTH PLUS SIGN
        ('ï¼Œ', ','),   # FULLWIDTH COMMA
        ('ï¼', '-'),   # FULLWIDTH HYPHEN-MINUS
        ('ï¼', '.'),   # FULLWIDTH FULL STOP
        ('ï¼', '/'),   # FULLWIDTH SOLIDUS
        ('ï¼š', ':'),   # FULLWIDTH COLON
        ('ï¼›', ';'),   # FULLWIDTH SEMICOLON
        ('ï¼œ', '<'),   # FULLWIDTH LESS-THAN SIGN
        ('ï¼', '='),   # FULLWIDTH EQUALS SIGN
        ('ï¼', '>'),   # FULLWIDTH GREATER-THAN SIGN
        ('ï¼Ÿ', '?'),   # FULLWIDTH QUESTION MARK
        ('ï¼ ', '@'),   # FULLWIDTH COMMERCIAL AT
        ('ï¼»', '['),   # FULLWIDTH LEFT SQUARE BRACKET
        ('ï¼¼', '\\'),  # FULLWIDTH REVERSE SOLIDUS
        ('ï¼½', ']'),   # FULLWIDTH RIGHT SQUARE BRACKET
        ('ï¼¾', '^'),   # FULLWIDTH CIRCUMFLEX ACCENT
        ('ï¼¿', '_'),   # FULLWIDTH LOW LINE
        ('ï½€', '`'),   # FULLWIDTH GRAVE ACCENT
        ('ï½›', '{'),   # FULLWIDTH LEFT CURLY BRACKET
        ('ï½œ', '|'),   # FULLWIDTH VERTICAL LINE
        ('ï½', '}'),   # FULLWIDTH RIGHT CURLY BRACKET
        ('ï½', '~'),   # FULLWIDTH TILDE
        
        # HTMLå®ä½“ç¼–ç 
        ('&#39;', "'"),  # APOSTROPHE
        ('&quot;', '"'), # QUOTATION MARK
        ('&amp;', '&'),  # AMPERSAND
        ('&lt;', '<'),   # LESS-THAN
        ('&gt;', '>'),   # GREATER-THAN
        
        # å…¶ä»–å¸¸è§æ›¿æ¢
        (''', "'"),   # LEFT SINGLE QUOTATION MARK
        (''', "'"),   # RIGHT SINGLE QUOTATION MARK
        ('"', '"'),   # LEFT DOUBLE QUOTATION MARK
        ('"', '"'),   # RIGHT DOUBLE QUOTATION MARK
        ('â€”', '-'),   # EM DASH
        ('â€“', '-'),   # EN DASH
        ('â€¦', '...'), # HORIZONTAL ELLIPSIS
    ]
    
    for old, new in replacements:
        name = name.replace(old, new)
    
    return name


def extract_vid_from_url(url: str) -> Optional[str]:
    """ä» YouTube URL ä¸­æå– video ID"""
    try:
        parsed = urlparse(url)
        if 'youtube.com' in parsed.netloc:
            query_params = parse_qs(parsed.query)
            return query_params.get('v', [None])[0]
        elif 'youtu.be' in parsed.netloc:
            return parsed.path.lstrip('/')
    except Exception as e:
        logger.warning(f"è§£æ URL å¤±è´¥: {url} - {str(e)}")
    return None


def load_download_log(csv_path: Path) -> Dict[str, str]:
    """
    åŠ è½½ youtube_download_log.csvï¼Œæ„å»º vid -> title æ˜ å°„
    
    Returns:
        {vid: title}
    """
    vid_to_title = {}
    
    # ä½¿ç”¨ utf-8-sig è‡ªåŠ¨ç§»é™¤ BOMï¼ˆByte Order Markï¼‰
    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            url = row.get('url', '')
            title = row.get('title', '')
            status = row.get('status', 'False')
            
            if status != 'True':
                continue
            
            vid = extract_vid_from_url(url)
            if vid and title:
                vid_to_title[vid] = title
    
    logger.info(f"  æ„å»º {len(vid_to_title)} æ¡ vidâ†’title æ˜ å°„")
    return vid_to_title


def find_video_file(vid: str, title: str, video_files: List[Path], use_loose_match: bool = False) -> Optional[Path]:
    """
    æ ¹æ® vid å’Œ title æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶ï¼ˆä½¿ç”¨é¢„è®¡ç®—ç´¢å¼•ä¼˜åŒ–ï¼‰
    
    æ€§èƒ½ä¼˜åŒ–ï¼š
    - åœ¨å¤šè¿›ç¨‹ç¯å¢ƒä¸‹ï¼Œä½¿ç”¨ _worker_video_index é¢„è®¡ç®—ç´¢å¼•ï¼ˆO(1) æŸ¥æ‰¾ï¼‰
    - åœ¨å•è¿›ç¨‹ç¯å¢ƒä¸‹ï¼Œä½¿ç”¨ä¼ ç»Ÿçº¿æ€§æŸ¥æ‰¾ï¼ˆå‘åå…¼å®¹ï¼‰
    
    åŒ¹é…ç­–ç•¥ï¼š
    - ä¸ä½¿ç”¨å®½æ¾åŒ¹é…ï¼ˆé»˜è®¤ï¼Œ3å±‚ï¼‰ï¼š
      1. ç›´æ¥åŒ¹é… title.mp4
      2. è§„èŒƒåŒ–ååŒ¹é…ï¼ˆç§»é™¤ emojiï¼‰
      3. vid åŒ¹é…ï¼ˆæ–‡ä»¶ååŒ…å« vidï¼‰
    
    - ä½¿ç”¨å®½æ¾åŒ¹é…ï¼ˆ--loose-matchï¼Œ7å±‚ï¼‰ï¼š
      1. ç›´æ¥åŒ¹é… title.mp4
      2. è§„èŒƒåŒ–ååŒ¹é…ï¼ˆç§»é™¤ emojiï¼‰
      3. å®½æ¾åŒ¹é…ï¼ˆç§»é™¤ emoji + æ ‡ç‚¹ç¬¦å·ï¼‰
      4. è¶…å®½æ¾åŒ¹é…ï¼ˆåªä¿ç•™å­—æ¯å’Œæ•°å­—ï¼‰
      5. è§„èŒƒåŒ– + ç½‘ç›˜å­—ç¬¦æ›¿æ¢
      6. vid åŒ¹é…ï¼ˆæ–‡ä»¶ååŒ…å« vidï¼‰
      7. æ¨¡ç³ŠåŒ¹é…ï¼ˆtitle å‰ 30 ä¸ªå­—ç¬¦ï¼‰
    
    Args:
        vid: è§†é¢‘ ID
        title: è§†é¢‘æ ‡é¢˜
        video_files: é¢„å…ˆæ”¶é›†çš„è§†é¢‘æ–‡ä»¶åˆ—è¡¨
        use_loose_match: æ˜¯å¦ä½¿ç”¨å®½æ¾åŒ¹é…ï¼ˆé»˜è®¤: Falseï¼‰
    """
    global _worker_video_index
    
    if not video_files:
        return None
    
    # ğŸš€ ä½¿ç”¨é¢„è®¡ç®—ç´¢å¼•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if _worker_video_index is not None:
        return _find_video_file_with_index(vid, title, _worker_video_index, use_loose_match)
    
    # ä¼ ç»Ÿçº¿æ€§æŸ¥æ‰¾ï¼ˆå‘åå…¼å®¹ï¼Œå•è¿›ç¨‹æ¨¡å¼ï¼‰
    return _find_video_file_linear(vid, title, video_files, use_loose_match)


def _find_video_file_with_index(
    vid: str,
    title: str,
    index: Dict,
    use_loose_match: bool
) -> Optional[Path]:
    """
    ä½¿ç”¨é¢„è®¡ç®—ç´¢å¼•æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶ï¼ˆO(1) å­—å…¸æŸ¥æ‰¾ï¼‰
    
    é‡è¦ï¼štitle ä¸éœ€è¦åº”ç”¨ normalize_netdisk_filenameï¼Œ
    å› ä¸ºç´¢å¼•æ„å»ºæ—¶å·²ç»å¯¹æ–‡ä»¶ååº”ç”¨äº†è¯¥å‡½æ•°ã€‚
    ç›´æ¥æ¯”è¾ƒå³å¯ã€‚
    
    Args:
        vid: è§†é¢‘ ID
        title: è§†é¢‘æ ‡é¢˜ï¼ˆæ¥è‡ª CSVï¼Œæ ‡å‡†æ ¼å¼ï¼‰
        index: é¢„è®¡ç®—çš„ç´¢å¼•å­—å…¸
        use_loose_match: æ˜¯å¦ä½¿ç”¨å®½æ¾åŒ¹é…
    
    Returns:
        è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å› None
    """
    # ç­–ç•¥ 1: ç›´æ¥åŒ¹é… O(1)
    # title å’Œç´¢å¼•ä¸­çš„ key éƒ½æ˜¯æ ‡å‡†æ ¼å¼ï¼Œç›´æ¥æ¯”è¾ƒ
    if title in index['direct']:
        return index['direct'][title]
    
    # ç­–ç•¥ 2: è§„èŒƒåŒ–åŒ¹é… O(1)
    normalized_title = normalize_title_for_filename(title, remove_punctuation=False)
    if normalized_title in index['normalized']:
        return index['normalized'][normalized_title]
    
    # å®½æ¾åŒ¹é…ç­–ç•¥ï¼ˆä»…åœ¨å¯ç”¨æ—¶ï¼‰
    if use_loose_match:
        # ç­–ç•¥ 3: å®½æ¾åŒ¹é… O(1)
        loose_title = normalize_title_for_filename(title, remove_punctuation=True)
        if loose_title in index['loose']:
            return index['loose'][loose_title]
        
        # ç­–ç•¥ 4: è¶…å®½æ¾åŒ¹é… O(1)
        ultra_loose_title = normalize_for_ultra_loose_match(title)
        if ultra_loose_title in index['ultra_loose']:
            return index['ultra_loose'][ultra_loose_title]
        
        # ç­–ç•¥ 5: vid åŒ…å«åŒ¹é… O(n) - ä»éœ€éå†ï¼Œä½†å·²é¢„å­˜ stem
        for video_file, stem in index['vid_contains']:
            if vid in stem:
                return video_file
        
        # ç­–ç•¥ 6: æ¨¡ç³ŠåŒ¹é…ï¼ˆå‰ 30 ä¸ªå­—ç¬¦ï¼‰O(n)
        title_prefix = normalize_title_for_filename(title, remove_punctuation=True)[:30].lower()
        for video_file, stem in index['vid_contains']:
            # å¯¹ stem ä¹Ÿåº”ç”¨ç½‘ç›˜è§„èŒƒåŒ–åå†æ¯”è¾ƒ
            stem_normalized = normalize_netdisk_filename(stem)
            file_prefix = normalize_title_for_filename(stem_normalized, remove_punctuation=True)[:30].lower()
            if title_prefix == file_prefix:
                return video_file
    else:
        # é»˜è®¤æ¨¡å¼ï¼švid åŒ…å«åŒ¹é…
        for video_file, stem in index['vid_contains']:
            if vid in stem:
                return video_file
    
    return None


def _find_video_file_linear(
    vid: str,
    title: str,
    video_files: List[Path],
    use_loose_match: bool
) -> Optional[Path]:
    """
    ä¼ ç»Ÿçº¿æ€§æŸ¥æ‰¾ï¼ˆå‘åå…¼å®¹ï¼‰
    
    é‡è¦ï¼šå¯¹æ–‡ä»¶ååº”ç”¨ç½‘ç›˜å­—ç¬¦è§„èŒƒåŒ–ä»¥åŒ¹é…ç´¢å¼•ç‰ˆæœ¬çš„è¡Œä¸º
    
    Args:
        vid: è§†é¢‘ ID
        title: è§†é¢‘æ ‡é¢˜
        video_files: è§†é¢‘æ–‡ä»¶åˆ—è¡¨
        use_loose_match: æ˜¯å¦ä½¿ç”¨å®½æ¾åŒ¹é…
    
    Returns:
        è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å› None
    """
    # ç­–ç•¥ 1: ç›´æ¥åŒ¹é…ï¼ˆæ€»æ˜¯å¯ç”¨ï¼‰
    for video_file in video_files:
        stem_normalized = normalize_netdisk_filename(video_file.stem)
        if stem_normalized == title:
            return video_file
    
    # ç­–ç•¥ 2: è§„èŒƒåŒ–ååŒ¹é…ï¼ˆæ€»æ˜¯å¯ç”¨ï¼‰
    normalized_title = normalize_title_for_filename(title, remove_punctuation=False)
    for video_file in video_files:
        stem_normalized = normalize_netdisk_filename(video_file.stem)
        normalized_filename = normalize_title_for_filename(stem_normalized, remove_punctuation=False)
        if normalized_filename == normalized_title:
            return video_file
    
    # ä»¥ä¸‹ç­–ç•¥ä»…åœ¨ use_loose_match=True æ—¶å¯ç”¨
    if use_loose_match:
        # ç­–ç•¥ 3: å®½æ¾åŒ¹é…ï¼ˆç§»é™¤ emoji + æ ‡ç‚¹ç¬¦å·ï¼‰
        loose_title = normalize_title_for_filename(title, remove_punctuation=True)
        for video_file in video_files:
            stem_normalized = normalize_netdisk_filename(video_file.stem)
            loose_filename = normalize_title_for_filename(stem_normalized, remove_punctuation=True)
            if loose_filename == loose_title:
                return video_file
        
        # ç­–ç•¥ 4: è¶…å®½æ¾åŒ¹é…ï¼ˆåªä¿ç•™å­—æ¯å’Œæ•°å­—ï¼‰
        ultra_loose_title = normalize_for_ultra_loose_match(title)
        for video_file in video_files:
            stem_normalized = normalize_netdisk_filename(video_file.stem)
            ultra_loose_filename = normalize_for_ultra_loose_match(stem_normalized)
            if ultra_loose_filename == ultra_loose_title:
                return video_file
        
        # ç­–ç•¥ 5: vid åŒ¹é…
        for video_file in video_files:
            if vid in video_file.stem:
                return video_file
        
        # ç­–ç•¥ 6: æ¨¡ç³ŠåŒ¹é…ï¼ˆå‰ 30 ä¸ªå­—ç¬¦ï¼‰
        title_prefix = normalize_title_for_filename(title, remove_punctuation=True)[:30].lower()
        for video_file in video_files:
            stem_normalized = normalize_netdisk_filename(video_file.stem)
            file_prefix = normalize_title_for_filename(stem_normalized, remove_punctuation=True)[:30].lower()
            if title_prefix == file_prefix:
                return video_file
    else:
        # é»˜è®¤æ¨¡å¼ï¼šåªä½¿ç”¨ vid åŒ¹é…ä½œä¸ºæœ€åçš„å°è¯•
        # ç­–ç•¥ 3: vid åŒ¹é…
        for video_file in video_files:
            if vid in video_file.stem:
                return video_file
    
    return None


def load_dataset_clips(
    test_json_path: Optional[Path],
    train_json_path: Optional[Path]
) -> List[VideoClip]:
    """
    åŠ è½½ dataset_test.json å’Œ dataset_train_LocalCorrelationFilter.json
    
    Returns:
        List[VideoClip]
    """
    clips = []
    
    # åŠ è½½ test æ•°æ®
    if test_json_path and test_json_path.exists():
        logger.info(f"  åŠ è½½ test æ•°æ®: {test_json_path}")
        with open(test_json_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        skipped_count = 0
        for item in test_data:
            # è·å–æ—¶é—´ï¼Œç¡®ä¿ None è¢«è½¬æ¢ä¸ºé»˜è®¤å€¼
            start_time = item.get('begin position')
            end_time = item.get('end position')
            
            # è·³è¿‡æ— æ•ˆç‰‡æ®µï¼ˆæ—¶é—´ä¸º Noneï¼‰
            if start_time is None or end_time is None:
                skipped_count += 1
                continue
            
            clip = VideoClip(
                vid=item.get('vid', ''),
                transcript=item.get('transcript clip', ''),
                start_time=start_time,
                end_time=end_time,
                size=item.get('size', []),
                data_type='test'
            )
            clips.append(clip)
        
        logger.info(f"    test: {len(clips)} ä¸ªæœ‰æ•ˆç‰‡æ®µï¼ˆæ€»è®¡{len(test_data)}ï¼Œè·³è¿‡{skipped_count}ä¸ªæ— æ•ˆï¼‰")
    
    # åŠ è½½ train æ•°æ®
    if train_json_path and train_json_path.exists():
        logger.info(f"  åŠ è½½ train æ•°æ®: {train_json_path}")
        
        # train æ•°æ®å¯èƒ½æ˜¯æµå¼ JSONï¼ˆæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼‰
        train_data = []
        with open(train_json_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            
            # å°è¯•ç›´æ¥è§£æä¸º JSON æ•°ç»„
            if content.startswith('['):
                train_data = json.loads(content)
            else:
                # æµå¼ JSONï¼Œæ¯è¡Œä¸€ä¸ªå¯¹è±¡
                for line in content.split('\n'):
                    line = line.strip()
                    if line:
                        try:
                            train_data.append(json.loads(line))
                        except json.JSONDecodeError:
                            continue
        
        train_skipped_count = 0
        train_valid_count = 0
        train_start_idx = len(clips)
        for item in train_data:
            # è·å–æ—¶é—´ï¼Œç¡®ä¿ None è¢«è½¬æ¢ä¸ºé»˜è®¤å€¼
            start_time = item.get('begin position')
            end_time = item.get('end position')
            
            # è·³è¿‡æ— æ•ˆç‰‡æ®µï¼ˆæ—¶é—´ä¸º Noneï¼‰
            if start_time is None or end_time is None:
                train_skipped_count += 1
                continue
            
            clip = VideoClip(
                vid=item.get('vid', ''),
                transcript=item.get('transcript clip', ''),
                start_time=start_time,
                end_time=end_time,
                size=item.get('size', []),
                data_type='train'
            )
            clips.append(clip)
            train_valid_count += 1
        
        logger.info(f"    train: {train_valid_count} ä¸ªæœ‰æ•ˆç‰‡æ®µï¼ˆæ€»è®¡{len(train_data)}ï¼Œè·³è¿‡{train_skipped_count}ä¸ªæ— æ•ˆï¼‰")
    
    return clips


def generate_text_input_pkl(
    transcript: str,
    output_path: Path
) -> bool:
    """
    ä½¿ç”¨ transformers AutoTokenizer ç”Ÿæˆ text_input.pklï¼ˆä¸ CLIP4MC å®˜æ–¹ä¸€è‡´ï¼‰
    
    Args:
        transcript: æ–‡æœ¬æè¿°
        output_path: è¾“å‡º .pkl æ–‡ä»¶è·¯å¾„
    
    Returns:
        æ˜¯å¦æˆåŠŸ
    """
    try:
        from transformers import AutoTokenizer
        
        # è·å– tokenizerï¼ˆå•ä¾‹ï¼Œåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
        # ä½¿ç”¨ä¸ CLIP4MC å®˜æ–¹ç›¸åŒçš„ tokenizer
        if not hasattr(generate_text_input_pkl, 'tokenizer'):
            generate_text_input_pkl.tokenizer = AutoTokenizer.from_pretrained(
                'openai/clip-vit-base-patch16'
            )
        
        tokenizer = generate_text_input_pkl.tokenizer
        
        # Tokenizeï¼ˆä¸å®˜æ–¹æ ¼å¼ä¸€è‡´ï¼‰
        tokens = tokenizer(
            transcript,
            max_length=77,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        tokens_np = tokens['input_ids'][0].numpy()
        
        # ä¿å­˜ä¸º CLIP4MC æ ¼å¼
        with open(output_path, 'wb') as f:
            pickle.dump({'tokens': tokens_np}, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        return True
    
    except Exception as e:
        logger.error(f"ç”Ÿæˆ text_input.pkl å¤±è´¥: {str(e)}")
        return False


def process_single_clip(
    clip: VideoClip,
    vid_to_title: Dict[str, str],
    video_files: List[Path],
    text_inputs_dir: Path,
    use_loose_match: bool,
    video_prefix: str,
    text_prefix: str,
    skip_text_generation: bool = False
) -> Tuple[Optional[Dict], Optional[Dict]]:
    """
    å¤„ç†å•ä¸ªè§†é¢‘ç‰‡æ®µï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰
    
    Args:
        clip: è§†é¢‘ç‰‡æ®µ
        vid_to_title: vid åˆ° title çš„æ˜ å°„
        video_files: é¢„å…ˆæ”¶é›†çš„è§†é¢‘æ–‡ä»¶åˆ—è¡¨
        text_inputs_dir: text_input.pkl è¾“å‡ºç›®å½•
        use_loose_match: æ˜¯å¦ä½¿ç”¨å®½æ¾åŒ¹é…
        video_prefix: è§†é¢‘è·¯å¾„å‰ç¼€
        text_prefix: text_input.pkl è·¯å¾„å‰ç¼€
        skip_text_generation: æ˜¯å¦è·³è¿‡ text token ç”Ÿæˆ
    
    Returns:
        (metadata_item, failure_item) å…ƒç»„
        - å¦‚æœæˆåŠŸ: (metadata_item, None)
        - å¦‚æœå¤±è´¥: (None, failure_item)
    """
    # è·å– title
    title = vid_to_title.get(clip.vid)
    if not title:
        return None, {
            'vid': clip.vid,
            'data_type': clip.data_type,
            'transcript': clip.transcript,
            'start_time': clip.start_time,
            'end_time': clip.end_time,
            'duration': clip.end_time - clip.start_time,
            'title': None,
            'reason': 'no_download_record',
            'message': 'åœ¨ä¸‹è½½æ—¥å¿—ä¸­æœªæ‰¾åˆ°æ­¤vidçš„è®°å½•',
            'suggestion': 'æ£€æŸ¥vidæ˜¯å¦æ­£ç¡®ï¼Œæˆ–è¯¥è§†é¢‘æ˜¯å¦åœ¨ä¸‹è½½æ—¥å¿—CSVä¸­'
        }
    
    # æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶
    video_file = find_video_file(clip.vid, title, video_files, use_loose_match=use_loose_match)
    if not video_file:
        return None, {
            'vid': clip.vid,
            'data_type': clip.data_type,
            'transcript': clip.transcript,
            'start_time': clip.start_time,
            'end_time': clip.end_time,
            'duration': clip.end_time - clip.start_time,
            'title': title,
            'reason': 'video_not_found',
            'message': f'ä¸‹è½½æ—¥å¿—ä¸­æœ‰è®°å½•ï¼Œä½†åœ¨è§†é¢‘ç›®å½•ä¸­æ‰¾ä¸åˆ°å¯¹åº”æ–‡ä»¶',
            'suggestion': f'æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ¹é…: æœŸæœ›"{title}.mp4"æˆ–ç›¸ä¼¼åç§°'
        }
    
    # ç”Ÿæˆ text_input.pklï¼ˆå¦‚æœéœ€è¦ï¼‰
    text_input_path = text_inputs_dir / f"{clip.vid}_text_input.pkl"
    
    if not skip_text_generation:
        if not text_input_path.exists():
            success = generate_text_input_pkl(clip.transcript, text_input_path)
            if not success:
                return None, {
                    'vid': clip.vid,
                    'data_type': clip.data_type,
                    'transcript': clip.transcript,
                    'start_time': clip.start_time,
                    'end_time': clip.end_time,
                    'duration': clip.end_time - clip.start_time,
                    'title': title,
                    'video_file': str(video_file),
                    'reason': 'tokenization_failed',
                    'message': 'è§†é¢‘æ–‡ä»¶æ‰¾åˆ°ï¼Œä½†ç”Ÿæˆtext_input.pklå¤±è´¥',
                    'suggestion': 'æ£€æŸ¥transcriptå†…å®¹æ˜¯å¦æœ‰æ•ˆï¼Œæˆ–transformersåº“æ˜¯å¦æ­£å¸¸'
                }
    
    # æ„å»ºè·¯å¾„ï¼ˆä½¿ç”¨ prefixï¼‰
    video_path = str(video_file.absolute())
    if video_prefix:
        video_path = video_prefix.rstrip('/') + '/' + video_file.name
    
    text_path = str(text_input_path.absolute())
    if text_prefix:
        text_path = text_prefix.rstrip('/') + '/' + text_input_path.name
    
    # æ„å»ºå…ƒæ•°æ®é¡¹
    metadata_item = {
        'vid': clip.vid,
        'video_path': video_path,
        'start_time': clip.start_time,
        'end_time': clip.end_time,
        'transcript': clip.transcript,
        'size': clip.size,
        'data_type': clip.data_type,
        'text_input_path': text_path if not skip_text_generation else None
    }
    
    return metadata_item, None


def main():
    parser = argparse.ArgumentParser(
        description="ç”Ÿæˆ Decord Pipeline å…ƒæ•°æ®æ–‡ä»¶",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--download-log", type=Path, required=True,
                       help="youtube_download_log.csv è·¯å¾„")
    parser.add_argument("--videos-dir", type=Path, required=True,
                       help="è§†é¢‘æ–‡ä»¶ç›®å½•")
    parser.add_argument("--text-inputs-dir", type=Path, required=True,
                       help="text_input.pkl è¾“å‡ºç›®å½•")
    parser.add_argument("--output", type=Path, required=True,
                       help="è¾“å‡ºå…ƒæ•°æ® JSON æ–‡ä»¶è·¯å¾„")
    
    # å¯é€‰å‚æ•°
    parser.add_argument("--test-json", type=Path,
                       help="dataset_test.json è·¯å¾„")
    parser.add_argument("--train-json", type=Path,
                       help="dataset_train_LocalCorrelationFilter.json è·¯å¾„")
    
    # åŒ¹é…å‚æ•°
    parser.add_argument("--loose-match", action='store_true',
                       help="å¯ç”¨å®½æ¾åŒ¹é…ï¼ˆç§»é™¤ç‰¹æ®Šå­—ç¬¦ã€emojiã€æ ‡ç‚¹ç¬¦å·ï¼‰")
    
    # Text token ç”Ÿæˆå‚æ•°
    parser.add_argument("--skip-text-generation", action='store_true',
                       help="è·³è¿‡ text_input.pkl ç”Ÿæˆï¼ˆä»…æµ‹è¯•è§†é¢‘åŒ¹é…æ€§èƒ½ï¼‰")
    
    # è·¯å¾„å‰ç¼€å‚æ•°
    parser.add_argument("--video-prefix", type=str, default="",
                       help="è§†é¢‘è·¯å¾„å‰ç¼€ï¼ˆä¾‹å¦‚ï¼š/mnt/data/ï¼‰")
    parser.add_argument("--text-prefix", type=str, default="",
                       help="text_input.pkl è·¯å¾„å‰ç¼€ï¼ˆä¾‹å¦‚ï¼š/mnt/data/ï¼‰")
    
    # å¤±è´¥åˆ†ææŠ¥å‘Šè¾“å‡º
    parser.add_argument("--unmatched-output", type=Path,
                       help="å¤±è´¥åˆ†ææŠ¥å‘Šè¾“å‡ºè·¯å¾„ï¼ˆJSONæ ¼å¼ï¼ŒåŒ…å«è¯¦ç»†å¤±è´¥åŸå› å’Œç»Ÿè®¡ï¼‰")
    
    # å¤šè¿›ç¨‹å‚æ•°
    parser.add_argument("--num-workers", type=int, default=1,
                       help="å¹¶è¡Œå¤„ç†çš„è¿›ç¨‹æ•°ï¼ˆé»˜è®¤: 1ï¼Œå»ºè®®: CPUæ ¸å¿ƒæ•°ï¼‰")
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥
    if not args.test_json and not args.train_json:
        logger.error("å¿…é¡»è‡³å°‘æŒ‡å®š --test-json æˆ– --train-json ä¹‹ä¸€")
        return 1
    
    if not args.download_log.exists():
        logger.error(f"ä¸‹è½½æ—¥å¿—ä¸å­˜åœ¨: {args.download_log}")
        return 1
    
    if not args.videos_dir.exists():
        logger.error(f"è§†é¢‘ç›®å½•ä¸å­˜åœ¨: {args.videos_dir}")
        return 1
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    args.text_inputs_dir.mkdir(parents=True, exist_ok=True)
    args.output.parent.mkdir(parents=True, exist_ok=True)
    
    logger.info("=" * 60)
    logger.info("å¼€å§‹ç”Ÿæˆ Decord Pipeline å…ƒæ•°æ®")
    logger.info("=" * 60)
    
    # 1. åŠ è½½æ•°æ®é›†ï¼ˆä½œä¸ºç‰‡æ®µç´¢å¼•ï¼‰
    logger.info("æ­¥éª¤ 1: åŠ è½½ç‰‡æ®µç´¢å¼•...")
    clips = load_dataset_clips(args.test_json, args.train_json)
    logger.info(f"  éœ€è¦å¤„ç† {len(clips)} ä¸ªç‰‡æ®µ")
    
    # 2. åŠ è½½ä¸‹è½½æ—¥å¿—ï¼ˆvidâ†’titleæ˜ å°„ï¼‰
    logger.info("æ­¥éª¤ 2: åŠ è½½ä¸‹è½½æ—¥å¿—...")
    vid_to_title = load_download_log(args.download_log)
    
    # 3. æ‰«æè§†é¢‘ç›®å½•ï¼ˆå®é™…å­˜åœ¨çš„è§†é¢‘æ–‡ä»¶ï¼‰
    logger.info("æ­¥éª¤ 3: æ‰«æè§†é¢‘ç›®å½•...")
    video_extensions = ['.mp4', '.avi', '.mkv', '.flv', '.mov', '.webm']
    video_files = []
    for ext in video_extensions:
        video_files.extend(args.videos_dir.glob(f'*{ext}'))
    logger.info(f"  æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶ï¼ˆä¸€ä¸ªè§†é¢‘å¯å¯¹åº”å¤šä¸ªç‰‡æ®µï¼‰")
    
    # 4. åŒ¹é…è§†é¢‘æ–‡ä»¶å¹¶ç”Ÿæˆå…ƒæ•°æ®
    if args.skip_text_generation:
        logger.info("æ­¥éª¤ 4: åŒ¹é…è§†é¢‘æ–‡ä»¶ï¼ˆè·³è¿‡ text_input.pkl ç”Ÿæˆï¼‰...")
        logger.info("  âš ï¸  æ€§èƒ½æµ‹è¯•æ¨¡å¼ï¼štext token ç”Ÿæˆå·²ç¦ç”¨")
    else:
        logger.info("æ­¥éª¤ 4: åŒ¹é…è§†é¢‘æ–‡ä»¶å¹¶ç”Ÿæˆ text_input.pkl...")
    logger.info(f"  ä½¿ç”¨ {args.num_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†")
    
    metadata = []
    matched_count = 0
    failed_count = 0
    failure_items = []  # æ”¶é›†å¤±è´¥é¡¹ï¼ˆå«è¯¦ç»†ä¿¡æ¯ï¼‰
    
    if args.num_workers == 1:
        # å•è¿›ç¨‹æ¨¡å¼ï¼ˆä¹Ÿä½¿ç”¨é¢„è®¡ç®—ç´¢å¼•ä¼˜åŒ–æ€§èƒ½ï¼‰
        global _worker_video_index
        logger.info(f"  æ„å»ºè§†é¢‘æ–‡ä»¶ç´¢å¼•ï¼ˆ{len(video_files)} ä¸ªæ–‡ä»¶ï¼‰...")
        _worker_video_index = _build_video_index(video_files, args.loose_match)
        logger.info("  âœ… ç´¢å¼•æ„å»ºå®Œæˆ")
        
        for clip in tqdm(clips, desc="å¤„ç†è¿›åº¦", unit="clip"):
            metadata_item, failure_item = process_single_clip(
                clip, vid_to_title, video_files, args.text_inputs_dir,
                args.loose_match, args.video_prefix, args.text_prefix,
                args.skip_text_generation
            )
            
            if metadata_item:
                metadata.append(metadata_item)
                matched_count += 1
            else:
                failure_items.append(failure_item)
                failed_count += 1
    else:
        # å¤šè¿›ç¨‹æ¨¡å¼ï¼ˆä½¿ç”¨ Pool initializer é¿å…é‡å¤ä¼ é€’æ•°æ®ï¼‰
        logger.info(f"  æ¯ä¸ª worker è¿›ç¨‹å°†ç‹¬ç«‹æ„å»ºè§†é¢‘ç´¢å¼•ï¼ˆ{len(video_files)} ä¸ªæ–‡ä»¶ï¼‰")
        with Pool(
            processes=args.num_workers,
            initializer=_init_metadata_worker,
            initargs=(
                vid_to_title,
                video_files,
                args.text_inputs_dir,
                args.loose_match,
                args.video_prefix,
                args.text_prefix,
                args.skip_text_generation
            )
        ) as pool:
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
            for metadata_item, failure_item in tqdm(
                pool.imap(_process_clip_worker, clips),
                total=len(clips),
                desc="å¤„ç†è¿›åº¦",
                unit="clip"
            ):
                if metadata_item:
                    metadata.append(metadata_item)
                    matched_count += 1
                else:
                    failure_items.append(failure_item)
                    failed_count += 1
    
    # 5. ä¿å­˜å…ƒæ•°æ®
    logger.info("æ­¥éª¤ 5: ä¿å­˜å…ƒæ•°æ®...")
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    # 6. ç”Ÿæˆå¹¶ä¿å­˜å¤±è´¥åˆ†ææŠ¥å‘Šï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.unmatched_output and failure_items:
        logger.info("æ­¥éª¤ 6: ç”Ÿæˆå¤±è´¥åˆ†ææŠ¥å‘Š...")
        
        # ç»Ÿè®¡å„ç§å¤±è´¥åŸå› 
        failure_stats = {}
        for item in failure_items:
            reason = item['reason']
            failure_stats[reason] = failure_stats.get(reason, 0) + 1
        
        # æŒ‰data_typeåˆ†ç±»
        failure_by_type = {'test': [], 'train': []}
        for item in failure_items:
            data_type = item.get('data_type', 'unknown')
            if data_type in failure_by_type:
                failure_by_type[data_type].append(item)
        
        # æ„å»ºå®Œæ•´æŠ¥å‘Š
        failure_report = {
            'summary': {
                'total_clips': len(clips),
                'matched': matched_count,
                'failed': failed_count,
                'success_rate': f"{matched_count/len(clips)*100:.2f}%",
                'failure_breakdown': failure_stats,
                'failure_by_data_type': {
                    'test': len(failure_by_type['test']),
                    'train': len(failure_by_type['train'])
                }
            },
            'failure_analysis': {
                'no_download_record': {
                    'description': 'vidåœ¨ä¸‹è½½æ—¥å¿—CSVä¸­ä¸å­˜åœ¨',
                    'count': failure_stats.get('no_download_record', 0),
                    'solution': '1. æ£€æŸ¥vidæ˜¯å¦æ­£ç¡®\n2. ç¡®è®¤è¯¥è§†é¢‘æ˜¯å¦å·²ä¸‹è½½\n3. æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å®Œæ•´'
                },
                'video_not_found': {
                    'description': 'vidåœ¨ä¸‹è½½æ—¥å¿—ä¸­å­˜åœ¨ï¼Œä½†æ‰¾ä¸åˆ°å¯¹åº”è§†é¢‘æ–‡ä»¶',
                    'count': failure_stats.get('video_not_found', 0),
                    'solution': '1. æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n2. æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ¹é…\n3. å°è¯•ä½¿ç”¨--loose-matchå‚æ•°'
                },
                'tokenization_failed': {
                    'description': 'è§†é¢‘æ–‡ä»¶æ‰¾åˆ°ï¼Œä½†ç”Ÿæˆtext_input.pklå¤±è´¥',
                    'count': failure_stats.get('tokenization_failed', 0),
                    'solution': '1. æ£€æŸ¥transcriptå†…å®¹\n2. æ£€æŸ¥transformersåº“æ˜¯å¦æ­£å¸¸\n3. æ£€æŸ¥ç£ç›˜ç©ºé—´'
                }
            },
            'failed_clips': failure_items
        }
        
        with open(args.unmatched_output, 'w', encoding='utf-8') as f:
            json.dump(failure_report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"  å¤±è´¥åˆ†ææŠ¥å‘Š: {args.unmatched_output}")
        logger.info(f"  å¤±è´¥åŸå› ç»Ÿè®¡:")
        for reason, count in failure_stats.items():
            logger.info(f"    - {reason}: {count} ä¸ªç‰‡æ®µ")
    
    # å®Œæˆ
    logger.info("=" * 60)
    logger.info("âœ… å…ƒæ•°æ®ç”Ÿæˆå®Œæˆï¼")
    logger.info("=" * 60)
    logger.info(f"æ•°æ®æµæ¦‚è§ˆ:")
    logger.info(f"  1ï¸âƒ£  ç‰‡æ®µç´¢å¼• (dataset): {len(clips)} ä¸ªç‰‡æ®µ")
    logger.info(f"  2ï¸âƒ£  ä¸‹è½½è®°å½• (csv): {len(vid_to_title)} æ¡æ˜ å°„")
    logger.info(f"  3ï¸âƒ£  è§†é¢‘æ–‡ä»¶ (å®é™…): {len(video_files)} ä¸ªæ–‡ä»¶")
    logger.info(f"  4ï¸âƒ£  åŒ¹é…ç»“æœ:")
    logger.info(f"      âœ… æˆåŠŸ: {matched_count} ä¸ªç‰‡æ®µ")
    logger.info(f"      âŒ å¤±è´¥: {failed_count} ä¸ªç‰‡æ®µ")
    logger.info(f"")
    logger.info(f"è¾“å‡ºæ–‡ä»¶:")
    logger.info(f"  ğŸ“„ metadata.json: {args.output}")
    if args.unmatched_output and failure_items:
        logger.info(f"  ğŸ“„ å¤±è´¥åˆ†ææŠ¥å‘Š: {args.unmatched_output}")
    logger.info("=" * 60)
    
    return 0


if __name__ == "__main__":
    import sys
    sys.exit(main())

