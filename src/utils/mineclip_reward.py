#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
MineCLIP å®˜æ–¹å¥–åŠ±åŒ…è£…å™¨
ä½¿ç”¨å·²å®‰è£…çš„ MineCLIP æ¨¡å‹è®¡ç®—å¯†é›†å¥–åŠ±
æ”¯æŒå•å¸§å’Œ16å¸§è§†é¢‘æ¨¡å¼
"""

import os
import gym
import numpy as np
import torch
from collections import deque


class MineCLIPRewardWrapper(gym.Wrapper):
    """
    ä½¿ç”¨å®˜æ–¹ MineCLIP æ¨¡å‹çš„å¥–åŠ±åŒ…è£…å™¨
    
    ç‰¹æ€§ï¼š
    - ä½¿ç”¨é¢„è®­ç»ƒçš„ MineCLIP æ¨¡å‹ï¼ˆattn æˆ– avgï¼‰
    - è®¡ç®—å½“å‰ç”»é¢ä¸ä»»åŠ¡æè¿°çš„ç›¸ä¼¼åº¦
    - æä¾›è¿ç»­å¯†é›†å¥–åŠ±ï¼ˆæ¯ä¸€æ­¥éƒ½æœ‰åé¦ˆï¼‰
    """
    
    def __init__(self, env, task_prompt, 
                 model_path=None,
                 variant="attn",
                 sparse_weight=10.0, 
                 mineclip_weight=0.1,
                 device="auto",
                 use_dynamic_weight=False,
                 weight_decay_steps=50000,
                 min_weight=0.01,
                 use_video_mode=True,
                 num_frames=16,
                 compute_frequency=4):
        """
        åˆå§‹åŒ– MineCLIP å¥–åŠ±åŒ…è£…å™¨
        
        Args:
            env: åŸºç¡€ç¯å¢ƒ
            task_prompt: ä»»åŠ¡æè¿°ï¼ˆè‹±æ–‡ï¼‰ï¼Œå¦‚ "chopping a tree with hand"
            model_path: MineCLIP æ¨¡å‹æƒé‡è·¯å¾„ï¼ˆ.pth æ–‡ä»¶ï¼‰
            variant: MineCLIP å˜ä½“ ("attn" æˆ– "avg")
            sparse_weight: ç¨€ç–å¥–åŠ±æƒé‡
            mineclip_weight: MineCLIP å¯†é›†å¥–åŠ±åˆå§‹æƒé‡
            device: è¿è¡Œè®¾å¤‡ ("cpu", "cuda", "mps", æˆ– "auto")
            use_dynamic_weight: æ˜¯å¦ä½¿ç”¨åŠ¨æ€æƒé‡è°ƒæ•´ï¼ˆè¯¾ç¨‹å­¦ä¹ ï¼‰
            weight_decay_steps: æƒé‡è¡°å‡åˆ°æœ€å°å€¼æ‰€éœ€çš„æ­¥æ•°
            min_weight: MineCLIPæƒé‡çš„æœ€å°å€¼
            use_video_mode: æ˜¯å¦ä½¿ç”¨16å¸§è§†é¢‘æ¨¡å¼ï¼ˆæ¨èï¼‰
            num_frames: è§†é¢‘å¸§æ•°ï¼ˆé»˜è®¤16ï¼Œç¬¦åˆMineCLIPå®˜æ–¹ï¼‰
            compute_frequency: æ¯Næ­¥è®¡ç®—ä¸€æ¬¡MineCLIPï¼ˆå‡å°‘å¼€é”€ï¼‰
        """
        super().__init__(env)
        
        self.task_prompt = task_prompt
        self.sparse_weight = sparse_weight
        self.initial_mineclip_weight = mineclip_weight
        self.mineclip_weight = mineclip_weight
        self.variant = variant
        
        # åŠ¨æ€æƒé‡è°ƒæ•´å‚æ•°
        self.use_dynamic_weight = use_dynamic_weight
        self.weight_decay_steps = weight_decay_steps
        self.min_weight = min_weight
        self.step_count = 0
        
        # è§†é¢‘æ¨¡å¼å‚æ•°
        self.use_video_mode = use_video_mode
        self.num_frames = num_frames
        self.compute_frequency = compute_frequency
        self.frame_buffer = deque(maxlen=num_frames) if use_video_mode else None
        
        # æ£€æµ‹è®¾å¤‡
        if device == "auto":
            if torch.cuda.is_available():
                self.device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = "mps"
            else:
                self.device = "cpu"
        else:
            self.device = device
        
        print(f"  MineCLIP å¥–åŠ±åŒ…è£…å™¨:")
        print(f"    ä»»åŠ¡æè¿°: {task_prompt}")
        print(f"    æ¨¡å‹å˜ä½“: {variant}")
        print(f"    è¿è¡Œæ¨¡å¼: {'ğŸ¬ 16å¸§è§†é¢‘æ¨¡å¼' if use_video_mode else 'ğŸ–¼ï¸  å•å¸§æ¨¡å¼'}")
        if use_video_mode:
            print(f"    è§†é¢‘å¸§æ•°: {num_frames}å¸§")
            print(f"    è®¡ç®—é¢‘ç‡: æ¯{compute_frequency}æ­¥")
        print(f"    ç¨€ç–æƒé‡: {sparse_weight}")
        print(f"    MineCLIPæƒé‡: {mineclip_weight} (åˆå§‹å€¼)")
        if use_dynamic_weight:
            print(f"    åŠ¨æ€æƒé‡: å¯ç”¨ (è¡°å‡æ­¥æ•°: {weight_decay_steps}, æœ€å°å€¼: {min_weight})")
        print(f"    è®¾å¤‡: {self.device}")
        
        # åŠ è½½ MineCLIP æ¨¡å‹
        self.mineclip_available = self._load_mineclip(model_path)
        
        if self.mineclip_available:
            print(f"    çŠ¶æ€: âœ“ MineCLIP æ¨¡å‹å·²åŠ è½½")
            # é¢„è®¡ç®—ä»»åŠ¡æ–‡æœ¬çš„ç‰¹å¾
            self.task_features = self._encode_text(task_prompt)
        else:
            print(f"    çŠ¶æ€: âœ— MineCLIP ä¸å¯ç”¨ï¼Œä½¿ç”¨ç¨€ç–å¥–åŠ±")
        
        # åˆå§‹åŒ–çŠ¶æ€
        self.previous_similarity = 0.0
    
    def _load_mineclip(self, model_path):
        """
        åŠ è½½ MineCLIP æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æƒé‡è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ è½½
        """
        try:
            # å¯¼å…¥ MineCLIP
            from mineclip import MineCLIP
            
            # MineCLIP é…ç½®ï¼ˆattn å’Œ avg çš„åŒºåˆ«åœ¨ pool_typeï¼‰
            configs = {
                "attn": {
                    "arch": "vit_base_p16_fz.v2.t2",
                    "pool_type": "attn.d2.nh8.glusw",
                    "resolution": (160, 256),
                    "image_feature_dim": 512,
                    "mlp_adapter_spec": "v0-2.t0",
                    "hidden_dim": 512
                },
                "avg": {
                    "arch": "vit_base_p16_fz.v2.t2",
                    "pool_type": "avg",
                    "resolution": (160, 256),
                    "image_feature_dim": 512,
                    "mlp_adapter_spec": "v0-2.t0",
                    "hidden_dim": 512
                }
            }
            
            if self.variant not in configs:
                print(f"    âœ— æœªçŸ¥çš„ variant: {self.variant}")
                return False
            
            # åˆ›å»ºæ¨¡å‹
            print(f"    æ­£åœ¨åŠ è½½ MineCLIP {self.variant} æ¨¡å‹...")
            config = configs[self.variant]
            self.model = MineCLIP(**config).to(self.device)
            
            # åŠ è½½é¢„è®­ç»ƒæƒé‡
            if model_path and os.path.exists(model_path):
                print(f"    ä» {model_path} åŠ è½½æƒé‡...")
                checkpoint = torch.load(model_path, map_location=self.device)
                
                # checkpoint å¯èƒ½æ˜¯å­—å…¸æˆ–ç›´æ¥æ˜¯ state_dict
                state_dict = None
                if isinstance(checkpoint, dict):
                    if 'state_dict' in checkpoint:
                        state_dict = checkpoint['state_dict']
                    elif 'model' in checkpoint:
                        state_dict = checkpoint['model']
                    else:
                        state_dict = checkpoint
                else:
                    state_dict = checkpoint
                
                # å¤„ç†é”®åï¼šå»æ‰ 'model.' å‰ç¼€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                new_state_dict = {}
                for key, value in state_dict.items():
                    if key.startswith('model.'):
                        new_key = key[6:]  # å»æ‰ 'model.' å‰ç¼€
                        new_state_dict[new_key] = value
                    else:
                        new_state_dict[key] = value
                
                self.model.load_state_dict(new_state_dict)
                print(f"    âœ“ æƒé‡åŠ è½½æˆåŠŸ")
            else:
                print(f"    âš ï¸ æœªæŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–ï¼ˆæ€§èƒ½ä¼šå¾ˆå·®ï¼‰")
                print(f"    è¯·ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹: https://github.com/MineDojo/MineCLIP")
            
            self.model.eval()
            
            # åŠ è½½ tokenizerï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°ï¼Œé¿å…æ¯æ¬¡è®¿é—® HuggingFaceï¼‰
            from transformers import CLIPTokenizer
            
            # æœ¬åœ° tokenizer è·¯å¾„
            local_tokenizer_path = "data/clip_tokenizer"
            
            if os.path.exists(local_tokenizer_path):
                print(f"    ä½¿ç”¨æœ¬åœ° tokenizer: {local_tokenizer_path}")
                self.tokenizer = CLIPTokenizer.from_pretrained(local_tokenizer_path)
            else:
                print(f"    æœ¬åœ° tokenizer ä¸å­˜åœ¨ï¼Œä» HuggingFace ä¸‹è½½...")
                print(f"    æç¤º: è¿è¡Œ 'python scripts/download_clip_tokenizer.py' å¯ç¦»çº¿ä½¿ç”¨")
                self.tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-base-patch16")
            
            return True
            
        except ImportError as e:
            print(f"    âœ— MineCLIP æœªå®‰è£…: {e}")
            print(f"    å®‰è£…å‘½ä»¤: pip install git+https://github.com/MineDojo/MineCLIP")
            return False
        except Exception as e:
            print(f"    âœ— MineCLIP åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _encode_text(self, text):
        """
        ç¼–ç æ–‡æœ¬ä¸ºç‰¹å¾å‘é‡
        
        Args:
            text: æ–‡æœ¬æè¿°
            
        Returns:
            torch.Tensor: æ–‡æœ¬ç‰¹å¾å‘é‡
        """
        with torch.no_grad():
            # Tokenize æ–‡æœ¬ï¼ˆå›ºå®šé•¿åº¦ 77ï¼‰
            tokens = self.tokenizer(
                text,
                return_tensors="pt",
                padding="max_length",
                max_length=77,
                truncation=True
            )
            
            # ç§»åˆ°è®¾å¤‡
            token_ids = tokens['input_ids'].to(self.device)
            
            # MineCLIP ç¼–ç æ–‡æœ¬
            text_features = self.model.encode_text(token_ids)
            return text_features
    
    def _encode_image(self, image):
        """
        ç¼–ç å›¾åƒä¸ºç‰¹å¾å‘é‡
        
        Args:
            image: å›¾åƒï¼ˆnumpy array, shape: (C, H, W), èŒƒå›´ [0, 1]ï¼‰
            
        Returns:
            image_features: å›¾åƒç‰¹å¾å‘é‡
        """
        with torch.no_grad():
            # è½¬æ¢ä¸º tensor å¹¶æ·»åŠ  batch ç»´åº¦
            if isinstance(image, np.ndarray):
                image = torch.from_numpy(image).float()
            
            # ç¡®ä¿èŒƒå›´åœ¨ [0, 1]
            if image.max() > 1.0:
                image = image / 255.0
            
            # æ·»åŠ  batch ç»´åº¦
            if image.dim() == 3:
                image = image.unsqueeze(0)
            
            # ç§»åˆ°è®¾å¤‡
            image = image.to(self.device)
            
            # CLIPæ ‡å‡†å½’ä¸€åŒ–ï¼ˆImageNetå‡å€¼å’Œæ ‡å‡†å·®ï¼‰
            mean = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)
            image = (image - mean) / std
            
            # MineCLIP ç¼–ç å›¾åƒï¼ˆä½¿ç”¨ forward_image_featuresï¼‰
            image_features = self.model.forward_image_features(image)
            
            return image_features
    
    def _compute_similarity(self, image):
        """
        è®¡ç®—å›¾åƒä¸ä»»åŠ¡çš„ç›¸ä¼¼åº¦
        
        Args:
            image: å½“å‰è§‚å¯Ÿå›¾åƒ
            
        Returns:
            similarity: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ä¹‹é—´ï¼‰
        """
        if not self.mineclip_available:
            return 0.0
        
        try:
            # ç¼–ç å›¾åƒ
            image_features = self._encode_image(image)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = torch.cosine_similarity(
                image_features, 
                self.task_features, 
                dim=-1
            )
            
            # è½¬æ¢ä¸º [0, 1] èŒƒå›´
            # cosine similarity èŒƒå›´æ˜¯ [-1, 1]ï¼Œæ˜ å°„åˆ° [0, 1]
            similarity = (similarity + 1.0) / 2.0
            
            return float(similarity.item())
        
        except Exception as e:
            print(f"    âš ï¸ ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _update_mineclip_weight(self):
        """
        æ ¹æ®è®­ç»ƒæ­¥æ•°åŠ¨æ€æ›´æ–°MineCLIPæƒé‡ï¼ˆè¯¾ç¨‹å­¦ä¹ ï¼‰
        
        ç­–ç•¥ï¼šä½¿ç”¨ä½™å¼¦è¡°å‡ï¼Œä»åˆå§‹æƒé‡é€æ¸è¡°å‡åˆ°æœ€å°æƒé‡
        - æ—©æœŸï¼šé«˜æƒé‡ï¼Œagentä¾èµ–MineCLIPå¼•å¯¼
        - åæœŸï¼šä½æƒé‡ï¼Œagentæ›´å¤šä¾èµ–ç¨€ç–å¥–åŠ±å’Œè‡ªèº«ç­–ç•¥
        """
        if not self.use_dynamic_weight:
            return
        
        # è®¡ç®—è¡°å‡è¿›åº¦ [0, 1]
        progress = min(self.step_count / self.weight_decay_steps, 1.0)
        
        # ä½™å¼¦è¡°å‡ï¼šä»1.0å¹³æ»‘ä¸‹é™åˆ°0.0
        decay_factor = 0.5 * (1.0 + np.cos(np.pi * progress))
        
        # è®¡ç®—å½“å‰æƒé‡
        weight_range = self.initial_mineclip_weight - self.min_weight
        self.mineclip_weight = self.min_weight + weight_range * decay_factor
    
    def _encode_video(self, frames):
        """
        ç¼–ç 16å¸§è§†é¢‘åºåˆ—ï¼ˆMineCLIPå®˜æ–¹æ–¹å¼ï¼‰
        
        Args:
            frames: List of [H, W, C] numpy arrays
            
        Returns:
            video_features: è§†é¢‘ç‰¹å¾å‘é‡
        """
        if not self.mineclip_available:
            return None
        
        try:
            with torch.no_grad():
                # MineCraftå®˜æ–¹å½’ä¸€åŒ–å‚æ•°
                MC_MEAN = torch.tensor([0.3331, 0.3245, 0.3051], device=self.device).view(1, 1, 3, 1, 1)
                MC_STD = torch.tensor([0.2439, 0.2493, 0.2873], device=self.device).view(1, 1, 3, 1, 1)
                
                # é¢„å¤„ç†å¸§åºåˆ—
                processed_frames = []
                for frame in frames:
                    # è½¬æ¢ä¸ºtensor
                    if isinstance(frame, np.ndarray):
                        frame_tensor = torch.from_numpy(frame).float()
                    else:
                        frame_tensor = frame.float()
                    
                    # ç¡®ä¿æ˜¯ [H, W, C] æ ¼å¼
                    if frame_tensor.dim() == 3 and frame_tensor.shape[0] == 3:
                        frame_tensor = frame_tensor.permute(1, 2, 0)  # [C, H, W] -> [H, W, C]
                    
                    # å½’ä¸€åŒ–åˆ° [0, 1]
                    if frame_tensor.max() > 1.0:
                        frame_tensor = frame_tensor / 255.0
                    
                    # [H, W, C] -> [C, H, W]
                    frame_tensor = frame_tensor.permute(2, 0, 1)
                    
                    processed_frames.append(frame_tensor)
                
                # å †å ä¸º [T, C, H, W]
                video_tensor = torch.stack(processed_frames).unsqueeze(0).to(self.device)  # [1, T, C, H, W]
                
                # MineCraftå½’ä¸€åŒ–
                video_tensor = (video_tensor - MC_MEAN) / MC_STD
                
                # ä½¿ç”¨MineCLIPçš„encode_videoï¼ˆå®Œæ•´å®˜æ–¹æµç¨‹ï¼‰
                video_features = self.model.encode_video(video_tensor)
                
                return video_features
        
        except Exception as e:
            print(f"    âš ï¸ è§†é¢‘ç¼–ç å¤±è´¥: {e}")
            return None
    
    def _compute_video_similarity(self, frames):
        """
        è®¡ç®—16å¸§è§†é¢‘ä¸ä»»åŠ¡çš„ç›¸ä¼¼åº¦
        
        Args:
            frames: List of 16 frames
            
        Returns:
            similarity: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ä¹‹é—´ï¼‰
        """
        if not self.mineclip_available or len(frames) < self.num_frames:
            return 0.0
        
        try:
            # ç¼–ç è§†é¢‘
            video_features = self._encode_video(frames)
            if video_features is None:
                return 0.0
            
            # å½’ä¸€åŒ–
            video_features = video_features / video_features.norm(dim=-1, keepdim=True)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = (video_features @ self.task_features.T).item()
            
            # è½¬æ¢ä¸º [0, 1] èŒƒå›´
            similarity = (similarity + 1.0) / 2.0
            
            return float(similarity)
        
        except Exception as e:
            print(f"    âš ï¸ è§†é¢‘ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def reset(self, **kwargs):
        """é‡ç½®ç¯å¢ƒ"""
        # MineDojo çš„ reset ä¸æ¥å—å‚æ•°
        obs = self.env.reset()
        
        # æ¸…ç©ºå¸§ç¼“å†²
        if self.use_video_mode and self.frame_buffer is not None:
            self.frame_buffer.clear()
            # ç”¨åˆå§‹å¸§å¡«å……ç¼“å†²åŒº
            for _ in range(self.num_frames):
                self.frame_buffer.append(obs.copy())
        
        if self.mineclip_available:
            # è®¡ç®—åˆå§‹ç›¸ä¼¼åº¦
            if self.use_video_mode and len(self.frame_buffer) == self.num_frames:
                self.previous_similarity = self._compute_video_similarity(list(self.frame_buffer))
            else:
                self.previous_similarity = self._compute_similarity(obs)
        
        return obs
    
    def step(self, action):
        """
        æ‰§è¡Œä¸€æ­¥ï¼Œè¿”å›å¢å¼ºçš„å¥–åŠ±
        
        MineCLIP å¯†é›†å¥–åŠ±æœºåˆ¶ï¼š
        - å•å¸§æ¨¡å¼ï¼šè®¡ç®—å½“å‰ç”»é¢ä¸ä»»åŠ¡çš„ç›¸ä¼¼åº¦
        - 16å¸§è§†é¢‘æ¨¡å¼ï¼šç´¯ç§¯16å¸§ï¼Œæ¯Næ­¥è®¡ç®—è§†é¢‘ä¸ä»»åŠ¡çš„ç›¸ä¼¼åº¦
        
        Args:
            action: åŠ¨ä½œ
            
        Returns:
            tuple: (è§‚å¯Ÿ, å¥–åŠ±, å®Œæˆæ ‡å¿—, ä¿¡æ¯)
        """
        obs, sparse_reward, done, info = self.env.step(action)
        
        # æ›´æ–°æ­¥æ•°è®¡æ•°å™¨
        self.step_count += 1
        
        # æ·»åŠ å¸§åˆ°ç¼“å†²åŒºï¼ˆè§†é¢‘æ¨¡å¼ï¼‰
        if self.use_video_mode and self.frame_buffer is not None:
            self.frame_buffer.append(obs.copy())
        
        if self.mineclip_available:
            # æ›´æ–°MineCLIPæƒé‡ï¼ˆå¦‚æœå¯ç”¨åŠ¨æ€è°ƒæ•´ï¼‰
            self._update_mineclip_weight()
            
            # è®¡ç®—å½“å‰ç›¸ä¼¼åº¦
            current_similarity = 0.0
            should_compute = False
            
            if self.use_video_mode:
                # 16å¸§è§†é¢‘æ¨¡å¼ï¼šæ¯Næ­¥è®¡ç®—ä¸€æ¬¡
                if self.step_count % self.compute_frequency == 0 and len(self.frame_buffer) == self.num_frames:
                    current_similarity = self._compute_video_similarity(list(self.frame_buffer))
                    should_compute = True
                else:
                    # éè®¡ç®—æ­¥ï¼Œä¿æŒä¸Šä¸€æ¬¡çš„ç›¸ä¼¼åº¦
                    current_similarity = self.previous_similarity
            else:
                # å•å¸§æ¨¡å¼ï¼šæ¯æ­¥éƒ½è®¡ç®—
                current_similarity = self._compute_similarity(obs)
                should_compute = True
            
            # MineCLIP å¯†é›†å¥–åŠ± = ç›¸ä¼¼åº¦è¿›æ­¥é‡
            if should_compute:
                mineclip_reward = current_similarity - self.previous_similarity
                self.previous_similarity = current_similarity
            else:
                mineclip_reward = 0.0
            
            # ç»„åˆå¥–åŠ±
            total_reward = (
                sparse_reward * self.sparse_weight + 
                mineclip_reward * self.mineclip_weight
            )
            
            # è®°å½•è¯¦ç»†ä¿¡æ¯
            info['sparse_reward'] = sparse_reward
            info['mineclip_reward'] = mineclip_reward
            info['mineclip_similarity'] = current_similarity
            info['mineclip_weight'] = self.mineclip_weight
            info['sparse_weight'] = self.sparse_weight
            info['total_reward'] = total_reward
        else:
            # MineCLIP ä¸å¯ç”¨ï¼Œåªä½¿ç”¨ç¨€ç–å¥–åŠ±
            total_reward = sparse_reward
        
        return obs, total_reward, done, info
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        return self.env.close()


def create_mineclip_wrapper(env, task_id, model_path=None, variant="attn"):
    """
    ä¸ºç¯å¢ƒæ·»åŠ  MineCLIP å¥–åŠ±åŒ…è£…å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        env: MineDojo ç¯å¢ƒ
        task_id: ä»»åŠ¡ IDï¼ˆç”¨äºç”Ÿæˆä»»åŠ¡æè¿°ï¼‰
        model_path: MineCLIP æ¨¡å‹è·¯å¾„
        variant: MineCLIP å˜ä½“ ("attn" æˆ– "avg")
        
    Returns:
        åŒ…è£…åçš„ç¯å¢ƒ
    """
    # æ ¹æ®ä»»åŠ¡ ID ç”Ÿæˆä»»åŠ¡æè¿°
    task_prompts = {
        "harvest_1_log": "chop down a tree and collect one wood log",
        "harvest_1_paper": "collect one piece of paper",
        "hunt_1_cow": "hunt and kill one cow",
        # å¯ä»¥æ·»åŠ æ›´å¤šä»»åŠ¡
    }
    
    prompt = task_prompts.get(task_id, f"complete task {task_id}")
    
    return MineCLIPRewardWrapper(
        env,
        task_prompt=prompt,
        model_path=model_path,
        variant=variant,
        sparse_weight=10.0,
        mineclip_weight=0.1
    )

