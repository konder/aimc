#!/usr/bin/env python3
"""
CLIP4MC æ•°æ®å‡†å¤‡è„šæœ¬

å°†åˆ‡ç‰‡å¥½çš„è§†é¢‘å’Œæ–‡æœ¬è½¬æ¢ä¸ºå®˜æ–¹ CLIP4MC è®­ç»ƒæ‰€éœ€çš„æ ¼å¼ï¼š
- text_input.pkl: {'tokens': ndarray(77,)}
- video_input.pkl: è§†é¢‘å¸§ (N, H, W, C) å°ºå¯¸ 160x256
- size.json: [size_0, size_1, ..., size_N] æ¯å¸§çš„ç›®æ ‡å®ä½“å¤§å°æ¯”ä¾‹ (0-1)

ä½¿ç”¨æ–¹æ³•:
    python src/utils/prepare_clip4mc_data.py \
        --pairs-json data/raw_videos/clip4mc_youtube/text_video_pairs.json \
        --clips-dir data/raw_videos/clip4mc_youtube/clips \
        --output-dir /Users/nanzhang/clip4mc/processed_data \
        --num-frames 16
"""

import argparse
import json
import pickle
import logging
import sys
from pathlib import Path
from typing import Optional, List, Dict, Any
import random

import numpy as np

try:
    import cv2
    HAS_CV2 = True
except ImportError:
    HAS_CV2 = False
    print("âš ï¸ OpenCV æœªå®‰è£…ï¼Œå°è¯•ä½¿ç”¨ PIL + imageio")

try:
    import imageio
    HAS_IMAGEIO = True
except ImportError:
    HAS_IMAGEIO = False

try:
    from PIL import Image
    HAS_PIL = True
except ImportError:
    HAS_PIL = False

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def extract_frames_cv2(video_path: Path, num_frames: int = 16, frame_height: int = 160, frame_width: int = 256) -> Optional[np.ndarray]:
    """
    ä½¿ç”¨ OpenCV ä»è§†é¢‘ä¸­æå–å¸§
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        num_frames: æå–å¸§æ•°
        frame_height: å¸§é«˜åº¦ (CLIP4MC å®˜æ–¹: 160)
        frame_width: å¸§å®½åº¦ (CLIP4MC å®˜æ–¹: 256)
    
    Returns:
        frames: (N, H, W, C) numpy array, RGB æ ¼å¼ï¼Œæˆ– None å¦‚æœå¤±è´¥
    """
    cap = cv2.VideoCapture(str(video_path))
    
    if not cap.isOpened():
        logger.warning(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
        return None
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if total_frames == 0:
        logger.warning(f"è§†é¢‘æ²¡æœ‰å¸§: {video_path}")
        cap.release()
        return None
    
    # å‡åŒ€é‡‡æ ·å¸§ç´¢å¼•
    if total_frames >= num_frames:
        indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    else:
        # å¸§æ•°ä¸è¶³æ—¶ï¼Œé‡å¤æœ€åä¸€å¸§
        indices = list(range(total_frames)) + [total_frames - 1] * (num_frames - total_frames)
    
    frames = []
    for idx in indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        
        if not ret:
            # å¦‚æœè¯»å–å¤±è´¥ï¼Œä½¿ç”¨é»‘å¸§
            frame = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)
        else:
            # BGR -> RGB
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            # è°ƒæ•´å¤§å°åˆ° CLIP4MC å®˜æ–¹å°ºå¯¸ (W, H)
            frame = cv2.resize(frame, (frame_width, frame_height))
        
        frames.append(frame)
    
    cap.release()
    
    return np.array(frames, dtype=np.uint8)


def extract_frames_imageio(video_path: Path, num_frames: int = 16, frame_height: int = 160, frame_width: int = 256) -> Optional[np.ndarray]:
    """
    ä½¿ç”¨ imageio ä»è§†é¢‘ä¸­æå–å¸§
    """
    try:
        reader = imageio.get_reader(str(video_path))
        all_frames = []
        
        for frame in reader:
            all_frames.append(frame)
        
        reader.close()
        
        if len(all_frames) == 0:
            logger.warning(f"è§†é¢‘æ²¡æœ‰å¸§: {video_path}")
            return None
        
        total_frames = len(all_frames)
        
        # å‡åŒ€é‡‡æ ·
        if total_frames >= num_frames:
            indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
        else:
            indices = list(range(total_frames)) + [total_frames - 1] * (num_frames - total_frames)
        
        frames = []
        for idx in indices:
            frame = all_frames[idx]
            
            # ä½¿ç”¨ PIL è°ƒæ•´å¤§å°
            if HAS_PIL:
                pil_img = Image.fromarray(frame)
                pil_img = pil_img.resize((frame_width, frame_height), Image.BILINEAR)
                frame = np.array(pil_img)
            else:
                # ç®€å•çš„æœ€è¿‘é‚»ç¼©æ”¾
                h, w = frame.shape[:2]
                y_indices = np.linspace(0, h - 1, frame_height, dtype=int)
                x_indices = np.linspace(0, w - 1, frame_width, dtype=int)
                frame = frame[y_indices][:, x_indices]
            
            frames.append(frame)
        
        return np.array(frames, dtype=np.uint8)
    
    except Exception as e:
        logger.warning(f"imageio è¯»å–è§†é¢‘å¤±è´¥: {video_path}, é”™è¯¯: {e}")
        return None


def extract_frames(video_path: Path, num_frames: int = 16, frame_height: int = 160, frame_width: int = 256) -> Optional[np.ndarray]:
    """
    ä»è§†é¢‘ä¸­æå–å¸§ï¼Œè‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æ–¹æ³•
    
    CLIP4MC å®˜æ–¹å°ºå¯¸: 160x256 (H x W)
    """
    if HAS_CV2:
        return extract_frames_cv2(video_path, num_frames, frame_height, frame_width)
    elif HAS_IMAGEIO:
        return extract_frames_imageio(video_path, num_frames, frame_height, frame_width)
    else:
        logger.error("éœ€è¦å®‰è£… opencv-python æˆ– imageio: pip install opencv-python imageio")
        return None


def tokenize_text_clip(text: str, context_length: int = 77) -> np.ndarray:
    """
    ä½¿ç”¨ CLIP tokenizer å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        context_length: token åºåˆ—é•¿åº¦
    
    Returns:
        tokens: (context_length,) numpy array
    """
    try:
        import open_clip
        tokenizer = open_clip.get_tokenizer('ViT-B-16')
        tokens = tokenizer([text])
        return tokens[0].numpy()
    except ImportError:
        pass
    
    # Fallback: ç®€å•çš„å­—ç¬¦çº§ tokenization
    logger.warning("open_clip æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å• tokenization")
    
    # ç®€å•çš„å­—ç¬¦çº§ tokenization (ä»…ç”¨äºæµ‹è¯•)
    # ä½¿ç”¨ ASCII ç ä½œä¸º tokenï¼ŒåŠ ä¸Š SOS/EOS
    SOS_TOKEN = 49406  # CLIP çš„ <|startoftext|>
    EOS_TOKEN = 49407  # CLIP çš„ <|endoftext|>
    
    tokens = [SOS_TOKEN]
    
    for char in text.lower()[:context_length - 2]:
        # ç®€å•æ˜ å°„ï¼šç©ºæ ¼å’Œæ ‡ç‚¹ç¬¦å·æ˜ å°„åˆ°ç‰¹å®šèŒƒå›´
        if char == ' ':
            tokens.append(267)  # ç©ºæ ¼åœ¨ CLIP vocab ä¸­çš„å¤§è‡´ä½ç½®
        elif char.isalpha():
            tokens.append(ord(char) - ord('a') + 320)  # å­—æ¯æ˜ å°„
        elif char.isdigit():
            tokens.append(ord(char) - ord('0') + 410)  # æ•°å­—æ˜ å°„
        else:
            tokens.append(256)  # å…¶ä»–å­—ç¬¦
    
    tokens.append(EOS_TOKEN)
    
    # Padding
    while len(tokens) < context_length:
        tokens.append(0)
    
    return np.array(tokens[:context_length], dtype=np.int64)


def prepare_sample(
    pair: Dict[str, Any],
    clips_dir: Path,
    output_dir: Path,
    num_frames: int,
    frame_height: int,
    frame_width: int,
    sample_idx: int
) -> Optional[Path]:
    """
    å‡†å¤‡å•ä¸ªæ ·æœ¬ï¼ˆCLIP4MC å®˜æ–¹æ ¼å¼ï¼‰
    
    è¾“å‡ºæ ¼å¼:
    - text_input.pkl: {'tokens': ndarray(77,)}
    - video_input.pkl: ndarray(N, H, W, C) å°ºå¯¸ 160x256
    - size.json: [size_0, ..., size_N] æ¯å¸§çš„ç›®æ ‡å®ä½“å¤§å°æ¯”ä¾‹
    
    Returns:
        æ ·æœ¬è¾“å‡ºç›®å½•ï¼Œæˆ– None å¦‚æœå¤±è´¥
    """
    vid = pair['vid']
    transcript = pair['transcript']
    clip_path_str = pair['clip_path']
    
    # æ„å»ºå®é™…çš„ clip è·¯å¾„
    clip_filename = Path(clip_path_str).name
    clip_path = clips_dir / clip_filename
    
    if not clip_path.exists():
        logger.warning(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {clip_path}")
        return None
    
    # æå–è§†é¢‘å¸§ (CLIP4MC å®˜æ–¹å°ºå¯¸: 160x256)
    frames = extract_frames(clip_path, num_frames, frame_height, frame_width)
    
    if frames is None:
        return None
    
    # Tokenize æ–‡æœ¬
    tokens = tokenize_text_clip(transcript)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    sample_dir = output_dir / f"sample_{sample_idx:06d}_{vid}"
    sample_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ video_input.pkl (ç›´æ¥ä¿å­˜ ndarray)
    with open(sample_dir / "video_input.pkl", "wb") as f:
        pickle.dump(frames, f)
    
    # ä¿å­˜ text_input.pkl (å®˜æ–¹æ ¼å¼: {'tokens': array})
    with open(sample_dir / "text_input.pkl", "wb") as f:
        pickle.dump({'tokens': tokens}, f)
    
    # ä¿å­˜ size.json (å®˜æ–¹æ ¼å¼: æ¯å¸§çš„ç›®æ ‡å®ä½“å¤§å°æ¯”ä¾‹)
    # ä¼˜å…ˆä½¿ç”¨å®˜æ–¹æ•°æ®ä¸­çš„ sizeï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å ä½å€¼
    if 'size' in pair and isinstance(pair['size'], list) and len(pair['size']) > 0:
        # ä½¿ç”¨å®˜æ–¹æä¾›çš„ size å€¼
        size_values = pair['size']
        size_source = 'official'
        # ç¡®ä¿ size é•¿åº¦ä¸å¸§æ•°ä¸€è‡´
        if len(size_values) != num_frames:
            # é‡é‡‡æ · size å€¼ä»¥åŒ¹é…å¸§æ•°
            if len(size_values) >= num_frames:
                indices = np.linspace(0, len(size_values) - 1, num_frames, dtype=int)
                size_values = [size_values[i] for i in indices]
            else:
                # ä¸è¶³æ—¶é‡å¤æœ€åä¸€ä¸ªå€¼
                size_values = size_values + [size_values[-1]] * (num_frames - len(size_values))
    else:
        # æ²¡æœ‰å®˜æ–¹ sizeï¼Œä½¿ç”¨å ä½å€¼
        # size > 0.02 æ—¶ä¸è¿›è¡Œ swap
        size_values = [0.5] * num_frames
        size_source = 'placeholder'
    
    with open(sample_dir / "size.json", "w") as f:
        json.dump(size_values, f)
    
    # ä¿å­˜å…ƒæ•°æ® (ç”¨äºè°ƒè¯•)
    meta = {
        'vid': vid,
        'transcript': transcript,
        'clip_path': str(clip_path),
        'num_frames': frames.shape[0],
        'frame_size': [frame_height, frame_width],
        'size_source': size_source,
        'size_values': size_values[:3] if len(size_values) > 3 else size_values,  # åªä¿å­˜å‰å‡ ä¸ªç”¨äºè°ƒè¯•
    }
    with open(sample_dir / "meta.json", "w", encoding="utf-8") as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)
    
    return sample_dir


def main():
    parser = argparse.ArgumentParser(description="CLIP4MC æ•°æ®å‡†å¤‡")
    
    parser.add_argument(
        "--pairs-json",
        type=Path,
        default=Path("data/raw_videos/clip4mc_youtube/text_video_pairs.json"),
        help="text_video_pairs.json æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--clips-dir",
        type=Path,
        default=Path("data/raw_videos/clip4mc_youtube/clips"),
        help="è§†é¢‘åˆ‡ç‰‡ç›®å½•"
    )
    
    parser.add_argument(
        "--output-dir",
        type=Path,
        default=Path("/Users/nanzhang/clip4mc/processed_data"),
        help="è¾“å‡ºç›®å½• (CLIP4MC processed_data ç›®å½•)"
    )
    
    parser.add_argument(
        "--num-frames",
        type=int,
        default=16,
        help="æ¯ä¸ªè§†é¢‘æå–çš„å¸§æ•°"
    )
    
    parser.add_argument(
        "--frame-height",
        type=int,
        default=160,
        help="å¸§é«˜åº¦ (CLIP4MC å®˜æ–¹: 160)"
    )
    
    parser.add_argument(
        "--frame-width",
        type=int,
        default=256,
        help="å¸§å®½åº¦ (CLIP4MC å®˜æ–¹: 256)"
    )
    
    parser.add_argument(
        "--train-ratio",
        type=float,
        default=0.8,
        help="è®­ç»ƒé›†æ¯”ä¾‹ (ä»…å½“ --split-mode=random æ—¶ä½¿ç”¨)"
    )
    
    parser.add_argument(
        "--split-mode",
        type=str,
        choices=['random', 'all_train', 'all_test'],
        default='random',
        help="æ•°æ®é›†åˆ’åˆ†æ¨¡å¼: random=éšæœºåˆ’åˆ†, all_train=å…¨éƒ¨ä½œä¸ºè®­ç»ƒé›†, all_test=å…¨éƒ¨ä½œä¸ºæµ‹è¯•é›†"
    )
    
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­"
    )
    
    parser.add_argument(
        "--max-samples",
        type=int,
        default=None,
        help="æœ€å¤§æ ·æœ¬æ•° (ç”¨äºæµ‹è¯•)"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    if not HAS_CV2 and not HAS_IMAGEIO:
        logger.error("éœ€è¦å®‰è£… opencv-python æˆ– imageio")
        logger.error("  pip install opencv-python")
        logger.error("  æˆ–")
        logger.error("  pip install imageio imageio-ffmpeg")
        sys.exit(1)
    
    # åŠ è½½æ•°æ®
    if not args.pairs_json.exists():
        logger.error(f"pairs JSON æ–‡ä»¶ä¸å­˜åœ¨: {args.pairs_json}")
        sys.exit(1)
    
    with open(args.pairs_json, encoding="utf-8") as f:
        pairs = json.load(f)
    
    logger.info(f"åŠ è½½äº† {len(pairs)} ä¸ªæ–‡æœ¬-è§†é¢‘å¯¹")
    
    # é™åˆ¶æ ·æœ¬æ•°
    if args.max_samples and len(pairs) > args.max_samples:
        random.seed(args.seed)
        pairs = random.sample(pairs, args.max_samples)
        logger.info(f"é™åˆ¶ä¸º {args.max_samples} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºè¾“å‡ºç›®å½• (ç›´æ¥åœ¨ output_dir ä¸‹åˆ›å»ºæ ·æœ¬ï¼Œä¸å†æœ‰ samples å­ç›®å½•)
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # å¤„ç†æ ·æœ¬
    successful_dirs: List[str] = []
    failed_count = 0
    
    logger.info(f"\n{'='*60}")
    logger.info("å¼€å§‹å¤„ç†æ ·æœ¬ (CLIP4MC å®˜æ–¹æ ¼å¼)...")
    logger.info(f"  å¸§å°ºå¯¸: {args.frame_height}x{args.frame_width}")
    logger.info(f"  å¸§æ•°: {args.num_frames}")
    logger.info(f"{'='*60}\n")
    
    iterator = enumerate(pairs)
    if tqdm:
        iterator = tqdm(list(iterator), desc="ğŸ¬ å¤„ç†è§†é¢‘", unit="sample")
    
    for idx, pair in iterator:
        result = prepare_sample(
            pair=pair,
            clips_dir=args.clips_dir,
            output_dir=args.output_dir,
            num_frames=args.num_frames,
            frame_height=args.frame_height,
            frame_width=args.frame_width,
            sample_idx=idx
        )
        
        if result:
            successful_dirs.append(str(result))
        else:
            failed_count += 1
    
    logger.info(f"\nå¤„ç†å®Œæˆ: æˆåŠŸ {len(successful_dirs)}, å¤±è´¥ {failed_count}")
    
    if not successful_dirs:
        logger.error("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ ·æœ¬")
        sys.exit(1)
    
    # åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›† (CLIP4MC å®˜æ–¹æ ¼å¼)
    random.seed(args.seed)
    random.shuffle(successful_dirs)
    
    n = len(successful_dirs)
    n_test = max(1, int(n * 0.1))
    n_val = max(1, int(n * 0.1))
    n_train = n - n_test - n_val
    
    train_dirs = successful_dirs[:n_train]
    val_dirs = successful_dirs[n_train:n_train + n_val]
    test_dirs = successful_dirs[n_train + n_val:]
    
    logger.info(f"è®­ç»ƒé›†: {len(train_dirs)} ä¸ªæ ·æœ¬")
    logger.info(f"éªŒè¯é›†: {len(val_dirs)} ä¸ªæ ·æœ¬")
    logger.info(f"æµ‹è¯•é›†: {len(test_dirs)} ä¸ªæ ·æœ¬")
    
    # ç”Ÿæˆ dataset_info.json (CLIP4MC å®˜æ–¹æ ¼å¼)
    dataset_info = {
        "train": train_dirs,
        "val": val_dirs,
        "test": test_dirs
    }
    
    dataset_info_path = args.output_dir / "dataset_info.json"
    with open(dataset_info_path, "w", encoding="utf-8") as f:
        json.dump(dataset_info, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nâœ“ dataset_info.json å·²ä¿å­˜: {dataset_info_path}")
    
    # æ‰“å°ä½¿ç”¨è¯´æ˜
    logger.info(f"\n{'='*60}")
    logger.info("æ•°æ®å‡†å¤‡å®Œæˆï¼ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼€å§‹è®­ç»ƒ:")
    logger.info(f"{'='*60}")
    logger.info(f"\ncd /Users/nanzhang/clip4mc")
    logger.info(f"conda activate minedojo-x86")
    logger.info(f"python train_ddp_clip4mc.py --batch_size 2 --epochs 5")


if __name__ == "__main__":
    main()

