#!/usr/bin/env python3
"""
CLIP4MC æ•°æ®å‡†å¤‡è„šæœ¬ - å¹¶è¡Œç‰ˆæœ¬

æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œã€æ–­ç‚¹ç»­ä¼ ã€è¿›åº¦ç›‘æ§

ä½¿ç”¨æ–¹æ³•:
    python src/utils/prepare_clip4mc_data_parallel.py \
        --pairs-json data/train_pairs.json \
        --clips-dir data/clips \
        --output-dir /path/to/processed_data \
        --num-workers 32 \
        --split-mode all_train
"""

import argparse
import json
import pickle
import logging
import sys
from pathlib import Path
from typing import Optional, List, Dict, Any, Tuple
import random
import time
from multiprocessing import Pool, Manager, cpu_count
from functools import partial

import numpy as np

try:
    import cv2
    HAS_CV2 = True
except ImportError:
    HAS_CV2 = False
    print("âš ï¸ OpenCV æœªå®‰è£…")
    sys.exit(1)

try:
    from tqdm import tqdm
    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(processName)s] - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def extract_frames_fast(video_path: Path, num_frames: int = 16, 
                        frame_height: int = 160, frame_width: int = 256) -> Optional[np.ndarray]:
    """
    å¿«é€Ÿæå–è§†é¢‘å¸§ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    ä¼˜åŒ–ç‚¹:
    1. ä½¿ç”¨ cv2.CAP_PROP_POS_FRAMES è·³å¸§
    2. é¿å…é‡å¤è¯»å–
    3. é¢„åˆ†é…æ•°ç»„
    """
    cap = cv2.VideoCapture(str(video_path), cv2.CAP_FFMPEG)
    
    if not cap.isOpened():
        return None
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if total_frames == 0:
        cap.release()
        return None
    
    # å‡åŒ€é‡‡æ ·å¸§ç´¢å¼•
    if total_frames >= num_frames:
        indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    else:
        indices = list(range(total_frames)) + [total_frames - 1] * (num_frames - total_frames)
    
    # é¢„åˆ†é…æ•°ç»„
    frames = np.zeros((num_frames, frame_height, frame_width, 3), dtype=np.uint8)
    
    for i, idx in enumerate(indices):
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        
        if ret:
            # BGR -> RGB + Resize (åˆå¹¶æ“ä½œ)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = cv2.resize(frame, (frame_width, frame_height), interpolation=cv2.INTER_LINEAR)
            frames[i] = frame
        # å¤±è´¥æ—¶ä½¿ç”¨é›¶å¸§ï¼ˆå·²é¢„åˆ†é…ï¼‰
    
    cap.release()
    
    return frames


def tokenize_text_clip_cached(text: str, tokenizer=None) -> np.ndarray:
    """
    ä½¿ç”¨ç¼“å­˜çš„ tokenizerï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
    """
    if tokenizer is None:
        try:
            import open_clip
            tokenizer = open_clip.get_tokenizer('ViT-B-16')
        except ImportError:
            # Fallback
            logger.warning("open_clip æœªå®‰è£…ï¼Œä½¿ç”¨ç®€å• tokenization")
            return simple_tokenize(text)
    
    tokens = tokenizer([text])
    return tokens[0].numpy()


def simple_tokenize(text: str, context_length: int = 77) -> np.ndarray:
    """ç®€å• tokenization fallback"""
    SOS_TOKEN, EOS_TOKEN = 49406, 49407
    tokens = [SOS_TOKEN]
    
    for char in text.lower()[:context_length - 2]:
        if char == ' ':
            tokens.append(267)
        elif char.isalpha():
            tokens.append(ord(char) - ord('a') + 320)
        elif char.isdigit():
            tokens.append(ord(char) - ord('0') + 410)
        else:
            tokens.append(256)
    
    tokens.append(EOS_TOKEN)
    while len(tokens) < context_length:
        tokens.append(0)
    
    return np.array(tokens[:context_length], dtype=np.int64)


def process_single_sample(
    task: Tuple[int, Dict[str, Any]],
    clips_dir: Path,
    output_dir: Path,
    num_frames: int,
    frame_height: int,
    frame_width: int,
    tokenizer=None
) -> Tuple[bool, Optional[str], Optional[str]]:
    """
    å¤„ç†å•ä¸ªæ ·æœ¬ï¼ˆworker å‡½æ•°ï¼‰
    
    Returns:
        (success, sample_dir, error_msg)
    """
    idx, pair = task
    
    try:
        vid = pair['vid']
        transcript = pair.get('transcript', pair.get('transcript clip', ''))
        clip_path_str = pair['clip_path']
        
        # æ„å»º clip è·¯å¾„
        clip_filename = Path(clip_path_str).name
        clip_path = clips_dir / clip_filename
        
        if not clip_path.exists():
            return False, None, f"è§†é¢‘ä¸å­˜åœ¨: {clip_path}"
        
        # æå–è§†é¢‘å¸§
        frames = extract_frames_fast(clip_path, num_frames, frame_height, frame_width)
        
        if frames is None:
            return False, None, f"è§†é¢‘è§£ç å¤±è´¥: {clip_path}"
        
        # Tokenize æ–‡æœ¬
        tokens = tokenize_text_clip_cached(transcript, tokenizer)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        sample_dir = output_dir / f"sample_{idx:06d}_{vid}"
        sample_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        with open(sample_dir / "video_input.pkl", "wb") as f:
            pickle.dump(frames, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        with open(sample_dir / "text_input.pkl", "wb") as f:
            pickle.dump({'tokens': tokens}, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        # ä¿å­˜ size
        if 'size' in pair and isinstance(pair['size'], list) and len(pair['size']) > 0:
            size_values = pair['size']
            if len(size_values) != num_frames:
                if len(size_values) >= num_frames:
                    indices = np.linspace(0, len(size_values) - 1, num_frames, dtype=int)
                    size_values = [size_values[i] for i in indices]
                else:
                    size_values = size_values + [size_values[-1]] * (num_frames - len(size_values))
        else:
            size_values = [0.5] * num_frames
        
        with open(sample_dir / "size.json", "w") as f:
            json.dump(size_values, f)
        
        return True, str(sample_dir), None
    
    except Exception as e:
        return False, None, f"å¤„ç†å¼‚å¸¸: {str(e)}"


def worker_init():
    """Worker è¿›ç¨‹åˆå§‹åŒ–ï¼ˆåŠ è½½ tokenizerï¼‰"""
    global _tokenizer
    try:
        import open_clip
        _tokenizer = open_clip.get_tokenizer('ViT-B-16')
    except:
        _tokenizer = None


def process_batch_parallel(
    pairs: List[Dict[str, Any]],
    clips_dir: Path,
    output_dir: Path,
    num_workers: int,
    num_frames: int = 16,
    frame_height: int = 160,
    frame_width: int = 256,
    resume_from: Optional[Path] = None
) -> List[str]:
    """
    å¹¶è¡Œå¤„ç†è§†é¢‘æ‰¹æ¬¡
    
    Args:
        pairs: è§†é¢‘å¯¹åˆ—è¡¨
        num_workers: å¹¶è¡Œè¿›ç¨‹æ•°
        resume_from: æ–­ç‚¹ç»­ä¼ æ–‡ä»¶è·¯å¾„
    
    Returns:
        successful_dirs: æˆåŠŸå¤„ç†çš„æ ·æœ¬ç›®å½•åˆ—è¡¨
    """
    # åŠ è½½å·²å¤„ç†çš„æ ·æœ¬ï¼ˆæ–­ç‚¹ç»­ä¼ ï¼‰
    processed_samples = set()
    if resume_from and resume_from.exists():
        with open(resume_from) as f:
            checkpoint = json.load(f)
            processed_samples = set(checkpoint.get('processed_indices', []))
        logger.info(f"ä»æ–­ç‚¹æ¢å¤: å·²å¤„ç† {len(processed_samples)} ä¸ªæ ·æœ¬")
    
    # è¿‡æ»¤æœªå¤„ç†çš„æ ·æœ¬
    tasks = [(i, pair) for i, pair in enumerate(pairs) if i not in processed_samples]
    
    if not tasks:
        logger.info("æ‰€æœ‰æ ·æœ¬å·²å¤„ç†å®Œæˆ")
        return []
    
    logger.info(f"å¾…å¤„ç†æ ·æœ¬: {len(tasks)}")
    
    # åˆ›å»ºè¿›ç¨‹æ± 
    process_func = partial(
        process_single_sample,
        clips_dir=clips_dir,
        output_dir=output_dir,
        num_frames=num_frames,
        frame_height=frame_height,
        frame_width=frame_width
    )
    
    successful_dirs = []
    failed_count = 0
    
    # ä½¿ç”¨ imap_unordered æ”¯æŒè¿›åº¦æ¡
    with Pool(num_workers, initializer=worker_init) as pool:
        if HAS_TQDM:
            results = tqdm(
                pool.imap_unordered(process_func, tasks, chunksize=10),
                total=len(tasks),
                desc="ğŸ¬ å¹¶è¡Œå¤„ç†",
                unit="video"
            )
        else:
            results = pool.imap_unordered(process_func, tasks, chunksize=10)
        
        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        checkpoint_interval = 1000
        processed_count = len(processed_samples)
        
        for success, sample_dir, error_msg in results:
            if success:
                successful_dirs.append(sample_dir)
                processed_count += 1
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                if resume_from and processed_count % checkpoint_interval == 0:
                    with open(resume_from, 'w') as f:
                        json.dump({
                            'processed_indices': list(processed_samples) + list(range(len(successful_dirs))),
                            'timestamp': time.time()
                        }, f)
            else:
                failed_count += 1
                if error_msg and failed_count <= 10:  # åªæ‰“å°å‰ 10 ä¸ªé”™è¯¯
                    logger.warning(error_msg)
    
    logger.info(f"å¤„ç†å®Œæˆ: æˆåŠŸ {len(successful_dirs)}, å¤±è´¥ {failed_count}")
    
    return successful_dirs


def main():
    parser = argparse.ArgumentParser(description="CLIP4MC æ•°æ®å‡†å¤‡ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰")
    
    parser.add_argument("--pairs-json", type=Path, required=True, help="pairs JSON æ–‡ä»¶")
    parser.add_argument("--clips-dir", type=Path, required=True, help="è§†é¢‘åˆ‡ç‰‡ç›®å½•")
    parser.add_argument("--output-dir", type=Path, required=True, help="è¾“å‡ºç›®å½•")
    
    parser.add_argument("--num-workers", type=int, default=cpu_count(), 
                       help=f"å¹¶è¡Œè¿›ç¨‹æ•° (é»˜è®¤: {cpu_count()})")
    parser.add_argument("--num-frames", type=int, default=16, help="æ¯ä¸ªè§†é¢‘æå–çš„å¸§æ•°")
    parser.add_argument("--frame-height", type=int, default=160, help="å¸§é«˜åº¦")
    parser.add_argument("--frame-width", type=int, default=256, help="å¸§å®½åº¦")
    
    parser.add_argument("--split-mode", type=str, choices=['random', 'all_train', 'all_test'],
                       default='random', help="æ•°æ®é›†åˆ’åˆ†æ¨¡å¼")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    parser.add_argument("--max-samples", type=int, default=None, help="æœ€å¤§æ ·æœ¬æ•°ï¼ˆæµ‹è¯•ç”¨ï¼‰")
    
    parser.add_argument("--resume", action='store_true', help="ä»æ–­ç‚¹ç»­ä¼ ")
    parser.add_argument("--checkpoint-file", type=Path, 
                       default=Path("checkpoint.json"), help="æ£€æŸ¥ç‚¹æ–‡ä»¶")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    if not HAS_CV2:
        logger.error("éœ€è¦å®‰è£… opencv-python")
        sys.exit(1)
    
    # åŠ è½½æ•°æ®
    if not args.pairs_json.exists():
        logger.error(f"pairs JSON æ–‡ä»¶ä¸å­˜åœ¨: {args.pairs_json}")
        sys.exit(1)
    
    with open(args.pairs_json, encoding="utf-8") as f:
        pairs = json.load(f)
    
    logger.info(f"åŠ è½½äº† {len(pairs)} ä¸ªæ–‡æœ¬-è§†é¢‘å¯¹")
    
    # é™åˆ¶æ ·æœ¬æ•°ï¼ˆæµ‹è¯•ç”¨ï¼‰
    if args.max_samples and len(pairs) > args.max_samples:
        random.seed(args.seed)
        pairs = random.sample(pairs, args.max_samples)
        logger.info(f"é™åˆ¶ä¸º {args.max_samples} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info(f"\n{'='*60}")
    logger.info(f"å¹¶è¡Œå¤„ç†é…ç½®:")
    logger.info(f"  è¿›ç¨‹æ•°: {args.num_workers}")
    logger.info(f"  å¸§å°ºå¯¸: {args.frame_height}x{args.frame_width}")
    logger.info(f"  å¸§æ•°: {args.num_frames}")
    logger.info(f"  æ–­ç‚¹ç»­ä¼ : {'å¼€å¯' if args.resume else 'å…³é—­'}")
    logger.info(f"{'='*60}\n")
    
    # å¼€å§‹å¤„ç†
    start_time = time.time()
    
    successful_dirs = process_batch_parallel(
        pairs=pairs,
        clips_dir=args.clips_dir,
        output_dir=args.output_dir,
        num_workers=args.num_workers,
        num_frames=args.num_frames,
        frame_height=args.frame_height,
        frame_width=args.frame_width,
        resume_from=args.checkpoint_file if args.resume else None
    )
    
    elapsed = time.time() - start_time
    
    if not successful_dirs:
        logger.error("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•æ ·æœ¬")
        sys.exit(1)
    
    logger.info(f"\næ€»è€—æ—¶: {elapsed/3600:.2f} å°æ—¶")
    logger.info(f"å¹³å‡é€Ÿåº¦: {len(successful_dirs)/elapsed:.2f} è§†é¢‘/ç§’")
    
    # åˆ’åˆ†æ•°æ®é›†
    n = len(successful_dirs)
    
    if args.split_mode == 'all_train':
        train_dirs = successful_dirs
        val_dirs = []
        test_dirs = []
        logger.info(f"[all_train] å…¨éƒ¨ {n} ä¸ªæ ·æœ¬ä½œä¸ºè®­ç»ƒé›†")
    elif args.split_mode == 'all_test':
        train_dirs = []
        val_dirs = []
        test_dirs = successful_dirs
        logger.info(f"[all_test] å…¨éƒ¨ {n} ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•é›†")
    else:
        random.seed(args.seed)
        random.shuffle(successful_dirs)
        
        n_test = max(1, int(n * 0.1))
        n_val = max(1, int(n * 0.1))
        n_train = n - n_test - n_val
        
        train_dirs = successful_dirs[:n_train]
        val_dirs = successful_dirs[n_train:n_train + n_val]
        test_dirs = successful_dirs[n_train + n_val:]
        
        logger.info(f"[random] éšæœºåˆ’åˆ†:")
    
    logger.info(f"  è®­ç»ƒé›†: {len(train_dirs)}")
    logger.info(f"  éªŒè¯é›†: {len(val_dirs)}")
    logger.info(f"  æµ‹è¯•é›†: {len(test_dirs)}")
    
    # ç”Ÿæˆ dataset_info.json
    dataset_info = {
        "train": train_dirs,
        "val": val_dirs,
        "test": test_dirs
    }
    
    dataset_info_path = args.output_dir / "dataset_info.json"
    with open(dataset_info_path, "w", encoding="utf-8") as f:
        json.dump(dataset_info, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nâœ“ dataset_info.json å·²ä¿å­˜: {dataset_info_path}")


if __name__ == "__main__":
    main()

