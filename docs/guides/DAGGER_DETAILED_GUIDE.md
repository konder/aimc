# DAgger (Dataset Aggregation) è¯¦ç»†æŒ‡å—

> **DAgger**: è§£å†³æ¨¡ä»¿å­¦ä¹ ä¸­"åˆ†å¸ƒåç§»"é—®é¢˜çš„è¿­ä»£å¼æ•°æ®æ”¶é›†ç®—æ³•

---

## ğŸ¯ **ä»€ä¹ˆæ˜¯DAggerï¼Ÿ**

**DAgger** (Dataset Aggregation) æ˜¯ç”±Rossç­‰äººåœ¨2011å¹´æå‡ºçš„æ”¹è¿›ç‰ˆè¡Œä¸ºå…‹éš†ç®—æ³•ã€‚

### **æ ¸å¿ƒæ€æƒ³**

ä¼ ç»Ÿè¡Œä¸ºå…‹éš†(BC)çš„é—®é¢˜ï¼š
```
ä¸“å®¶æ¼”ç¤º: sâ‚€ â†’ sâ‚ â†’ sâ‚‚ â†’ sâ‚ƒ (ä¸“å®¶è½¨è¿¹)
å­¦ä¹ ç­–ç•¥: sâ‚€ â†’ sâ‚' â†’ sâ‚‚'' â†’ sâ‚ƒ''' (ç•¥æœ‰åå·®)
         â†‘    â†‘     â†‘      â†‘
      ç›¸åŒ  ç•¥å  æ›´å   å®Œå…¨åç¦»ï¼
```

**é—®é¢˜**: ä¸€æ—¦åç¦»ä¸“å®¶æ¼”ç¤ºï¼Œç­–ç•¥ä¼šè¶Šæ¥è¶Šå·®ï¼ˆ**åˆ†å¸ƒåç§»é—®é¢˜**ï¼‰

**DAggerè§£å†³æ–¹æ¡ˆ**: åœ¨ç­–ç•¥è®¿é—®çš„æ–°çŠ¶æ€ä¸Šæ”¶é›†ä¸“å®¶æ ‡æ³¨ï¼

---

## ğŸ”„ **DAggerç®—æ³•æµç¨‹**

### **å®Œæ•´æµç¨‹å›¾**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ1: åˆå§‹è®­ç»ƒ                         â”‚
â”‚  ç”¨ä¸“å®¶æ¼”ç¤ºDâ‚€è®­ç»ƒåˆå§‹ç­–ç•¥Ï€â‚             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ2: ç­–ç•¥æ‰§è¡Œ (ç¬¬iè½®)                 â”‚
â”‚  è¿è¡Œå½“å‰ç­–ç•¥Ï€áµ¢ï¼Œæ”¶é›†æ–°è½¨è¿¹              â”‚
â”‚  è®°å½•è®¿é—®çš„çŠ¶æ€ Sáµ¢ = {sâ‚, sâ‚‚, ...}      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ3: ä¸“å®¶æ ‡æ³¨                         â”‚
â”‚  äººå·¥/ä¸“å®¶å¯¹Sáµ¢ä¸­çš„çŠ¶æ€æ ‡æ³¨æ­£ç¡®åŠ¨ä½œ       â”‚
â”‚  Dáµ¢ = {(s, a*) | s âˆˆ Sáµ¢}               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  é˜¶æ®µ4: æ•°æ®èšåˆ                         â”‚
â”‚  D_all = Dâ‚€ âˆª Dâ‚ âˆª ... âˆª Dáµ¢           â”‚
â”‚  ç”¨æ‰€æœ‰æ•°æ®é‡æ–°è®­ç»ƒç­–ç•¥Ï€áµ¢â‚Šâ‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
         é‡å¤2-4ï¼Œç›´åˆ°æ”¶æ•›
```

### **ä¼ªä»£ç **

```python
def dagger(expert, initial_demonstrations, num_iterations):
    # åˆå§‹åŒ–
    D = initial_demonstrations  # ä¸“å®¶æ¼”ç¤ºæ•°æ®é›†
    Ï€ = train_policy(D)         # åˆå§‹ç­–ç•¥
    
    for i in range(num_iterations):
        # 1. ç”¨å½“å‰ç­–ç•¥æ”¶é›†è½¨è¿¹
        trajectories = []
        for episode in range(num_episodes):
            states = rollout_policy(Ï€)  # è¿è¡Œç­–ç•¥
            trajectories.extend(states)
        
        # 2. ä¸“å®¶æ ‡æ³¨
        labeled_data = []
        for state in trajectories:
            expert_action = expert.get_action(state)  # äººå·¥æ ‡æ³¨
            labeled_data.append((state, expert_action))
        
        # 3. èšåˆæ•°æ®
        D = D + labeled_data
        
        # 4. é‡æ–°è®­ç»ƒ
        Ï€ = train_policy(D)
    
    return Ï€
```

---

## ğŸ”¬ **ä¸ºä»€ä¹ˆDAggeræœ‰æ•ˆï¼Ÿ**

### **é—®é¢˜ï¼šä¼ ç»ŸBCçš„åˆ†å¸ƒåç§»**

**è®­ç»ƒåˆ†å¸ƒ** vs **æµ‹è¯•åˆ†å¸ƒ**:

```python
# BCè®­ç»ƒæ—¶
è®­ç»ƒçŠ¶æ€åˆ†å¸ƒ = ä¸“å®¶è®¿é—®çš„çŠ¶æ€
P_train(s) = åªåŒ…å«ä¸“å®¶è½¨è¿¹ä¸Šçš„çŠ¶æ€

# BCæµ‹è¯•æ—¶
æµ‹è¯•çŠ¶æ€åˆ†å¸ƒ = å­¦ä¹ ç­–ç•¥è®¿é—®çš„çŠ¶æ€
P_test(s) = åŒ…å«å­¦ä¹ ç­–ç•¥åç¦»åçš„çŠ¶æ€

# é—®é¢˜: P_train â‰  P_test âš ï¸
```

**ç»“æœ**: ç­–ç•¥åœ¨è®­ç»ƒæ—¶è§è¿‡çš„çŠ¶æ€è¡¨ç°å¥½ï¼Œä½†åç¦»åæ²¡è§è¿‡æ–°çŠ¶æ€ï¼Œè¡¨ç°å´©æºƒï¼

### **DAggerçš„è§£å†³**

```python
# DAggeræ¯è½®è¿­ä»£
P_train(s) é€æ¸åŒ…å«ç­–ç•¥Ï€áµ¢è®¿é—®çš„çŠ¶æ€

# ç»è¿‡å¤šè½®å
P_train(s) â‰ˆ P_test(s)  âœ…

# ç»“æœ: ç­–ç•¥åœ¨è‡ªå·±è®¿é—®çš„çŠ¶æ€ä¸Šä¹Ÿæœ‰è®­ç»ƒæ•°æ®ï¼
```

---

## ğŸ“Š **DAgger vs BC å¯¹æ¯”**

### **è®­ç»ƒè¿‡ç¨‹å¯¹æ¯”**

| æ–¹é¢ | ä¼ ç»ŸBC | DAgger |
|------|--------|--------|
| æ•°æ®æ”¶é›† | ä¸€æ¬¡æ€§ï¼ˆä¸“å®¶æ¼”ç¤ºï¼‰ | è¿­ä»£å¼ï¼ˆå¤šè½®æ”¶é›†ï¼‰|
| æ•°æ®åˆ†å¸ƒ | åªæœ‰ä¸“å®¶çŠ¶æ€ | ä¸“å®¶+ç­–ç•¥çŠ¶æ€ |
| æ ‡æ³¨æˆæœ¬ | ä½ï¼ˆä¸€æ¬¡ï¼‰ | ä¸­ï¼ˆå¤šæ¬¡ï¼Œä½†æ¯æ¬¡å°‘é‡ï¼‰|
| é²æ£’æ€§ | å·®ï¼ˆåç¦»åå´©æºƒï¼‰ | å¥½ï¼ˆè§è¿‡åç¦»çŠ¶æ€ï¼‰|
| æœ€ç»ˆæ€§èƒ½ | ä¸­ | é«˜ |

### **æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹**

**ä»»åŠ¡**: Minecraftç æ ‘ï¼Œ200æ­¥

| ç®—æ³• | åˆå§‹æˆåŠŸç‡ | 10è½®åæˆåŠŸç‡ | æ•°æ®é‡ | æ ‡æ³¨æˆæœ¬ |
|------|-----------|-------------|--------|---------|
| BC | 60% | 60% | 5K | 1å°æ—¶ï¼ˆä¸€æ¬¡ï¼‰|
| DAgger | 60% | **90%** | 15K | 3å°æ—¶ï¼ˆåˆ†3æ¬¡ï¼‰|
| BC + PPO | 60% â†’ 85% | 85% | 5K | 1å°æ—¶ |

---

## ğŸ› ï¸ **åœ¨Minecraftç æ ‘ä¸­å®ç°DAgger**

### **å®Œæ•´å®ç°æ–¹æ¡ˆ**

#### **ç¬¬1è½®: åˆå§‹BCè®­ç»ƒ**

```python
# 1. æ”¶é›†åˆå§‹ä¸“å®¶æ¼”ç¤ºï¼ˆæ‰‹åŠ¨å½•åˆ¶ï¼‰
æ¼”ç¤ºæ•°é‡: 10æ¬¡
æ•°æ®é‡: ~4000å¸§
æ—¶é—´: 30åˆ†é’Ÿ

# 2. è®­ç»ƒåˆå§‹ç­–ç•¥Ï€â‚
python src/training/train_bc.py \
  --data data/expert_demos/round_0.pkl \
  --output checkpoints/dagger_round_1.zip

# 3. è¯„ä¼°
æˆåŠŸç‡: ~60%
ä¸»è¦å¤±è´¥: åç¦»æ ‘æœ¨ã€å¡åœ¨åœ°å½¢
```

#### **ç¬¬2è½®: ç¬¬ä¸€æ¬¡DAggerè¿­ä»£**

```python
# 1. è¿è¡Œç­–ç•¥Ï€â‚æ”¶é›†æ–°çŠ¶æ€
python tools/run_policy_collect_states.py \
  --model checkpoints/dagger_round_1.zip \
  --episodes 20 \
  --output data/policy_states/round_1/

# è¾“å‡º: 
# - 20ä¸ªepisodeçš„çŠ¶æ€åºåˆ—
# - åŒ…å«å¤±è´¥åœºæ™¯ï¼ˆåç¦»ã€å¡ä½ç­‰ï¼‰

# 2. äººå·¥æ ‡æ³¨ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
python tools/label_states.py \
  --states data/policy_states/round_1/ \
  --output data/expert_labels/round_1.pkl

# äº¤äº’å¼æ ‡æ³¨ç•Œé¢:
# æ˜¾ç¤ºçŠ¶æ€ â†’ ä½ ç»™å‡ºæ­£ç¡®åŠ¨ä½œ â†’ ä¿å­˜æ ‡æ³¨
# é‡ç‚¹æ ‡æ³¨: å¤±è´¥/åç¦»çš„å…³é”®æ—¶åˆ»

# æ ‡æ³¨é‡: ~500-1000ä¸ªå…³é”®çŠ¶æ€
# æ—¶é—´: 30-40åˆ†é’Ÿ

# 3. èšåˆæ•°æ®
Dâ‚ = Dâ‚€ âˆª æ–°æ ‡æ³¨æ•°æ®

# 4. é‡æ–°è®­ç»ƒÏ€â‚‚
python src/training/train_bc.py \
  --data data/dagger_combined/round_1.pkl \
  --output checkpoints/dagger_round_2.zip

# 5. è¯„ä¼°
æˆåŠŸç‡: ~75% (+15%æå‡ï¼)
```

#### **ç¬¬3è½®: ç¬¬äºŒæ¬¡DAggerè¿­ä»£**

```python
# é‡å¤ç›¸åŒæµç¨‹
è¿è¡ŒÏ€â‚‚ â†’ æ ‡æ³¨æ–°å¤±è´¥åœºæ™¯ â†’ èšåˆæ•°æ® â†’ è®­ç»ƒÏ€â‚ƒ

æˆåŠŸç‡: ~85% (+10%æå‡)
```

#### **ç¬¬4-5è½®: ç»§ç»­è¿­ä»£ç›´åˆ°æ”¶æ•›**

```python
æˆåŠŸç‡æ›²çº¿:
60% â†’ 75% â†’ 85% â†’ 90% â†’ 92% (æ”¶æ•›)
```

---

## ğŸ’» **ä»£ç å®ç°**

### **1. ç­–ç•¥è¿è¡Œå¹¶æ”¶é›†çŠ¶æ€**

```python
# tools/run_policy_collect_states.py

import gym
import minedojo
import numpy as np
from stable_baselines3 import PPO

def collect_policy_states(model_path, num_episodes, output_dir):
    """è¿è¡Œç­–ç•¥å¹¶æ”¶é›†è®¿é—®çš„çŠ¶æ€"""
    
    # åŠ è½½ç­–ç•¥
    policy = PPO.load(model_path)
    
    # åˆ›å»ºç¯å¢ƒ
    env = minedojo.make(task_id="harvest_1_log")
    
    all_states = []
    
    for ep in range(num_episodes):
        obs = env.reset()
        done = False
        episode_states = []
        
        while not done:
            # ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
            action, _ = policy.predict(obs, deterministic=False)
            
            # ä¿å­˜å½“å‰çŠ¶æ€
            episode_states.append({
                'observation': obs.copy(),
                'step': len(episode_states),
                'episode': ep
            })
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, info = env.step(action)
        
        # ä¿å­˜episode
        episode_success = info.get('success', False)
        np.save(
            f"{output_dir}/episode_{ep}_success_{episode_success}.npy",
            episode_states
        )
        
        all_states.extend(episode_states)
        print(f"Episode {ep}: {len(episode_states)} states, success={episode_success}")
    
    env.close()
    return all_states
```

### **2. äº¤äº’å¼çŠ¶æ€æ ‡æ³¨å·¥å…·**

```python
# tools/label_states.py

import cv2
import numpy as np
from collections import deque

class StateLabeler:
    """äº¤äº’å¼çŠ¶æ€æ ‡æ³¨å·¥å…·"""
    
    def __init__(self):
        self.action_mapping = {
            'w': [1, 0, 0, 12, 12, 0, 0, 0],  # å‰è¿›
            's': [2, 0, 0, 12, 12, 0, 0, 0],  # åé€€
            'a': [0, 1, 0, 12, 12, 0, 0, 0],  # å·¦ç§»
            'd': [0, 2, 0, 12, 12, 0, 0, 0],  # å³ç§»
            'f': [0, 0, 0, 12, 12, 3, 0, 0],  # æ”»å‡»
            'i': [0, 0, 0, 8, 12, 0, 0, 0],   # å‘ä¸Šçœ‹
            'k': [0, 0, 0, 16, 12, 0, 0, 0],  # å‘ä¸‹çœ‹
            'j': [0, 0, 0, 12, 8, 0, 0, 0],   # å‘å·¦çœ‹
            'l': [0, 0, 0, 12, 16, 0, 0, 0],  # å‘å³çœ‹
            'n': None,  # è·³è¿‡æ­¤çŠ¶æ€
        }
    
    def label_episode(self, states_file, output_file):
        """æ ‡æ³¨ä¸€ä¸ªepisodeçš„å…³é”®çŠ¶æ€"""
        
        # åŠ è½½çŠ¶æ€
        states = np.load(states_file, allow_pickle=True)
        
        labeled_data = []
        current_idx = 0
        
        print("\n" + "="*60)
        print("çŠ¶æ€æ ‡æ³¨å·¥å…·")
        print("="*60)
        print("æ§åˆ¶:")
        print("  WASD - ç§»åŠ¨")
        print("  IJKL - è§†è§’")
        print("  F - æ”»å‡»")
        print("  N - è·³è¿‡æ­¤çŠ¶æ€")
        print("  Q - å®Œæˆæ ‡æ³¨")
        print("="*60)
        
        while current_idx < len(states):
            state = states[current_idx]
            obs = state['observation']
            
            # æ˜¾ç¤ºå½“å‰çŠ¶æ€
            display_img = cv2.cvtColor(
                obs.transpose(1, 2, 0), 
                cv2.COLOR_RGB2BGR
            )
            display_img = cv2.resize(display_img, (640, 480))
            
            # æ·»åŠ ä¿¡æ¯
            info_text = f"Episode step: {state['step']} | Total: {current_idx}/{len(states)}"
            cv2.putText(display_img, info_text, (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            
            cv2.imshow('State Labeling', display_img)
            
            # ç­‰å¾…æ ‡æ³¨
            key = chr(cv2.waitKey(0) & 0xFF)
            
            if key == 'q':
                break
            elif key in self.action_mapping:
                action = self.action_mapping[key]
                if action is not None:
                    labeled_data.append({
                        'observation': obs,
                        'action': np.array(action),
                        'step': state['step'],
                        'episode': state['episode']
                    })
                    print(f"  âœ“ å·²æ ‡æ³¨: {key} â†’ {action}")
                current_idx += 1
            else:
                print(f"  âš ï¸ æœªçŸ¥æŒ‰é”®: {key}")
        
        # ä¿å­˜æ ‡æ³¨æ•°æ®
        np.save(output_file, labeled_data)
        print(f"\nâœ“ å·²ä¿å­˜ {len(labeled_data)} ä¸ªæ ‡æ³¨")
        
        cv2.destroyAllWindows()
        return labeled_data
```

### **3. DAggerä¸»å¾ªç¯**

```python
# src/training/train_dagger.py

def dagger_training(
    initial_data_path,
    num_iterations=5,
    episodes_per_iteration=20,
    bc_epochs=30
):
    """DAggerè®­ç»ƒä¸»å¾ªç¯"""
    
    # åˆå§‹åŒ–
    all_data = load_data(initial_data_path)
    
    for iteration in range(num_iterations):
        print(f"\n{'='*60}")
        print(f"DAgger Iteration {iteration + 1}/{num_iterations}")
        print(f"{'='*60}")
        
        # 1. è®­ç»ƒå½“å‰ç­–ç•¥
        model_path = f"checkpoints/dagger_iter_{iteration}.zip"
        train_bc(
            data=all_data,
            output=model_path,
            epochs=bc_epochs
        )
        
        # 2. è¯„ä¼°å½“å‰ç­–ç•¥
        success_rate = evaluate_policy(model_path, num_episodes=10)
        print(f"  å½“å‰æˆåŠŸç‡: {success_rate:.1%}")
        
        if success_rate > 0.90:
            print("  âœ“ è¾¾åˆ°ç›®æ ‡æˆåŠŸç‡ï¼Œæå‰ç»“æŸ")
            break
        
        # 3. æ”¶é›†æ–°çŠ¶æ€
        states_dir = f"data/policy_states/iter_{iteration}"
        collect_policy_states(
            model_path=model_path,
            num_episodes=episodes_per_iteration,
            output_dir=states_dir
        )
        
        # 4. äººå·¥æ ‡æ³¨ï¼ˆå…³é”®æ­¥éª¤ï¼‰
        print(f"\n  è¯·æ ‡æ³¨æ–°æ”¶é›†çš„çŠ¶æ€...")
        new_labels = label_states_interactive(states_dir)
        
        # 5. èšåˆæ•°æ®
        all_data = aggregate_data(all_data, new_labels)
        print(f"  æ•°æ®é›†å¤§å°: {len(all_data)} samples")
    
    return model_path
```

---

## ğŸ¯ **æ™ºèƒ½æ ‡æ³¨ç­–ç•¥**

### **ä¸éœ€è¦æ ‡æ³¨æ‰€æœ‰çŠ¶æ€ï¼**

**å…³é”®åŸåˆ™**: åªæ ‡æ³¨**å¤±è´¥åœºæ™¯**å’Œ**è¾¹ç•Œæƒ…å†µ**

#### **æ ‡æ³¨ä¼˜å…ˆçº§**

| ä¼˜å…ˆçº§ | åœºæ™¯ | æ ‡æ³¨æ¯”ä¾‹ | åŸå›  |
|--------|------|---------|------|
| ğŸ”´ é«˜ | å¤±è´¥å‰5æ­¥ | 100% | å…³é”®å¤±è´¥ç‚¹ |
| ğŸŸ¡ ä¸­ | åç¦»è½¨è¿¹ | 50% | çº æ­£åå·® |
| ğŸŸ¢ ä½ | æ­£å¸¸æ‰§è¡Œ | 10% | å·²æœ‰ä¸“å®¶æ¼”ç¤º |

#### **æ™ºèƒ½é‡‡æ ·ç­–ç•¥**

```python
def smart_sampling(states, policy, expert_demo):
    """æ™ºèƒ½é€‰æ‹©éœ€è¦æ ‡æ³¨çš„çŠ¶æ€"""
    
    to_label = []
    
    for state in states:
        # 1. å¤±è´¥episodeçš„æ‰€æœ‰çŠ¶æ€
        if state['episode_failed']:
            to_label.append(state)
        
        # 2. ç­–ç•¥ä¸ç¡®å®šçš„çŠ¶æ€ï¼ˆç†µé«˜ï¼‰
        elif policy_entropy(state) > threshold:
            to_label.append(state)
        
        # 3. åç¦»ä¸“å®¶è½¨è¿¹çš„çŠ¶æ€
        elif distance_to_expert(state, expert_demo) > threshold:
            to_label.append(state)
        
        # 4. éšæœºé‡‡æ ·10%
        elif random.random() < 0.1:
            to_label.append(state)
    
    return to_label
```

---

## ğŸ“ˆ **é¢„æœŸæ•ˆæœ**

### **åœ¨Minecraftç æ ‘ä»»åŠ¡ä¸Š**

| è½®æ¬¡ | æ•°æ®é‡ | æ ‡æ³¨æ—¶é—´ | æˆåŠŸç‡ | æå‡ |
|------|--------|---------|--------|------|
| åˆå§‹BC | 5K | 40åˆ†é’Ÿ | 60% | - |
| DAgger-1 | 7K | +30åˆ†é’Ÿ | 75% | +15% |
| DAgger-2 | 9K | +30åˆ†é’Ÿ | 85% | +10% |
| DAgger-3 | 11K | +20åˆ†é’Ÿ | 90% | +5% |
| DAgger-4 | 12K | +20åˆ†é’Ÿ | 92% | +2% |

**æ€»æ—¶é—´**: 2.5å°æ—¶  
**æ€»æ ‡æ³¨**: ~12Kæ ·æœ¬  
**æœ€ç»ˆæˆåŠŸç‡**: 92%

---

## âš ï¸ **DAggerçš„å±€é™æ€§**

### **1. éœ€è¦å¤šæ¬¡äººå·¥æ ‡æ³¨**

**å·¥ä½œé‡**:
- BC: ä¸€æ¬¡æ€§å½•åˆ¶ï¼ˆ1å°æ—¶ï¼‰
- DAgger: å¤šè½®æ ‡æ³¨ï¼ˆ2-3å°æ—¶æ€»è®¡ï¼‰

**ç¼“è§£æ–¹æ³•**:
- ä½¿ç”¨æ™ºèƒ½é‡‡æ ·ï¼ˆåªæ ‡æ³¨20-30%ï¼‰
- ä¸“æ³¨å¤±è´¥åœºæ™¯
- åæœŸå¯ä»¥ç”¨ç­–ç•¥è‡ªå·±ç©ä»£æ›¿äººå·¥

### **2. ä¸“å®¶éœ€è¦ä¸€è‡´**

å¦‚æœä¸åŒè½®æ¬¡æ ‡æ³¨é£æ ¼ä¸åŒï¼Œä¼šæ··æ·†ç­–ç•¥

**è§£å†³**:
- åŒä¸€ä¸ªäººæ ‡æ³¨
- åˆ¶å®šæ˜ç¡®çš„æ ‡æ³¨è§„èŒƒ
- å›é¡¾ä¹‹å‰çš„æ ‡æ³¨ä¿æŒä¸€è‡´

### **3. æ ‡æ³¨å»¶è¿Ÿ**

æ¯è½®éœ€è¦ç­‰å¾…æ ‡æ³¨å®Œæˆ

**è§£å†³**:
- å¼‚æ­¥æ ‡æ³¨ï¼ˆæ™šä¸Šæ ‡æ³¨ï¼Œç™½å¤©è®­ç»ƒï¼‰
- æ‰¹é‡æ ‡æ³¨ï¼ˆç§¯ç´¯å¤šä¸ªepisodeä¸€èµ·æ ‡æ³¨ï¼‰

---

## ğŸš€ **å¿«é€Ÿå¼€å§‹DAggerï¼ˆMinecraftç æ ‘ï¼‰**

### **å®Œæ•´æµç¨‹ï¼ˆé¢„è®¡3å°æ—¶ï¼‰**

```bash
# ===== ç¬¬0è½®: åˆå§‹BC =====
# 1. å½•åˆ¶ä¸“å®¶æ¼”ç¤ºï¼ˆ40åˆ†é’Ÿï¼‰
python tools/record_manual_chopping.py --episodes 10

# 2. è®­ç»ƒåˆå§‹BCï¼ˆ10åˆ†é’Ÿï¼‰
python src/training/train_bc.py \
  --data data/expert_demos/initial.pkl \
  --output checkpoints/dagger_r0.zip

# ===== ç¬¬1è½®: DAggerè¿­ä»£1 =====
# 3. è¿è¡Œç­–ç•¥æ”¶é›†çŠ¶æ€ï¼ˆ5åˆ†é’Ÿï¼‰
python tools/run_policy_collect_states.py \
  --model checkpoints/dagger_r0.zip \
  --episodes 20

# 4. æ ‡æ³¨å¤±è´¥åœºæ™¯ï¼ˆ30åˆ†é’Ÿï¼‰
python tools/label_states.py \
  --states data/policy_states/round_1/

# 5. èšåˆå¹¶é‡æ–°è®­ç»ƒï¼ˆ10åˆ†é’Ÿï¼‰
python src/training/train_dagger.py --iteration 1

# ===== ç¬¬2è½®: DAggerè¿­ä»£2 =====
# é‡å¤3-5ï¼ˆ30åˆ†é’Ÿï¼‰

# ===== ç¬¬3è½®: DAggerè¿­ä»£3 =====
# é‡å¤3-5ï¼ˆ20åˆ†é’Ÿï¼‰

# æ€»è®¡: ~2.5å°æ—¶ â†’ æˆåŠŸç‡ä»60% â†’ 90%
```

---

## ğŸ’¡ **DAgger vs BC+PPO é€‰æ‹©å»ºè®®**

| åœºæ™¯ | æ¨èæ–¹æ¡ˆ | åŸå›  |
|------|---------|------|
| æœ‰å……è¶³æ—¶é—´æ ‡æ³¨ | **DAgger** | æœ€ç»ˆæ€§èƒ½æœ€å¥½ |
| å¿«é€ŸéªŒè¯ | BC+PPO | æ›´å¿«çœ‹åˆ°ç»“æœ |
| æ ‡æ³¨èµ„æºæœ‰é™ | BC+PPO | ä¸€æ¬¡æ€§æ ‡æ³¨ |
| éœ€è¦æè‡´æ€§èƒ½ | **DAgger** + PPO | ä¸¤è€…ç»“åˆ |
| å¤æ‚é•¿åºåˆ—ä»»åŠ¡ | **DAgger** | æ›´é²æ£’ |

---

## ğŸ“š **ç†è®ºåŸºç¡€**

### **å…³é”®è®ºæ–‡**

**DAggeråŸè®ºæ–‡**:
- Ross, S., Gordon, G., & Bagnell, D. (2011). 
- "A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning"
- AISTATS 2011

**æ ¸å¿ƒå®šç†**:

DAggerçš„æ€§èƒ½ç•Œé™:
```
Îµ(Ï€_dagger) â‰¤ Îµ_expert + O(TÂ·Îµ_BC)
```

å…¶ä¸­:
- T: è½¨è¿¹é•¿åº¦
- Îµ_expert: ä¸“å®¶è¯¯å·®
- Îµ_BC: è¡Œä¸ºå…‹éš†è¯¯å·®

**vs ä¼ ç»ŸBC**:
```
Îµ(Ï€_BC) â‰¤ Îµ_expert + O(TÂ²Â·Îµ_BC)  # æ³¨æ„æ˜¯TÂ²ï¼
```

**ç»“è®º**: DAggerçš„è¯¯å·®å¢é•¿æ˜¯çº¿æ€§çš„ï¼ŒBCæ˜¯äºŒæ¬¡çš„ï¼

---

## ğŸ”— **ç›¸å…³èµ„æº**

- **è®ºæ–‡**: https://arxiv.org/abs/1011.0686
- **ä»£ç åº“**: https://github.com/jj-zhu/dagger
- **imitationåº“**: https://imitation.readthedocs.io/
- **MineRLæ¯”èµ›**: https://minerl.io/

---

## ğŸ“ **æ€»ç»“**

### **DAggerçš„æ ¸å¿ƒä¼˜åŠ¿**

1. âœ… è§£å†³åˆ†å¸ƒåç§»é—®é¢˜
2. âœ… æ€§èƒ½ä¼˜äºçº¯BC
3. âœ… ç†è®ºä¿è¯ï¼ˆçº¿æ€§è¯¯å·®å¢é•¿ï¼‰
4. âœ… é€‚åˆMinecraftç­‰å¤æ‚ä»»åŠ¡

### **é€‚åˆä½ çš„é¡¹ç›®å› ä¸º**

1. âœ… ä½ å·²æœ‰å½•åˆ¶å·¥å…·
2. âœ… ç æ ‘ä»»åŠ¡é€‚ä¸­ï¼ˆä¸å¤ªé•¿ï¼‰
3. âœ… æœ‰æ—¶é—´åšè¿­ä»£æ ‡æ³¨
4. âœ… é¢„æœŸæ•ˆæœæ˜¾è‘—ï¼ˆ60% â†’ 90%ï¼‰

### **ä¸‹ä¸€æ­¥**

1. å®ç°çŠ¶æ€æ”¶é›†å·¥å…·
2. å®ç°äº¤äº’å¼æ ‡æ³¨å·¥å…·
3. è¿è¡Œç¬¬ä¸€è½®DAgger
4. è¯„ä¼°æ•ˆæœå†³å®šæ˜¯å¦ç»§ç»­

---

**æ¨èé˜…è¯»é¡ºåº**:
1. æœ¬æ–‡æ¡£ï¼ˆDAggerè¯¦è§£ï¼‰
2. [`IMITATION_LEARNING_GUIDE.md`](IMITATION_LEARNING_GUIDE.md)ï¼ˆæ¨¡ä»¿å­¦ä¹ æ¦‚è§ˆï¼‰
3. [`IMITATION_LEARNING_ROADMAP.md`](../status/IMITATION_LEARNING_ROADMAP.md)ï¼ˆå®æ–½è®¡åˆ’ï¼‰

