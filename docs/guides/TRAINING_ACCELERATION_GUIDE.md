# MineDojo æŠ€èƒ½è®­ç»ƒåŠ é€ŸæŒ‡å—

## é—®é¢˜èƒŒæ™¯

ä»é›¶å¼€å§‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒMinecraftæŠ€èƒ½ï¼ˆå¦‚ç æ ‘ã€é‡‡çŸ¿ç­‰ï¼‰é¢ä¸´çš„æ ¸å¿ƒæŒ‘æˆ˜ï¼š

- âŒ **ç¨€ç–å¥–åŠ±**ï¼šæ™ºèƒ½ä½“å¯èƒ½éœ€è¦æ•°åƒæ­¥æ‰èƒ½è·å¾—ç¬¬ä¸€æ¬¡æ­£å‘å¥–åŠ±
- âŒ **æ¢ç´¢å›°éš¾**ï¼šåŠ¨ä½œç©ºé—´å¤§ï¼ˆæ•°ç™¾ä¸ªç¦»æ•£åŠ¨ä½œç»„åˆï¼‰ï¼ŒçŠ¶æ€ç©ºé—´æ›´å¤§
- âŒ **è®­ç»ƒæ—¶é—´é•¿**ï¼šå¯èƒ½éœ€è¦æ•°ç™¾ä¸‡æ­¥ï¼ˆå‡ å¤©åˆ°å‡ å‘¨ï¼‰æ‰èƒ½å­¦ä¼šåŸºç¡€æŠ€èƒ½
- âŒ **æ ·æœ¬æ•ˆç‡ä½**ï¼šçº¯å¼ºåŒ–å­¦ä¹ éœ€è¦å¤§é‡è¯•é”™

**ç›®æ ‡**ï¼šè®­ç»ƒå¤šä¸ªæŠ€èƒ½å¹¶é€šè¿‡agentç»„åˆè¯„ä¼° â†’ éœ€è¦é«˜æ•ˆçš„å•æŠ€èƒ½è®­ç»ƒæ–¹æ³•

---

## ğŸš€ æ–¹æ¡ˆä¸€ï¼šæ¨¡ä»¿å­¦ä¹ ï¼ˆImitation Learningï¼‰ã€æ¨èã€‘

### 1.1 ä½¿ç”¨MineDojoçš„YouTubeè§†é¢‘æ•°æ®é›†

MineDojoæä¾›äº†**å¤§è§„æ¨¡YouTubeæ¸¸æˆè§†é¢‘æ•°æ®é›†**ï¼ŒåŒ…å«æ•°åƒå°æ—¶çš„äººç±»ç©å®¶æ¸¸æˆå½•åƒã€‚

#### å®ç°æ­¥éª¤

**ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨MineCLIPä½œä¸ºå¥–åŠ±å‡½æ•°**

MineCLIPæ˜¯MineDojoæä¾›çš„é¢„è®­ç»ƒè§†è§‰-è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥å°†æ–‡å­—ä»»åŠ¡æè¿°ä¸æ¸¸æˆç”»é¢å…³è”ã€‚

```python
import minedojo
from minedojo.sim import MinecraftSim
import torch

# åˆ›å»ºå¸¦MineCLIPå¥–åŠ±çš„ç¯å¢ƒ
env = minedojo.make(
    task_id="harvest_log",  # ç æ ‘ä»»åŠ¡
    image_size=(160, 256),
    reward_fn="mineclip",  # ä½¿ç”¨MineCLIPä½œä¸ºå¥–åŠ±
    use_voxel=False,
)

# MineCLIPä¼šæ ¹æ®ä»»åŠ¡æè¿°è‡ªåŠ¨è®¡ç®—å¯†é›†å¥–åŠ±
# ä¾‹å¦‚ï¼š"chop down a tree" â†’ é è¿‘æ ‘æœ¨ã€æŒ¥åŠ¨æ–§å¤´ã€è·å¾—æœ¨å¤´éƒ½ä¼šæœ‰å¥–åŠ±
```

**ä¼˜ç‚¹**ï¼š
- âœ… å°†ç¨€ç–å¥–åŠ±è½¬ä¸ºå¯†é›†å¥–åŠ±
- âœ… æ— éœ€äººå·¥æ ‡æ³¨æ•°æ®
- âœ… å¯ä»¥ç›´æ¥ç”¨æ–‡å­—æè¿°æ–°ä»»åŠ¡

**ç¬¬äºŒæ­¥ï¼šè¡Œä¸ºå…‹éš†ï¼ˆBehavior Cloningï¼‰**

å¦‚æœæœ‰äººç±»æ¼”ç¤ºæ•°æ®ï¼Œå¯ä»¥å…ˆç”¨ç›‘ç£å­¦ä¹ é¢„è®­ç»ƒç­–ç•¥ã€‚

```python
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
import numpy as np

# 1. æ”¶é›†äººç±»æ¼”ç¤ºæ•°æ®ï¼ˆå¯ä»¥è‡ªå·±ç©æˆ–ä½¿ç”¨MineDojoæ•°æ®é›†ï¼‰
# MineDojoæä¾›äº†contractoræ•°æ®é›†ï¼ˆäººç±»ä¸“å®¶å½•åˆ¶ï¼‰
env = minedojo.make(
    task_id="harvest_log",
    image_size=(160, 256),
)

# 2. è¡Œä¸ºå…‹éš†é¢„è®­ç»ƒ
# å‡è®¾ä½ æœ‰demonstrations.pklæ–‡ä»¶ï¼ˆobs, actionså¯¹ï¼‰
from imitation.algorithms import bc
from imitation.data import rollout

# åŠ è½½æ¼”ç¤ºæ•°æ®
demonstrations = load_demonstrations("demos/harvest_log.pkl")

# è®­ç»ƒè¡Œä¸ºå…‹éš†æ¨¡å‹
bc_trainer = bc.BC(
    observation_space=env.observation_space,
    action_space=env.action_space,
    demonstrations=demonstrations,
    batch_size=32,
)
bc_trainer.train(n_epochs=100)

# 3. ç”¨è¡Œä¸ºå…‹éš†æ¨¡å‹åˆå§‹åŒ–PPO
model = PPO("CnnPolicy", env, verbose=1)
model.policy.load_state_dict(bc_trainer.policy.state_dict())

# 4. ç»§ç»­å¼ºåŒ–å­¦ä¹ å¾®è°ƒ
model.learn(total_timesteps=500000)
```

**ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨VPTï¼ˆVideo Pre-Trainingï¼‰**

OpenAIå¼€å‘çš„VPTæ–¹æ³•ï¼Œå¯ä»¥ä»æœªæ ‡æ³¨çš„YouTubeè§†é¢‘å­¦ä¹ ã€‚

å‚è€ƒé¡¹ç›®ï¼š
- [Video Pre-Training (VPT)](https://github.com/openai/Video-Pre-Training)
- [MineRL BASALT Competition](https://www.aicrowd.com/challenges/neurips-2022-minerl-basalt-competition)

```bash
# å®‰è£…VPTç›¸å…³ä¾èµ–
pip install video-pre-training
pip install minerl

# ä½¿ç”¨é¢„è®­ç»ƒçš„VPTæ¨¡å‹
from vpt import load_model

# åŠ è½½é¢„è®­ç»ƒåŸºç¡€æ¨¡å‹
model = load_model("vpt-foundation-2x.model")

# åœ¨MineDojoç¯å¢ƒä¸­ä½¿ç”¨
# VPTæ¨¡å‹å¯ä»¥ä½œä¸ºåˆå§‹ç­–ç•¥ï¼Œç„¶ååœ¨MineDojoä¸­å¾®è°ƒ
```

---

## ğŸ¯ æ–¹æ¡ˆäºŒï¼šè¯¾ç¨‹å­¦ä¹ ï¼ˆCurriculum Learningï¼‰

### 2.1 æ¸è¿›å¼ä»»åŠ¡éš¾åº¦

ä¸è¦ç›´æ¥è®­ç»ƒå¤æ‚ä»»åŠ¡ï¼Œè€Œæ˜¯ä»ç®€å•ä»»åŠ¡å¼€å§‹é€æ­¥å¢åŠ éš¾åº¦ã€‚

**ç¤ºä¾‹ï¼šç æ ‘æŠ€èƒ½çš„è¯¾ç¨‹è®¾è®¡**

```python
# é˜¶æ®µ1ï¼šå¯¼èˆª - å­¦ä¹ ç§»åŠ¨å’Œè§†è§’æ§åˆ¶
curriculum = [
    # Level 1: ç®€å•ç§»åŠ¨ï¼ˆå¹³åœ°ï¼Œç›®æ ‡è¿‘ï¼‰
    {
        "task": "navigate_to_block",
        "target": "oak_log",
        "distance": 10,
        "terrain": "flat",
        "timesteps": 50000,
    },
    
    # Level 2: å¤æ‚å¯¼èˆªï¼ˆæœ‰éšœç¢ï¼Œç›®æ ‡è¿œï¼‰
    {
        "task": "navigate_to_block",
        "target": "oak_log",
        "distance": 50,
        "terrain": "forest",
        "timesteps": 100000,
    },
    
    # Level 3: ç æ ‘ï¼ˆè¿‘è·ç¦»ï¼Œæ–§å¤´åœ¨æ‰‹ï¼‰
    {
        "task": "harvest_log",
        "initial_items": [{"type": "wooden_axe", "quantity": 1}],
        "spawn_near_tree": True,
        "timesteps": 100000,
    },
    
    # Level 4: å®Œæ•´ä»»åŠ¡ï¼ˆéœ€è¦æ‰¾æ ‘ã€ç æ ‘ï¼‰
    {
        "task": "harvest_log",
        "initial_items": [],
        "spawn_near_tree": False,
        "timesteps": 200000,
    },
]

# è®­ç»ƒå¾ªç¯
for level in curriculum:
    print(f"Training level: {level['task']}")
    
    # åˆ›å»ºç¯å¢ƒ
    env = create_custom_env(**level)
    
    # å¦‚æœæœ‰ä¸Šä¸€é˜¶æ®µçš„æ¨¡å‹ï¼ŒåŠ è½½å®ƒ
    if previous_model is not None:
        model = PPO.load(previous_model)
        model.set_env(env)
    else:
        model = PPO("CnnPolicy", env)
    
    # è®­ç»ƒå½“å‰é˜¶æ®µ
    model.learn(total_timesteps=level['timesteps'])
    
    # ä¿å­˜æ¨¡å‹ä¾›ä¸‹ä¸€é˜¶æ®µä½¿ç”¨
    previous_model = f"checkpoints/curriculum_level_{level['task']}.zip"
    model.save(previous_model)
```

### 2.2 è‡ªåŠ¨è¯¾ç¨‹å­¦ä¹ ï¼ˆAutomatic Curriculum Learningï¼‰

ä½¿ç”¨PLRï¼ˆPrioritized Level Replayï¼‰ç­‰æ–¹æ³•è‡ªåŠ¨è°ƒæ•´ä»»åŠ¡éš¾åº¦ã€‚

```python
# ä¼ªä»£ç ç¤ºä¾‹
class AutoCurriculumEnv:
    def __init__(self):
        self.difficulty_level = 0
        self.success_rate = []
    
    def reset(self):
        # æ ¹æ®æœ€è¿‘æˆåŠŸç‡è°ƒæ•´éš¾åº¦
        recent_success = np.mean(self.success_rate[-10:])
        
        if recent_success > 0.7:
            self.difficulty_level += 1  # å¢åŠ éš¾åº¦
        elif recent_success < 0.3:
            self.difficulty_level = max(0, self.difficulty_level - 1)  # é™ä½éš¾åº¦
        
        # ç”Ÿæˆç›¸åº”éš¾åº¦çš„ç¯å¢ƒé…ç½®
        config = self.generate_config(self.difficulty_level)
        return self.env.reset(config)
```

---

## ğŸ æ–¹æ¡ˆä¸‰ï¼šå¥–åŠ±å¡‘å½¢ï¼ˆReward Shapingï¼‰

### 3.1 æ‰‹å·¥è®¾è®¡ä¸­é—´å¥–åŠ±

ä¸ºä»»åŠ¡çš„ä¸­é—´æ­¥éª¤æä¾›å¥–åŠ±ï¼Œå¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ã€‚

**ç¤ºä¾‹ï¼šç æ ‘ä»»åŠ¡çš„å¥–åŠ±å‡½æ•°**

```python
class RewardShapedEnv:
    def __init__(self, base_env):
        self.base_env = base_env
        self.previous_inventory = {}
        self.previous_distance_to_tree = float('inf')
        
    def step(self, action):
        obs, reward, done, info = self.base_env.step(action)
        
        # åŸå§‹ä»»åŠ¡å¥–åŠ±ï¼ˆè·å¾—æœ¨å¤´ï¼‰
        shaped_reward = reward
        
        # 1. é è¿‘æ ‘æœ¨ â†’ +0.01
        current_distance = self.get_distance_to_nearest_tree(obs)
        if current_distance < self.previous_distance_to_tree:
            shaped_reward += 0.01
        self.previous_distance_to_tree = current_distance
        
        # 2. é¢å‘æ ‘æœ¨ â†’ +0.005
        if self.is_facing_tree(obs):
            shaped_reward += 0.005
        
        # 3. æ‰‹æŒå·¥å…· â†’ +0.02
        if self.is_holding_axe(obs):
            shaped_reward += 0.02
        
        # 4. æ”»å‡»æ ‘æœ¨ â†’ +0.1
        if self.is_attacking_tree(obs, action):
            shaped_reward += 0.1
        
        # 5. è·å¾—æœ¨å¤´ â†’ +1.0ï¼ˆåŸå§‹ä»»åŠ¡å¥–åŠ±ï¼‰
        # å·²ç»åŒ…å«åœ¨base rewardä¸­
        
        return obs, shaped_reward, done, info
    
    def get_distance_to_nearest_tree(self, obs):
        # ä½¿ç”¨voxelæ•°æ®æˆ–è§†è§‰æ£€æµ‹æ‰¾æœ€è¿‘çš„æ ‘
        # ç®€åŒ–å®ç°ï¼šå‡è®¾MineDojoæä¾›
        voxel = obs.get('voxels', None)
        if voxel is not None:
            # è®¡ç®—åˆ°æœ€è¿‘oak_logæ–¹å—çš„è·ç¦»
            tree_positions = np.where(voxel['block_name'] == 'oak_log')
            if len(tree_positions[0]) > 0:
                distances = np.sqrt(np.sum(tree_positions**2, axis=0))
                return np.min(distances)
        return float('inf')
    
    def is_facing_tree(self, obs):
        # æ£€æŸ¥è§†é‡ä¸­å¿ƒæ˜¯å¦æœ‰æ ‘æœ¨
        # å¯ä»¥ç”¨ç®€å•çš„å›¾åƒå¤„ç†æˆ–MineCLIP
        pass
    
    def is_holding_axe(self, obs):
        # æ£€æŸ¥å½“å‰æ‰‹æŒç‰©å“
        inventory = obs.get('inventory', {})
        return 'axe' in inventory.get('mainhand', {}).get('type', '')
    
    def is_attacking_tree(self, obs, action):
        # æ£€æŸ¥æ˜¯å¦åœ¨æ”»å‡»åŠ¨ä½œä¸”é¢å‘æ ‘æœ¨
        return action == ATTACK_ACTION and self.is_facing_tree(obs)
```

**ä½¿ç”¨æ–¹æ³•**ï¼š

```python
# åˆ›å»ºåŒ…è£…åçš„ç¯å¢ƒ
base_env = minedojo.make(task_id="harvest_log")
shaped_env = RewardShapedEnv(base_env)

# æ­£å¸¸è®­ç»ƒ
model = PPO("CnnPolicy", shaped_env)
model.learn(total_timesteps=500000)
```

### 3.2 ä½¿ç”¨æ½œåœ¨ç©ºé—´è·ç¦»ä½œä¸ºå¥–åŠ±

åˆ©ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ï¼ˆå¦‚MineCLIPã€VPTç‰¹å¾æå–å™¨ï¼‰è®¡ç®—çŠ¶æ€ä¸ç›®æ ‡çš„ç›¸ä¼¼åº¦ã€‚

```python
import torch
from mineclip import MineCLIP

class LatentDistanceReward:
    def __init__(self, target_description="a player chopping down a tree"):
        # åŠ è½½MineCLIPæ¨¡å‹
        self.mineclip = MineCLIP()
        self.target_description = target_description
        
        # ç¼–ç ç›®æ ‡æè¿°
        self.target_embedding = self.mineclip.encode_text(target_description)
    
    def compute_reward(self, observation):
        # ç¼–ç å½“å‰è§‚å¯Ÿ
        obs_embedding = self.mineclip.encode_image(observation)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºå¥–åŠ±
        similarity = torch.cosine_similarity(
            obs_embedding, 
            self.target_embedding
        )
        
        # è½¬æ¢ä¸ºå¥–åŠ±ï¼ˆ0åˆ°1ä¹‹é—´ï¼‰
        reward = (similarity + 1) / 2
        return reward.item()
```

---

## ğŸ§  æ–¹æ¡ˆå››ï¼šä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹

### 4.1 MineCLIPä½œä¸ºè§†è§‰ç¼–ç å™¨

ä¸è¦ä»éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„MineCLIPä½œä¸ºç‰¹å¾æå–å™¨ã€‚

```python
from stable_baselines3 import PPO
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
import torch.nn as nn
from mineclip import MineCLIP

class MineCLIPFeaturesExtractor(BaseFeaturesExtractor):
    """ä½¿ç”¨é¢„è®­ç»ƒMineCLIPä½œä¸ºç‰¹å¾æå–å™¨"""
    
    def __init__(self, observation_space, features_dim=512):
        super().__init__(observation_space, features_dim)
        
        # åŠ è½½é¢„è®­ç»ƒMineCLIP
        self.mineclip = MineCLIP()
        
        # å†»ç»“MineCLIPå‚æ•°ï¼ˆå¯é€‰ï¼‰
        for param in self.mineclip.parameters():
            param.requires_grad = False
        
        # æ·»åŠ ä»»åŠ¡ç‰¹å®šçš„å¤´éƒ¨
        self.head = nn.Sequential(
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, features_dim),
            nn.ReLU()
        )
    
    def forward(self, observations):
        # ä½¿ç”¨MineCLIPç¼–ç å›¾åƒ
        features = self.mineclip.encode_image(observations)
        
        # é€šè¿‡ä»»åŠ¡ç‰¹å®šå¤´éƒ¨
        return self.head(features)

# ä½¿ç”¨è‡ªå®šä¹‰ç‰¹å¾æå–å™¨
policy_kwargs = dict(
    features_extractor_class=MineCLIPFeaturesExtractor,
    features_extractor_kwargs=dict(features_dim=512),
)

model = PPO(
    "CnnPolicy",
    env,
    policy_kwargs=policy_kwargs,
    verbose=1
)

model.learn(total_timesteps=500000)
```

**ä¼˜ç‚¹**ï¼š
- âœ… åˆ©ç”¨å¤§è§„æ¨¡é¢„è®­ç»ƒçŸ¥è¯†
- âœ… æ›´å¿«æ”¶æ•›
- âœ… æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›

### 4.2 ä½¿ç”¨VPTåŸºç¡€æ¨¡å‹

```python
# åŠ è½½OpenAIçš„VPTåŸºç¡€æ¨¡å‹
from vpt import load_vpt_model

# åŠ è½½foundationæ¨¡å‹ï¼ˆåœ¨YouTubeè§†é¢‘ä¸Šé¢„è®­ç»ƒï¼‰
vpt_model = load_vpt_model("foundation-2x")

# æ–¹å¼1ï¼šç›´æ¥å¾®è°ƒVPTæ¨¡å‹
# åœ¨MineDojoç¯å¢ƒä¸­ç»§ç»­è®­ç»ƒVPTæ¨¡å‹

# æ–¹å¼2ï¼šä½¿ç”¨VPTç‰¹å¾ä½œä¸ºåˆå§‹åŒ–
# å°†VPTçš„æƒé‡è¿ç§»åˆ°ä½ çš„ç­–ç•¥ç½‘ç»œ
model = PPO("CnnPolicy", env)
model.policy.features_extractor.load_state_dict(
    vpt_model.img_encoder.state_dict(), 
    strict=False
)
```

---

## ğŸ¤– æ–¹æ¡ˆäº”ï¼šåˆ†å±‚å¼ºåŒ–å­¦ä¹ ï¼ˆHierarchical RLï¼‰

### 5.1 æŠ€èƒ½åº“è®¾è®¡

å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¯å¤ç”¨çš„ä½çº§æŠ€èƒ½ã€‚

**æŠ€èƒ½å±‚æ¬¡ç»“æ„ç¤ºä¾‹**ï¼š

```
é«˜çº§ä»»åŠ¡ï¼šåˆ¶ä½œæœ¨åˆ¶å·¥å…·
â”œâ”€â”€ æŠ€èƒ½1ï¼šå¯»æ‰¾æ ‘æœ¨ï¼ˆå¯¼èˆªï¼‰
â”œâ”€â”€ æŠ€èƒ½2ï¼šç æ ‘ï¼ˆè·å–æœ¨å¤´ï¼‰
â”œâ”€â”€ æŠ€èƒ½3ï¼šæ‰“å¼€ç‰©å“æ 
â””â”€â”€ æŠ€èƒ½4ï¼šåˆæˆç‰©å“

ä½çº§æŠ€èƒ½ï¼š
- ç§»åŠ¨ï¼ˆå‰åå·¦å³ï¼‰
- è½¬å‘ï¼ˆæ”¹å˜è§†è§’ï¼‰
- è·³è·ƒ
- æ”»å‡»
- ä½¿ç”¨ç‰©å“
```

**å®ç°æ–¹å¼**ï¼š

```python
class SkillLibrary:
    """æŠ€èƒ½åº“ - å­˜å‚¨å’Œç®¡ç†å·²å­¦ä¹ çš„ä½çº§æŠ€èƒ½"""
    
    def __init__(self):
        self.skills = {}
    
    def add_skill(self, name, policy_path, description=""):
        """æ·»åŠ æ–°æŠ€èƒ½"""
        self.skills[name] = {
            "policy": PPO.load(policy_path),
            "description": description,
        }
    
    def get_skill(self, name):
        """è·å–æŠ€èƒ½ç­–ç•¥"""
        return self.skills[name]["policy"]
    
    def list_skills(self):
        """åˆ—å‡ºæ‰€æœ‰æŠ€èƒ½"""
        return list(self.skills.keys())


class HierarchicalAgent:
    """åˆ†å±‚æ™ºèƒ½ä½“ - é«˜çº§ç­–ç•¥é€‰æ‹©æŠ€èƒ½ï¼Œä½çº§ç­–ç•¥æ‰§è¡Œ"""
    
    def __init__(self, skill_library):
        self.skill_library = skill_library
        self.high_level_policy = None  # é€‰æ‹©æŠ€èƒ½çš„ç­–ç•¥
        self.current_skill = None
        self.skill_steps = 0
        self.max_skill_steps = 100  # æ¯ä¸ªæŠ€èƒ½æœ€å¤šæ‰§è¡Œ100æ­¥
    
    def select_skill(self, obs, task_embedding):
        """é«˜çº§ç­–ç•¥ï¼šæ ¹æ®è§‚å¯Ÿå’Œä»»åŠ¡é€‰æ‹©æŠ€èƒ½"""
        # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„ç½‘ç»œé€‰æ‹©æŠ€èƒ½
        skill_probs = self.high_level_policy(obs, task_embedding)
        skill_idx = torch.argmax(skill_probs)
        
        skill_names = self.skill_library.list_skills()
        return skill_names[skill_idx]
    
    def act(self, obs, task_description):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        # å¦‚æœå½“å‰æ²¡æœ‰æŠ€èƒ½æˆ–æŠ€èƒ½å·²æ‰§è¡Œè¶³å¤Ÿæ­¥æ•°ï¼Œé€‰æ‹©æ–°æŠ€èƒ½
        if self.current_skill is None or self.skill_steps >= self.max_skill_steps:
            task_emb = encode_task(task_description)
            self.current_skill = self.select_skill(obs, task_emb)
            self.skill_steps = 0
        
        # ä½¿ç”¨å½“å‰æŠ€èƒ½çš„ç­–ç•¥æ‰§è¡ŒåŠ¨ä½œ
        policy = self.skill_library.get_skill(self.current_skill)
        action, _ = policy.predict(obs)
        self.skill_steps += 1
        
        return action


# ä½¿ç”¨ç¤ºä¾‹
# 1. è®­ç»ƒåŸºç¡€æŠ€èƒ½
skill_lib = SkillLibrary()

# è®­ç»ƒ"å¯¼èˆªåˆ°æ ‘æœ¨"æŠ€èƒ½
nav_env = create_navigation_env(target="oak_log")
nav_model = PPO("CnnPolicy", nav_env)
nav_model.learn(total_timesteps=100000)
nav_model.save("skills/navigate_to_tree.zip")
skill_lib.add_skill("navigate_to_tree", "skills/navigate_to_tree.zip")

# è®­ç»ƒ"ç æ ‘"æŠ€èƒ½ï¼ˆå‡è®¾å·²ç»åœ¨æ ‘æ—è¾¹ï¼‰
chop_env = create_chopping_env(spawn_near_tree=True)
chop_model = PPO("CnnPolicy", chop_env)
chop_model.learn(total_timesteps=100000)
chop_model.save("skills/chop_tree.zip")
skill_lib.add_skill("chop_tree", "skills/chop_tree.zip")

# 2. è®­ç»ƒé«˜çº§ç­–ç•¥ï¼ˆç»„åˆæŠ€èƒ½ï¼‰
agent = HierarchicalAgent(skill_lib)
# è®­ç»ƒé«˜çº§ç­–ç•¥é€‰æ‹©æ­£ç¡®çš„æŠ€èƒ½åºåˆ—...
```

### 5.2 ä½¿ç”¨Optionsæ¡†æ¶

```python
from stable_baselines3 import PPO
import numpy as np

class Option:
    """ä¸€ä¸ªå¯é‡ç”¨çš„æŠ€èƒ½/é€‰é¡¹"""
    
    def __init__(self, policy, initiation_set, termination_fn):
        self.policy = policy  # ç­–ç•¥ç½‘ç»œ
        self.initiation_set = initiation_set  # ä½•æ—¶å¯ä»¥å¯åŠ¨
        self.termination_fn = termination_fn  # ä½•æ—¶ç»ˆæ­¢
        self.active = False
    
    def can_initiate(self, state):
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥åœ¨å½“å‰çŠ¶æ€å¯åŠ¨æ­¤é€‰é¡¹"""
        return self.initiation_set(state)
    
    def should_terminate(self, state):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥ç»ˆæ­¢æ­¤é€‰é¡¹"""
        return self.termination_fn(state)
    
    def get_action(self, state):
        """ä½¿ç”¨æ­¤é€‰é¡¹çš„ç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        return self.policy.predict(state)[0]


# å®šä¹‰é€‰é¡¹
option_navigate = Option(
    policy=PPO.load("skills/navigate.zip"),
    initiation_set=lambda s: True,  # ä»»ä½•çŠ¶æ€éƒ½å¯ä»¥å¯¼èˆª
    termination_fn=lambda s: is_near_tree(s),  # é è¿‘æ ‘æœ¨æ—¶ç»ˆæ­¢
)

option_chop = Option(
    policy=PPO.load("skills/chop.zip"),
    initiation_set=lambda s: is_near_tree(s),  # åªæœ‰åœ¨æ ‘æ—æ‰èƒ½ç æ ‘
    termination_fn=lambda s: has_collected_log(s),  # è·å¾—æœ¨å¤´æ—¶ç»ˆæ­¢
)
```

---

## ğŸ® æ–¹æ¡ˆå…­ï¼šäººæœºåä½œï¼ˆHuman-in-the-Loopï¼‰

### 6.1 æ”¶é›†äººç±»æ¼”ç¤º

æœ€ç›´æ¥çš„æ–¹æ³•ï¼šè‡ªå·±ç©æ¸¸æˆï¼Œæ”¶é›†é«˜è´¨é‡æ¼”ç¤ºæ•°æ®ã€‚

```python
import minedojo
import pickle

def collect_demonstrations(task_id, num_episodes=10):
    """ä½¿ç”¨é”®ç›˜æ§åˆ¶æ”¶é›†æ¼”ç¤ºæ•°æ®"""
    
    env = minedojo.make(
        task_id=task_id,
        image_size=(160, 256),
    )
    
    demonstrations = []
    
    for episode in range(num_episodes):
        obs = env.reset()
        episode_data = {"observations": [], "actions": []}
        done = False
        
        print(f"\n=== Episode {episode + 1}/{num_episodes} ===")
        print("Use keyboard to control:")
        print("  W/A/S/D: Move")
        print("  Mouse: Look around")
        print("  Space: Jump")
        print("  Left Click: Attack")
        print("  Q: Quit episode\n")
        
        while not done:
            # æ˜¾ç¤ºå½“å‰ç”»é¢
            env.render()
            
            # è·å–é”®ç›˜è¾“å…¥ï¼ˆéœ€è¦å®ç°keyboard_to_actionå‡½æ•°ï¼‰
            action = keyboard_to_action()
            
            if action == "quit":
                break
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_obs, reward, done, info = env.step(action)
            
            # è®°å½•æ•°æ®
            episode_data["observations"].append(obs)
            episode_data["actions"].append(action)
            
            obs = next_obs
            
            if reward > 0:
                print(f"âœ“ Reward: {reward}")
        
        demonstrations.append(episode_data)
        print(f"Episode completed: {len(episode_data['actions'])} steps")
    
    # ä¿å­˜æ¼”ç¤ºæ•°æ®
    output_file = f"demonstrations/{task_id}_demos.pkl"
    with open(output_file, 'wb') as f:
        pickle.dump(demonstrations, f)
    
    print(f"\nâœ“ Saved {num_episodes} demonstrations to {output_file}")
    env.close()
    
    return demonstrations


# ä½¿ç”¨
demos = collect_demonstrations("harvest_log", num_episodes=20)
```

### 6.2 äº¤äº’å¼å­¦ä¹ ï¼ˆInteractive Learningï¼‰

æ™ºèƒ½ä½“åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡åˆ°å›°éš¾æ—¶è¯·æ±‚äººç±»å¸®åŠ©ã€‚

```python
class InteractiveLearning:
    """äº¤äº’å¼å­¦ä¹  - æ™ºèƒ½ä½“å¯ä»¥è¯·æ±‚äººç±»å¸®åŠ©"""
    
    def __init__(self, env, model):
        self.env = env
        self.model = model
        self.human_intervention_threshold = 0.1  # ç½®ä¿¡åº¦é˜ˆå€¼
        self.demonstration_buffer = []
    
    def train_episode(self):
        obs = self.env.reset()
        done = False
        episode_reward = 0
        
        while not done:
            # è·å–æ¨¡å‹çš„åŠ¨ä½œå’Œç½®ä¿¡åº¦
            action, _, _, confidence = self.model.predict(
                obs, 
                return_all=True
            )
            
            # å¦‚æœæ¨¡å‹ä¸ç¡®å®šï¼Œè¯·æ±‚äººç±»å¸®åŠ©
            if confidence < self.human_intervention_threshold:
                print("ğŸ¤” Agent is uncertain. Please demonstrate the action.")
                action = get_human_action()  # è·å–äººç±»è¾“å…¥
                
                # å°†äººç±»æ¼”ç¤ºåŠ å…¥buffer
                self.demonstration_buffer.append((obs, action))
                
                # æ¯æ”¶é›†10ä¸ªäººç±»æ¼”ç¤ºï¼Œè¿›è¡Œä¸€æ¬¡è¡Œä¸ºå…‹éš†æ›´æ–°
                if len(self.demonstration_buffer) >= 10:
                    self.update_from_demonstrations()
            
            obs, reward, done, info = self.env.step(action)
            episode_reward += reward
        
        return episode_reward
    
    def update_from_demonstrations(self):
        """ä½¿ç”¨äººç±»æ¼”ç¤ºæ›´æ–°ç­–ç•¥"""
        # ç®€åŒ–çš„è¡Œä¸ºå…‹éš†æ›´æ–°
        for obs, action in self.demonstration_buffer:
            self.model.policy.learn_from_expert(obs, action)
        
        self.demonstration_buffer.clear()
        print("âœ“ Updated policy from human demonstrations")
```

---

## ğŸ“Š æ–¹æ¡ˆä¸ƒï¼šç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆOffline RLï¼‰

### 7.1 ä½¿ç”¨ç°æœ‰æ•°æ®é›†

MineDojoå’ŒMineRLæä¾›äº†å¤§é‡äººç±»ç©å®¶çš„æ¸¸æˆæ•°æ®ã€‚

```python
# ä½¿ç”¨MineRLæ•°æ®é›†
import minerl

# ä¸‹è½½æ•°æ®é›†
data = minerl.data.make("MineRLTreechop-v0")

# åŠ è½½è½¨è¿¹
trajectories = []
for state, action, reward, next_state, done in data.batch_iter(
    batch_size=32, 
    num_epochs=1
):
    trajectories.append((state, action, reward, next_state, done))

# ä½¿ç”¨ç¦»çº¿RLç®—æ³•è®­ç»ƒï¼ˆå¦‚CQL, IQL, BCQï¼‰
from d3rlpy.algos import CQLConfig
from d3rlpy.dataset import MDPDataset

# åˆ›å»ºç¦»çº¿æ•°æ®é›†
dataset = MDPDataset(
    observations=...,
    actions=...,
    rewards=...,
    terminals=...,
)

# è®­ç»ƒCQLæ¨¡å‹
cql = CQLConfig().create()
cql.fit(dataset, n_steps=100000)
```

### 7.2 CQLï¼ˆConservative Q-Learningï¼‰

```python
# ä½¿ç”¨d3rlpyåº“å®ç°CQL
from d3rlpy.algos import CQL
from d3rlpy.dataset import MDPDataset

# å‡è®¾å·²ç»æ”¶é›†äº†ç¦»çº¿æ•°æ®
offline_dataset = load_offline_data("harvest_log_data.h5")

# åˆ›å»ºCQLæ¨¡å‹
cql = CQL(
    learning_rate=3e-4,
    batch_size=256,
    use_gpu=True,
)

# çº¯ç¦»çº¿è®­ç»ƒ
cql.fit(
    offline_dataset,
    n_steps=500000,
)

# ä¿å­˜æ¨¡å‹
cql.save_model("checkpoints/cql_harvest_log.pt")
```

---

## ğŸ”„ æ–¹æ¡ˆå…«ï¼šå¤šä»»åŠ¡å­¦ä¹ å’Œè¿ç§»å­¦ä¹ 

### 8.1 å¤šä»»åŠ¡è®­ç»ƒ

åŒæ—¶è®­ç»ƒå¤šä¸ªç›¸å…³ä»»åŠ¡ï¼Œå…±äº«åº•å±‚è¡¨ç¤ºã€‚

```python
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv

# åˆ›å»ºå¤šä¸ªç›¸å…³ä»»åŠ¡çš„ç¯å¢ƒ
def make_multi_task_env():
    """åˆ›å»ºå¤šä»»åŠ¡ç¯å¢ƒ"""
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªä»»åŠ¡
    tasks = [
        "harvest_log",
        "harvest_oak_log",  
        "harvest_birch_log",
        "harvest_spruce_log",
    ]
    
    task = np.random.choice(tasks)
    env = minedojo.make(task_id=task, image_size=(160, 256))
    
    return env


# åˆ›å»ºå¤šä¸ªå¤šä»»åŠ¡ç¯å¢ƒ
envs = SubprocVecEnv([make_multi_task_env for _ in range(4)])

# è®­ç»ƒå¤šä»»åŠ¡ç­–ç•¥
model = PPO("CnnPolicy", envs, verbose=1)
model.learn(total_timesteps=1000000)

# è¿™ä¸ªæ¨¡å‹å¯ä»¥æ³›åŒ–åˆ°ä¸åŒç±»å‹çš„ç æ ‘ä»»åŠ¡
```

### 8.2 ä»ç®€å•ä»»åŠ¡è¿ç§»

å…ˆåœ¨ç®€å•ä»»åŠ¡ä¸Šè®­ç»ƒï¼Œç„¶åè¿ç§»åˆ°å¤æ‚ä»»åŠ¡ã€‚

```python
# 1. åœ¨ç®€å•ä»»åŠ¡ä¸Šè®­ç»ƒï¼ˆä¾‹å¦‚ï¼šharvest_milkï¼Œå¥–åŠ±æ›´å®¹æ˜“è·å¾—ï¼‰
simple_env = minedojo.make("harvest_milk")
model = PPO("CnnPolicy", simple_env)
model.learn(total_timesteps=200000)

# 2. ä¿å­˜è§†è§‰ç¼–ç å™¨çš„æƒé‡
torch.save(
    model.policy.features_extractor.state_dict(),
    "checkpoints/visual_encoder_pretrained.pth"
)

# 3. åœ¨ç›®æ ‡ä»»åŠ¡ä¸Šè®­ç»ƒï¼Œä½¿ç”¨é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨
target_env = minedojo.make("harvest_log")
target_model = PPO("CnnPolicy", target_env)

# åŠ è½½é¢„è®­ç»ƒæƒé‡
target_model.policy.features_extractor.load_state_dict(
    torch.load("checkpoints/visual_encoder_pretrained.pth")
)

# å¯é€‰ï¼šå†»ç»“å‰å‡ å±‚
for param in list(target_model.policy.features_extractor.parameters())[:10]:
    param.requires_grad = False

# ç»§ç»­è®­ç»ƒ
target_model.learn(total_timesteps=500000)
```

---

## ğŸ› ï¸ å®æ–½å»ºè®®

### æœ€ä¼˜ç»„åˆæ–¹æ¡ˆï¼ˆæ¨èï¼‰

æ ¹æ®ä½ çš„éœ€æ±‚ï¼ˆè®­ç»ƒå¤šä¸ªæŠ€èƒ½å¹¶ç»„åˆï¼‰ï¼Œæˆ‘æ¨èä»¥ä¸‹ç»„åˆï¼š

```
æ–¹æ¡ˆä¸€ï¼ˆæ¨¡ä»¿å­¦ä¹ ï¼‰ + æ–¹æ¡ˆäºŒï¼ˆè¯¾ç¨‹å­¦ä¹ ï¼‰ + æ–¹æ¡ˆå››ï¼ˆé¢„è®­ç»ƒæ¨¡å‹ï¼‰
```

**å…·ä½“å®æ–½æ­¥éª¤**ï¼š

#### ç¬¬1é˜¶æ®µï¼šä½¿ç”¨MineCLIPè¿›è¡Œå¥–åŠ±å¡‘å½¢ï¼ˆ1-2å‘¨ï¼‰

```python
# åˆ›å»ºå¯†é›†å¥–åŠ±ç¯å¢ƒ
env = minedojo.make(
    task_id="harvest_log",
    image_size=(160, 256),
    reward_fn="mineclip",  # ä½¿ç”¨MineCLIP
)

# ä½¿ç”¨MineCLIPç‰¹å¾æå–å™¨
policy_kwargs = dict(
    features_extractor_class=MineCLIPFeaturesExtractor,
)

model = PPO("CnnPolicy", env, policy_kwargs=policy_kwargs)
model.learn(total_timesteps=500000)
```

**é¢„æœŸæ•ˆæœ**ï¼š
- è®­ç»ƒæ—¶é—´å‡å°‘50-70%
- æ›´å¿«çœ‹åˆ°æœ‰æ„ä¹‰çš„è¡Œä¸º
- æ›´å¥½çš„æ”¶æ•›æ€§

#### ç¬¬2é˜¶æ®µï¼šè¯¾ç¨‹å­¦ä¹ è®­ç»ƒå¤šä¸ªæŠ€èƒ½ï¼ˆ2-4å‘¨ï¼‰

ä¸ºæ¯ä¸ªæŠ€èƒ½è®¾è®¡è¯¾ç¨‹ï¼š

```python
skills_curriculum = {
    "chop_tree": [
        {"difficulty": "easy", "spawn_near": True, "has_axe": True},
        {"difficulty": "medium", "spawn_near": False, "has_axe": True},
        {"difficulty": "hard", "spawn_near": False, "has_axe": False},
    ],
    
    "mine_stone": [
        {"difficulty": "easy", "spawn_near": True, "has_pickaxe": True},
        {"difficulty": "medium", "spawn_near": False, "has_pickaxe": True},
        {"difficulty": "hard", "spawn_near": False, "has_pickaxe": False},
    ],
    
    "hunt_animal": [
        {"difficulty": "easy", "animal": "cow", "spawn_near": True},
        {"difficulty": "medium", "animal": "sheep", "spawn_near": False},
        {"difficulty": "hard", "animal": "chicken", "spawn_near": False},
    ],
}

# ä¾æ¬¡è®­ç»ƒæ¯ä¸ªæŠ€èƒ½
for skill_name, curriculum in skills_curriculum.items():
    print(f"\n{'='*60}")
    print(f"Training skill: {skill_name}")
    print(f"{'='*60}\n")
    
    previous_model = None
    
    for level in curriculum:
        print(f"  Level: {level['difficulty']}")
        
        # åˆ›å»ºç¯å¢ƒ
        env = create_skill_env(skill_name, level)
        
        # åŠ è½½ä¸Šä¸€é˜¶æ®µæ¨¡å‹æˆ–åˆ›å»ºæ–°æ¨¡å‹
        if previous_model:
            model = PPO.load(previous_model)
            model.set_env(env)
        else:
            model = PPO("CnnPolicy", env, policy_kwargs=policy_kwargs)
        
        # è®­ç»ƒ
        model.learn(total_timesteps=200000)
        
        # ä¿å­˜
        model_path = f"skills/{skill_name}_{level['difficulty']}.zip"
        model.save(model_path)
        previous_model = model_path
    
    # ä¿å­˜æœ€ç»ˆæŠ€èƒ½
    final_path = f"skills/{skill_name}_final.zip"
    model.save(final_path)
    print(f"âœ“ Skill {skill_name} trained and saved to {final_path}")
```

#### ç¬¬3é˜¶æ®µï¼šç»„åˆæŠ€èƒ½å’Œé«˜çº§ç­–ç•¥ï¼ˆ1-2å‘¨ï¼‰

```python
# åˆ›å»ºæŠ€èƒ½åº“
skill_library = SkillLibrary()
skill_library.add_skill("chop_tree", "skills/chop_tree_final.zip")
skill_library.add_skill("mine_stone", "skills/mine_stone_final.zip")
skill_library.add_skill("hunt_animal", "skills/hunt_animal_final.zip")

# è®­ç»ƒé«˜çº§ç­–ç•¥ç»„åˆæŠ€èƒ½
hierarchical_agent = HierarchicalAgent(skill_library)

# åœ¨å¤æ‚ä»»åŠ¡ä¸Šè®­ç»ƒï¼ˆä¾‹å¦‚ï¼š"make wooden tools"ï¼‰
complex_env = minedojo.make("make_wooden_pickaxe")
train_hierarchical_policy(hierarchical_agent, complex_env)
```

### æ—¶é—´å’Œèµ„æºä¼°ç®—

| æ–¹æ¡ˆ | é¢„æœŸåŠ é€Ÿ | å®æ–½éš¾åº¦ | æ‰€éœ€èµ„æº |
|------|----------|----------|----------|
| æ–¹æ¡ˆä¸€ï¼šæ¨¡ä»¿å­¦ä¹ ï¼ˆMineCLIPï¼‰ | **3-5x** | â­â­ ä¸­ç­‰ | MineDojoè‡ªå¸¦ |
| æ–¹æ¡ˆäºŒï¼šè¯¾ç¨‹å­¦ä¹  | **2-3x** | â­ ç®€å• | éœ€è¦è®¾è®¡è¯¾ç¨‹ |
| æ–¹æ¡ˆä¸‰ï¼šå¥–åŠ±å¡‘å½¢ | **2-4x** | â­â­â­ è¾ƒéš¾ | éœ€è¦ä»»åŠ¡çŸ¥è¯† |
| æ–¹æ¡ˆå››ï¼šé¢„è®­ç»ƒæ¨¡å‹ | **3-10x** | â­â­ ä¸­ç­‰ | MineCLIP/VPT |
| æ–¹æ¡ˆäº”ï¼šåˆ†å±‚RL | **å¤§å‹é¡¹ç›®** | â­â­â­â­ å¾ˆéš¾ | å¤æ‚ç³»ç»Ÿè®¾è®¡ |
| æ–¹æ¡ˆå…­ï¼šäººæœºåä½œ | **5-10x** | â­ ç®€å• | éœ€è¦äººå·¥æ—¶é—´ |
| æ–¹æ¡ˆä¸ƒï¼šç¦»çº¿RL | **10x+** | â­â­â­â­ å¾ˆéš¾ | å¤§é‡ç¦»çº¿æ•°æ® |
| æ–¹æ¡ˆå…«ï¼šå¤šä»»åŠ¡å­¦ä¹  | **2-3x** | â­â­ ä¸­ç­‰ | å¤šGPU |

### å¿«é€ŸåŸå‹ï¼ˆ1å‘¨å†…çœ‹åˆ°æ•ˆæœï¼‰

å¦‚æœä½ æƒ³å¿«é€ŸéªŒè¯ï¼Œä»è¿™é‡Œå¼€å§‹ï¼š

```python
# quick_start.py - å¿«é€Ÿå¼€å§‹è„šæœ¬

import minedojo
from stable_baselines3 import PPO

# 1. ä½¿ç”¨MineCLIPå¥–åŠ±ï¼ˆæœ€é‡è¦ï¼ï¼‰
env = minedojo.make(
    task_id="harvest_log",
    image_size=(160, 256),
    reward_fn="mineclip",  # å¯†é›†å¥–åŠ±
)

# 2. ç®€å•çš„å¥–åŠ±å¡‘å½¢
class SimpleRewardWrapper:
    def __init__(self, env):
        self.env = env
    
    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        
        # é¢å¤–å¥–åŠ±ï¼šæ‰‹æŒå·¥å…·
        if self.holding_tool(obs):
            reward += 0.01
        
        # é¢å¤–å¥–åŠ±ï¼šé¢å‘æ–¹å—
        if self.facing_block(obs):
            reward += 0.005
        
        return obs, reward, done, info
    
    # ... å…¶ä»–æ–¹æ³•

env = SimpleRewardWrapper(env)

# 3. è®­ç»ƒ
model = PPO("CnnPolicy", env, verbose=1, tensorboard_log="logs/")
model.learn(total_timesteps=200000)  # å…ˆç”¨è¾ƒå°‘æ­¥æ•°æµ‹è¯•

# 4. ä¿å­˜
model.save("checkpoints/quick_harvest.zip")

print("âœ“ Quick start training completed!")
print("Check TensorBoard: tensorboard --logdir logs/")
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

### è®ºæ–‡
- [MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge](https://arxiv.org/abs/2206.08853)
- [Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos](https://arxiv.org/abs/2206.11795)
- [MineCLIP: Aligning Language and Video in Minecraft](https://openai.com/research/vpt)

### ä»£ç åº“
- [MineDojoå®˜æ–¹åº“](https://github.com/MineDojo/MineDojo)
- [OpenAI VPT](https://github.com/openai/Video-Pre-Training)
- [MineRLæ•°æ®é›†](https://github.com/minerllabs/minerl)
- [imitationåº“](https://github.com/HumanCompatibleAI/imitation) - è¡Œä¸ºå…‹éš†å’Œæ¨¡ä»¿å­¦ä¹ 
- [d3rlpy](https://github.com/takuseno/d3rlpy) - ç¦»çº¿å¼ºåŒ–å­¦ä¹ 

### å·¥å…·
- [MineCLIPæ¨¡å‹](https://github.com/MineDojo/MineCLIP)
- [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3)

---

## æ€»ç»“

è®­ç»ƒMinecraftæŠ€èƒ½çš„å…³é”®æ˜¯**è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜**ã€‚æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ï¼š

1. âœ… **ä½¿ç”¨MineCLIPæä¾›å¯†é›†å¥–åŠ±**ï¼ˆæœ€ç®€å•ã€æœ€æœ‰æ•ˆï¼‰
2. âœ… **è¯¾ç¨‹å­¦ä¹ **ï¼ˆä»ç®€å•åˆ°å¤æ‚ï¼‰
3. âœ… **åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹**ï¼ˆMineCLIPã€VPTï¼‰
4. âœ… **æ”¶é›†å°‘é‡äººç±»æ¼”ç¤º**ï¼ˆbootstrapåˆå§‹ç­–ç•¥ï¼‰

**ä¸è¦**ä»çº¯éšæœºç­–ç•¥å¼€å§‹ï¼è¿™æ˜¯æœ€æ…¢ã€æœ€ä½æ•ˆçš„æ–¹å¼ã€‚

**æ¨èè·¯å¾„**ï¼š
- ç¬¬1å‘¨ï¼šMineCLIP + ç®€å•å¥–åŠ±å¡‘å½¢ â†’ å¿«é€ŸéªŒè¯
- ç¬¬2-4å‘¨ï¼šè¯¾ç¨‹å­¦ä¹ è®­ç»ƒå¤šä¸ªåŸºç¡€æŠ€èƒ½
- ç¬¬5-6å‘¨ï¼šåˆ†å±‚ç­–ç•¥ç»„åˆæŠ€èƒ½
- ç¬¬7å‘¨+ï¼šåœ¨å®é™…ä»»åŠ¡ä¸­è¯„ä¼°å’Œä¼˜åŒ–

ç¥ä½ è®­ç»ƒé¡ºåˆ©ï¼ğŸš€

