# STEVE-1è¯„ä¼°ç³»ç»Ÿä½¿ç”¨æŒ‡å—

**æ›´æ–°æ—¥æœŸ**: 2025-11-06  
**é€‚ç”¨ç‰ˆæœ¬**: Phase 1 å®Œæˆç‰ˆ

---

## ğŸ“‹ å¿«é€Ÿå¼€å§‹

### æ–¹å¼1: ä½¿ç”¨ä¾¿æ·è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# è¿è¡Œè¯„ä¼°å¯åŠ¨å™¨
./scripts/run_steve1_evaluation.sh

# é€‰æ‹©è¯„ä¼°æ¨¡å¼:
#   1) æµ‹è¯•ç¯å¢ƒ (ä»…æµ‹è¯•åˆ›å»ºè¯„ä¼°å™¨)
#   2) å¿«é€Ÿè¯„ä¼° (1ä¸ªä»»åŠ¡ Ã— 2æ¬¡trial)
#   3) æ ‡å‡†è¯„ä¼° (3ä¸ªä»»åŠ¡ Ã— 3æ¬¡trial)
#   4) å®Œæ•´è¯„ä¼° (10ä¸ªä»»åŠ¡ Ã— 10æ¬¡trial)
```

### æ–¹å¼2: ä½¿ç”¨Pythonä»£ç 

```bash
# é€šè¿‡run_minedojo_x86.shè¿è¡Œ
scripts/run_minedojo_x86.sh python -c "
from src.evaluation import STEVE1Evaluator

# åˆ›å»ºè¯„ä¼°å™¨
evaluator = STEVE1Evaluator(
    model_path='data/weights/steve1/steve1.weights',
    device='auto'  # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¾å¤‡
)

# è¿è¡Œå¿«é€Ÿæµ‹è¯•
evaluator.quick_test(n_trials=3)
"
```

---

## ğŸ¯ è¯„ä¼°å™¨é…ç½®

### STEVE1Evaluator å‚æ•°

```python
from src.evaluation import STEVE1Evaluator

evaluator = STEVE1Evaluator(
    model_path="data/weights/steve1/steve1.weights",  # STEVE-1æƒé‡è·¯å¾„
    mineclip_path="data/weights/mineclip/attn.pth",   # MineCLIPæƒé‡è·¯å¾„
    device="auto",                                     # è®¾å¤‡é€‰æ‹©
    task_config_path="config/eval_tasks.yaml"         # ä»»åŠ¡é…ç½®æ–‡ä»¶
)
```

### Deviceé€‰é¡¹

- `"auto"` - è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¾å¤‡ (cuda > mps > cpu) **æ¨è**
- `"cuda"` - ä½¿ç”¨NVIDIA GPU
- `"mps"` - ä½¿ç”¨Apple Silicon GPU
- `"cpu"` - ä½¿ç”¨CPU

---

## ğŸ“Š è¯„ä¼°æ–¹æ³•

### 1. è¯„ä¼°å•ä¸ªä»»åŠ¡

```python
# è¯„ä¼°å•ä¸ªä»»åŠ¡ï¼ˆè‹±æ–‡æŒ‡ä»¤ï¼‰
result = evaluator.evaluate_task(
    task_id="harvest_1_log",
    lang="en",
    n_trials=10
)

print(f"æˆåŠŸç‡: {result['success_rate']:.1f}%")
```

### 2. å¿«é€Ÿæµ‹è¯•ï¼ˆ3ä¸ªä»»åŠ¡ï¼‰

```python
# å¿«é€Ÿæµ‹è¯•é›†ï¼šharvest_1_log, harvest_1_dirt, combat_cow_forest_barehand
evaluator.quick_test(n_trials=3)
```

### 3. å®Œæ•´åŸºçº¿è¯„ä¼°ï¼ˆ10ä¸ªä»»åŠ¡ï¼‰

```python
# å®Œæ•´åŸºçº¿æµ‹è¯•é›†ï¼ˆ10ä¸ªä»»åŠ¡ï¼‰
evaluator.run_baseline_evaluation(n_trials=10)
```

### 4. è‡ªå®šä¹‰ä»»åŠ¡é›†

```python
# è¯„ä¼°è‡ªå®šä¹‰ä»»åŠ¡é›†
comparisons = evaluator.evaluate_task_set(
    set_name="baseline_test",  # æˆ– "quick_test"
    n_trials=10
)
```

---

## ğŸ“ è¯„ä¼°ç»“æœ

### æŠ¥å‘Šä½ç½®

è¯„ä¼°å®Œæˆåï¼ŒæŠ¥å‘Šä¼šè‡ªåŠ¨ä¿å­˜åˆ°ï¼š

```
results/evaluation/
â”œâ”€â”€ quick_test_report_2025-11-06T10-22-06.json     # JSONæ ¼å¼
â””â”€â”€ quick_test_report_2025-11-06T10-22-06.txt      # æ–‡æœ¬æ ¼å¼
```

### æŠ¥å‘Šå†…å®¹

#### JSONæŠ¥å‘Šç»“æ„

```json
{
  "generated_at": "2025-11-06T10:22:06",
  "total_tasks": 3,
  "summary": {
    "overall_en_success_rate": 83.3,
    "overall_zh_auto_success_rate": 83.3,
    "avg_gap": 33.3,
    "tasks_evaluated": 3
  },
  "detailed_results": [
    {
      "task_id": "harvest_1_log",
      "en_success_rate": 100.0,
      "zh_auto_success_rate": 50.0,
      "gap": 50.0,
      "semantic_variance": 40.8,
      "en_trials": [...],
      "zh_auto_trials": [...],
      ...
    }
  ]
}
```

#### æ–‡æœ¬æŠ¥å‘Šç¤ºä¾‹

```
====================================================================================================
è¯„ä¼°ç»“æœå¯¹æ¯”è¡¨ (Evaluation Results Comparison)
====================================================================================================
Task ID                              EN   ZH(Auto)    ZH(Man)      Gap      Var
----------------------------------------------------------------------------------------------------
harvest_1_log                    100.0%      50.0%      50.0%    50.0%    40.8%
harvest_1_dirt                   100.0%     100.0%     100.0%     0.0%    23.6%
combat_cow_forest_barehand        50.0%     100.0%     100.0%    50.0%    23.6%
----------------------------------------------------------------------------------------------------
Overall Average                   83.3%      83.3%        N/A    33.3%      N/A
====================================================================================================
```

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### ä½¿ç”¨Pythonè„šæœ¬

åˆ›å»ºè‡ªå®šä¹‰è¯„ä¼°è„šæœ¬ `my_evaluation.py`:

```python
#!/usr/bin/env python
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.evaluation import STEVE1Evaluator

def main():
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = STEVE1Evaluator(
        model_path="data/weights/steve1/steve1.weights",
        device="auto"
    )
    
    # è‡ªå®šä¹‰è¯„ä¼°æµç¨‹
    tasks = ["harvest_1_log", "harvest_1_dirt", "harvest_1_apple"]
    
    for task_id in tasks:
        print(f"\nè¯„ä¼°ä»»åŠ¡: {task_id}")
        result = evaluator.evaluate_task(task_id, "en", n_trials=5)
        print(f"  æˆåŠŸç‡: {result['success_rate']:.1f}%")

if __name__ == "__main__":
    main()
```

è¿è¡Œ:

```bash
scripts/run_minedojo_x86.sh python my_evaluation.py
```

### è¯„ä¼°ä¸­æ–‡æŒ‡ä»¤

```python
# è¯„ä¼°ä¸­æ–‡æŒ‡ä»¤ï¼ˆè‡ªåŠ¨ç¿»è¯‘ï¼‰
result_zh = evaluator.evaluate_task(
    task_id="harvest_1_log",
    lang="zh_auto",
    n_trials=10,
    instruction_override="ç æ ‘"  # ä½¿ç”¨ä¸­æ–‡æŒ‡ä»¤
)

# è¯„ä¼°è‹±æ–‡æŒ‡ä»¤
result_en = evaluator.evaluate_task(
    task_id="harvest_1_log",
    lang="en",
    n_trials=10
)

# è®¡ç®—Gap
gap = abs(result_en['success_rate'] - result_zh['success_rate'])
print(f"ä¸­è‹±æ–‡Gap: {gap:.1f}%")
```

---

## ğŸ“‹ ä»»åŠ¡åˆ—è¡¨

### Quick Test Setï¼ˆå¿«é€Ÿæµ‹è¯•é›†ï¼‰

1. `harvest_1_log` - ç æ ‘ï¼ˆç®€å•ï¼‰
2. `harvest_1_dirt` - é‡‡é›†æ³¥åœŸï¼ˆç®€å•ï¼‰
3. `combat_cow_forest_barehand` - ç©ºæ‰‹æ€ç‰›ï¼ˆä¸­ç­‰ï¼‰

### Baseline Test Setï¼ˆåŸºçº¿æµ‹è¯•é›†ï¼‰

1. `harvest_1_log` - ç æ ‘
2. `harvest_1_apple` - é‡‡é›†è‹¹æœ
3. `harvest_1_crafting_table` - åˆ¶ä½œå·¥ä½œå°
4. `harvest_1_iron_ingot` - å†¶ç‚¼é“é”­
5. `combat_zombie_forest_leather_armors_wooden_sword_shield` - å‡»æ€åƒµå°¸
6. `techtree_from_barehand_to_wooden_pickaxe` - åˆ¶ä½œæœ¨é•
7. `techtree_from_barehand_to_iron_sword` - åˆ¶ä½œé“å‰‘
8. `harvest_1_cooked_beef` - çƒ¹é¥ªç†Ÿç‰›è‚‰
9. `harvest_1_torch` - åˆ¶ä½œç«æŠŠ
10. `combat_spider_plains_iron_armors_iron_sword_shield` - å‡»æ€èœ˜è››

å®Œæ•´ä»»åŠ¡åˆ—è¡¨è§: `config/eval_tasks.yaml`

---

## ğŸš¨ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•æŒ‡å®šä½¿ç”¨GPU?

```python
evaluator = STEVE1Evaluator(device="auto")  # è‡ªåŠ¨é€‰æ‹©
# æˆ–
evaluator = STEVE1Evaluator(device="cuda")  # å¼ºåˆ¶ä½¿ç”¨CUDA
```

### Q2: è¯„ä¼°å¾ˆæ…¢æ€ä¹ˆåŠ?

- å‡å°‘trialsæ•°é‡: `n_trials=3` è€Œä¸æ˜¯ `10`
- ä½¿ç”¨quick_testè€Œä¸æ˜¯baseline_evaluation
- ç¡®ä¿ä½¿ç”¨GPUåŠ é€Ÿ

### Q3: MineDojoç¯å¢ƒåˆ›å»ºå¤±è´¥?

ç¡®ä¿ä½¿ç”¨`run_minedojo_x86.sh`å¯åŠ¨:

```bash
scripts/run_minedojo_x86.sh python your_script.py
```

### Q4: å¦‚ä½•æŸ¥çœ‹è¯¦ç»†æ—¥å¿—?

```python
import logging
logging.basicConfig(level=logging.DEBUG)  # è®¾ç½®ä¸ºDEBUGçº§åˆ«
```

### Q5: å¦‚ä½•åªæµ‹è¯•ä¸­æ–‡æŒ‡ä»¤?

```python
result = evaluator.evaluate_task(
    "harvest_1_log",
    lang="zh_auto",
    n_trials=10,
    instruction_override="ç æ ‘"
)
```

---

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡è¯´æ˜

### ä¸»è¦æŒ‡æ ‡

1. **EN Success Rate** - è‹±æ–‡baselineæˆåŠŸç‡
2. **ZH(Auto) Success Rate** - ä¸­æ–‡è‡ªåŠ¨ç¿»è¯‘æˆåŠŸç‡
3. **Language Gap** - ä¸­è‹±æ–‡æˆåŠŸç‡å·®è·
4. **Semantic Variance** - è¯­ä¹‰å˜ä½“é²æ£’æ€§ï¼ˆæ–¹å·®ï¼‰

### æŒ‡æ ‡è§£è¯»

- **Gapè¶Šå°è¶Šå¥½** - è¯´æ˜ç¿»è¯‘è´¨é‡å¥½
- **Varianceè¶Šå°è¶Šå¥½** - è¯´æ˜å¯¹ä¸åŒè¡¨è¿°é²æ£’
- **Success Rate** - ç»å¯¹æˆåŠŸç‡

---

## ğŸ¯ ä¸‹ä¸€æ­¥

è¯„ä¼°å®Œæˆåï¼Œå¯ä»¥:

1. åˆ†ææŠ¥å‘Šï¼Œè¯†åˆ«é—®é¢˜ä»»åŠ¡
2. ä¼˜åŒ–æœ¯è¯­è¯å…¸ (`data/chinese_terms.json`)
3. å¾®è°ƒæ¨¡å‹æé«˜æˆåŠŸç‡
4. æ‰©å±•è¯„ä¼°ä»»åŠ¡é›†

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- **æŠ€æœ¯æ–¹æ¡ˆ**: `docs/design/CHINESE_AIMC_AGENT_TECHNICAL_PLAN.md`
- **Phase 1æ€»ç»“**: `docs/status/PHASE1_STEVE1_INTEGRATION_COMPLETE.md`
- **ä»»åŠ¡é…ç½®**: `config/eval_tasks.yaml`
- **æœ¯è¯­è¯å…¸**: `data/chinese_terms.json`

---

**æ›´æ–°æ—¶é—´**: 2025-11-06  
**ç»´æŠ¤è€…**: AIMC Team

