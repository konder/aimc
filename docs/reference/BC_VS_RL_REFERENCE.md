# è¡Œä¸ºå…‹éš†(BC) vs å¼ºåŒ–å­¦ä¹ (RL) å®ç°ç»†èŠ‚å¯¹æ¯”

> **æ ¸å¿ƒé—®é¢˜**: STEVE-1ç”¨çš„æ˜¯BCè¿˜æ˜¯RLï¼Ÿä¸¤è€…åœ¨å®ç°ä¸Šæœ‰ä»€ä¹ˆå…·ä½“åŒºåˆ«ï¼Ÿ

---

## ğŸ¯ ç›´æ¥ç­”æ¡ˆ

**STEVE-1ä½¿ç”¨çš„æ˜¯è¡Œä¸ºå…‹éš†ï¼ˆBehavior Cloning, BCï¼‰**ï¼Œä¸æ˜¯ä¼ ç»Ÿçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ã€‚

è™½ç„¶å®ƒä½¿ç”¨äº†Goal-Conditionedçš„æ€æƒ³ï¼ˆæ¥è‡ªRLé¢†åŸŸï¼‰ï¼Œä½†**è®­ç»ƒæ–¹å¼æ˜¯çº¯ç›‘ç£å­¦ä¹ **ã€‚

---

## ğŸ“Š æ ¸å¿ƒåŒºåˆ«å¯¹æ¯”è¡¨

| ç»´åº¦ | è¡Œä¸ºå…‹éš† (BC) | å¼ºåŒ–å­¦ä¹  (RL) |
|------|--------------|---------------|
| **å­¦ä¹ æ–¹å¼** | ç›‘ç£å­¦ä¹  | è¯•é”™å­¦ä¹  |
| **éœ€è¦çš„æ•°æ®** | ä¸“å®¶æ¼”ç¤º | ç¯å¢ƒäº¤äº’ |
| **æŸå¤±å‡½æ•°** | æ¨¡ä»¿æŸå¤±ï¼ˆäº¤å‰ç†µ/MSEï¼‰ | ç­–ç•¥æ¢¯åº¦/Qå­¦ä¹  |
| **æ˜¯å¦éœ€è¦å¥–åŠ±** | âŒ ä¸éœ€è¦ | âœ… éœ€è¦ |
| **æ˜¯å¦éœ€è¦ç¯å¢ƒäº¤äº’** | âŒ ä¸éœ€è¦ | âœ… éœ€è¦ |
| **è®­ç»ƒé€Ÿåº¦** | å¿« | æ…¢ |
| **æ•°æ®æ•ˆç‡** | é«˜ | ä½ |
| **STEVE-1ä½¿ç”¨** | âœ… æ˜¯ | âŒ å¦ |

---

## ğŸ” å®ç°ç»†èŠ‚æ·±å…¥å¯¹æ¯”

### 1. è®­ç»ƒå¾ªç¯çš„æ ¹æœ¬åŒºåˆ«

#### BC (STEVE-1ä½¿ç”¨çš„æ–¹æ³•)

```python
# è¡Œä¸ºå…‹éš† - çº¯ç›‘ç£å­¦ä¹ 
# æ–‡ä»¶: src/training/steve1/training/train.py (ç®€åŒ–ç‰ˆ)

for epoch in range(num_epochs):
    for obs, actions, firsts in dataloader:  # â† ä»ç¦»çº¿æ•°æ®é›†åŠ è½½
        # obs: ä¸“å®¶çœ‹åˆ°çš„è§‚å¯Ÿ
        # actions: ä¸“å®¶æ‰§è¡Œçš„åŠ¨ä½œï¼ˆæ ‡ç­¾ï¼‰
        # firsts: åºåˆ—è¾¹ç•Œæ ‡è®°
        
        # å‰å‘ä¼ æ’­ï¼šé¢„æµ‹åŠ¨ä½œåˆ†å¸ƒ
        pi_logits, vpred, hidden_state = policy(obs, hidden_state, firsts)
        
        # è®¡ç®—æŸå¤±ï¼šè®©æ¨¡å‹è¾“å‡ºæ¥è¿‘ä¸“å®¶åŠ¨ä½œ
        log_prob = compute_log_prob(pi_logits, actions)  # ä¸“å®¶åŠ¨ä½œçš„å¯¹æ•°ä¼¼ç„¶
        loss = -log_prob.mean()  # è´Ÿå¯¹æ•°ä¼¼ç„¶ = äº¤å‰ç†µ
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # â­ å…³é”®ï¼šæ²¡æœ‰ç¯å¢ƒäº¤äº’ï¼Œæ²¡æœ‰å¥–åŠ±
```

**å…³é”®ç‰¹å¾**ï¼š
- âœ… æ•°æ®æ¥è‡ª**ç¦»çº¿æ•°æ®é›†**ï¼ˆé¢„å…ˆå½•åˆ¶çš„ä¸“å®¶æ¼”ç¤ºï¼‰
- âœ… æŸå¤±å‡½æ•°æ˜¯**æ¨¡ä»¿æŸå¤±**ï¼šè®©æ¨¡å‹è¾“å‡ºæ¥è¿‘ä¸“å®¶
- âœ… ä¸éœ€è¦ä¸ç¯å¢ƒäº¤äº’
- âœ… ä¸éœ€è¦å¥–åŠ±ä¿¡å·

#### RL (ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ )

```python
# å¼ºåŒ–å­¦ä¹  - åœ¨çº¿å­¦ä¹ 
# ä¾‹å¦‚: PPOç®—æ³•

for iteration in range(num_iterations):
    # 1. æ”¶é›†æ•°æ®ï¼šä¸ç¯å¢ƒäº¤äº’
    trajectory = []
    state = env.reset()
    for t in range(episode_length):
        # ç”¨å½“å‰ç­–ç•¥é‡‡æ ·åŠ¨ä½œ
        action = policy.sample(state)
        
        # ä¸ç¯å¢ƒäº¤äº’ï¼Œè·å¾—å¥–åŠ±
        next_state, reward, done, info = env.step(action)  # â† å…³é”®ï¼
        
        trajectory.append((state, action, reward, next_state, done))
        state = next_state
        
        if done:
            break
    
    # 2. è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿
    returns = compute_returns(trajectory)  # ä½¿ç”¨å¥–åŠ±
    advantages = compute_advantages(trajectory, value_function)
    
    # 3. ç­–ç•¥ä¼˜åŒ–
    for epoch in range(ppo_epochs):
        # è®¡ç®—ç­–ç•¥æ¢¯åº¦æŸå¤±
        log_prob = policy.log_prob(actions, states)
        ratio = torch.exp(log_prob - old_log_prob)
        
        # PPOè£å‰ªç›®æ ‡
        loss = -torch.min(
            ratio * advantages,
            torch.clamp(ratio, 1-epsilon, 1+epsilon) * advantages
        ).mean()
        
        loss.backward()
        optimizer.step()
    
    # â­ å…³é”®ï¼šéœ€è¦ç¯å¢ƒäº¤äº’ï¼Œéœ€è¦å¥–åŠ±ä¿¡å·
```

**å…³é”®ç‰¹å¾**ï¼š
- âœ… éœ€è¦**åœ¨çº¿ä¸ç¯å¢ƒäº¤äº’**
- âœ… éœ€è¦**å¥–åŠ±ä¿¡å·** r(s,a)
- âœ… é€šè¿‡è¯•é”™å­¦ä¹ 
- âœ… ä¼˜åŒ–ç´¯ç§¯å›æŠ¥

---

### 2. æŸå¤±å‡½æ•°çš„å…·ä½“åŒºåˆ«

#### BCæŸå¤±ï¼ˆSTEVE-1ï¼‰

```python
# è¡Œä¸ºå…‹éš†æŸå¤± = è´Ÿå¯¹æ•°ä¼¼ç„¶ = äº¤å‰ç†µ

def bc_loss(policy_output, expert_actions):
    """
    è®©æ¨¡å‹è¾“å‡ºçš„åŠ¨ä½œåˆ†å¸ƒå°½å¯èƒ½æ¥è¿‘ä¸“å®¶åŠ¨ä½œ
    """
    # 1. æ¨¡å‹é¢„æµ‹åŠ¨ä½œåˆ†å¸ƒ
    pi_logits = policy_output['logits']  # [B, T, action_dim]
    
    # 2. ä¸“å®¶åŠ¨ä½œï¼ˆæ ‡ç­¾ï¼‰
    expert_actions = expert_actions  # [B, T, action_dim]
    
    # 3. è®¡ç®—ä¸“å®¶åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
    log_prob_buttons = F.cross_entropy(
        pi_logits['buttons'], 
        expert_actions['buttons']
    )
    log_prob_camera = F.cross_entropy(
        pi_logits['camera'],
        expert_actions['camera']
    )
    
    # 4. æ€»æŸå¤±
    loss = log_prob_buttons + log_prob_camera
    
    return loss

# ä¼˜åŒ–ç›®æ ‡ï¼š
# max E[log Ï€(a_expert | s)]
# = min -E[log Ï€(a_expert | s)]  â† BCæŸå¤±
```

**å«ä¹‰**ï¼šæœ€å¤§åŒ–ä¸“å®¶åŠ¨ä½œçš„ä¼¼ç„¶ï¼Œè®©æ¨¡å‹å­¦ä¼šæ¨¡ä»¿ä¸“å®¶ã€‚

#### RLæŸå¤±ï¼ˆä¾‹å¦‚PPOï¼‰

```python
# å¼ºåŒ–å­¦ä¹ æŸå¤± = ç­–ç•¥æ¢¯åº¦ + ä»·å€¼å‡½æ•°

def rl_loss(policy_output, trajectory, old_policy):
    """
    ä¼˜åŒ–ç´¯ç§¯å›æŠ¥
    """
    # 1. è®¡ç®—ä¼˜åŠ¿å‡½æ•°
    returns = compute_returns(trajectory)  # åŸºäºå¥–åŠ±
    values = value_function(states)
    advantages = returns - values  # A(s,a) = Q(s,a) - V(s)
    
    # 2. ç­–ç•¥æ¢¯åº¦æŸå¤±ï¼ˆPPOè£å‰ªï¼‰
    log_prob = policy.log_prob(actions, states)
    old_log_prob = old_policy.log_prob(actions, states)
    ratio = torch.exp(log_prob - old_log_prob)
    
    clipped_ratio = torch.clamp(ratio, 1-epsilon, 1+epsilon)
    policy_loss = -torch.min(
        ratio * advantages,
        clipped_ratio * advantages
    ).mean()
    
    # 3. ä»·å€¼å‡½æ•°æŸå¤±
    value_loss = F.mse_loss(values, returns)
    
    # 4. ç†µæ­£åˆ™åŒ–
    entropy = policy.entropy(states)
    
    # 5. æ€»æŸå¤±
    total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy
    
    return total_loss

# ä¼˜åŒ–ç›®æ ‡ï¼š
# max E[Î£ Î³^t * r_t]  â† ç´¯ç§¯å›æŠ¥
```

**å«ä¹‰**ï¼šæœ€å¤§åŒ–æœŸæœ›ç´¯ç§¯å›æŠ¥ï¼Œè®©æ¨¡å‹å­¦ä¼šè·å¾—é«˜å¥–åŠ±ã€‚

---

### 3. æ•°æ®æ¥æºçš„åŒºåˆ«

#### BCæ•°æ®ï¼ˆç¦»çº¿ï¼‰

```python
# STEVE-1æ•°æ®å‡†å¤‡
# 1. æ”¶é›†ä¸“å®¶æ¼”ç¤ºï¼ˆä¸€æ¬¡æ€§ï¼‰
episodes = download_vpt_dataset()  # äººç±»ç©å®¶å½•åƒ

# 2. ä¿å­˜ä¸ºç¦»çº¿æ•°æ®é›†
for episode in episodes:
    save_episode(episode, 'data/dataset/')

# 3. è®­ç»ƒæ—¶åŠ è½½ï¼ˆæ— éœ€ç¯å¢ƒï¼‰
dataset = MinecraftDataset('data/dataset/')
dataloader = DataLoader(dataset, batch_size=12)

# 4. è®­ç»ƒ
for obs, actions, firsts in dataloader:
    loss = bc_loss(policy(obs), actions)
    loss.backward()
    
# â­ ä¸éœ€è¦è¿è¡ŒMinecraftç¯å¢ƒ
```

#### RLæ•°æ®ï¼ˆåœ¨çº¿ï¼‰

```python
# ä¼ ç»ŸRLæ•°æ®æ”¶é›†
# 1. éœ€è¦è¿è¡Œç¯å¢ƒ
env = MinecraftEnv()

# 2. åœ¨çº¿æ”¶é›†æ•°æ®
for iteration in range(num_iterations):
    trajectory = []
    state = env.reset()  # â† å¯åŠ¨æ¸¸æˆ
    
    for t in range(episode_length):
        action = policy.sample(state)
        next_state, reward, done, info = env.step(action)  # â† è¿è¡Œæ¸¸æˆ
        
        trajectory.append((state, action, reward, next_state))
        state = next_state
    
    # 3. ç«‹å³ç”¨äºè®­ç»ƒ
    loss = rl_loss(policy, trajectory)
    loss.backward()

# â­ å¿…é¡»æŒç»­è¿è¡Œç¯å¢ƒ
```

---

### 4. æ˜¯å¦éœ€è¦å¥–åŠ±å‡½æ•°

#### BC - ä¸éœ€è¦å¥–åŠ±

```python
# STEVE-1è®­ç»ƒæ ·æœ¬
training_sample = {
    'img': frames[t],              # è§‚å¯Ÿ
    'mineclip_embed': embeds[t+N], # æ¡ä»¶
    'action': actions[t]           # æ ‡ç­¾ï¼ˆä¸“å®¶åŠ¨ä½œï¼‰
}

# æŸå¤±è®¡ç®—
loss = -log P(action_expert | img, embed)

# â­ æ²¡æœ‰rewardï¼åªæœ‰ä¸“å®¶åŠ¨ä½œä½œä¸ºç›‘ç£ä¿¡å·
```

#### RL - å¿…é¡»æœ‰å¥–åŠ±

```python
# RLè®­ç»ƒéœ€è¦å¥–åŠ±å‡½æ•°
def reward_function(state, action, next_state):
    """å®šä¹‰ä»€ä¹ˆæ˜¯"å¥½"çš„è¡Œä¸º"""
    reward = 0
    
    # ä¾‹å¦‚ï¼šç æ ‘ä»»åŠ¡
    if has_wood_in_inventory(next_state):
        reward += 10.0  # è·å¾—æœ¨å¤´ â†’ æ­£å¥–åŠ±
    
    if health_decreased(state, next_state):
        reward -= 1.0   # å—ä¼¤ â†’ è´Ÿå¥–åŠ±
    
    return reward

# è®­ç»ƒæ—¶ä½¿ç”¨
state, action, reward, next_state = env.step(action)
returns = compute_returns([reward1, reward2, ...])
loss = policy_gradient_loss(returns)

# â­ å¥–åŠ±æ˜¯å­¦ä¹ ä¿¡å·çš„æ ¸å¿ƒ
```

---

## ğŸ“ ä¸ºä»€ä¹ˆSTEVE-1é€‰æ‹©BCè€Œä¸æ˜¯RLï¼Ÿ

### BCçš„ä¼˜åŠ¿

```
1. æ•°æ®æ•ˆç‡é«˜
   âœ… BC: 100å°æ—¶äººç±»å½•åƒ â†’ è®­ç»ƒå®Œæˆ
   âŒ RL: éœ€è¦æ•°ç™¾ä¸‡æ­¥ç¯å¢ƒäº¤äº’

2. è®­ç»ƒé€Ÿåº¦å¿«
   âœ… BC: æ•°å¤©GPUè®­ç»ƒ
   âŒ RL: æ•°å‘¨ç”šè‡³æ•°æœˆ

3. ä¸éœ€è¦å¥–åŠ±å‡½æ•°
   âœ… BC: ç›´æ¥ä»æ¼”ç¤ºå­¦ä¹ 
   âŒ RL: éœ€è¦ç²¾å¿ƒè®¾è®¡å¥–åŠ±å‡½æ•°ï¼ˆéå¸¸éš¾ï¼ï¼‰

4. ç¨³å®šæ€§å¥½
   âœ… BC: ç›‘ç£å­¦ä¹ ï¼Œæ”¶æ•›ç¨³å®š
   âŒ RL: è®­ç»ƒä¸ç¨³å®šï¼Œå®¹æ˜“å‘æ•£

5. é€‚åˆå¤æ‚ä»»åŠ¡
   âœ… BC: å¯ä»¥å­¦ä¹ å¤æ‚çš„äººç±»è¡Œä¸º
   âŒ RL: å¤æ‚ä»»åŠ¡å¾ˆéš¾é€šè¿‡å¥–åŠ±å‡½æ•°å®šä¹‰
```

### RLçš„ä¼˜åŠ¿

```
1. å¯ä»¥è¶…è¶Šä¸“å®¶
   âœ… RL: é€šè¿‡æ¢ç´¢å‘ç°æ›´å¥½çš„ç­–ç•¥
   âŒ BC: ä¸Šé™æ˜¯ä¸“å®¶æ°´å¹³

2. ä¸éœ€è¦æ¼”ç¤ºæ•°æ®
   âœ… RL: ä»é›¶å¼€å§‹å­¦ä¹ 
   âŒ BC: å¿…é¡»æœ‰é«˜è´¨é‡æ¼”ç¤º

3. å¯ä»¥åœ¨çº¿é€‚åº”
   âœ… RL: æŒç»­ä¸ç¯å¢ƒäº¤äº’ï¼Œé€‚åº”å˜åŒ–
   âŒ BC: ç¦»çº¿è®­ç»ƒï¼Œéš¾ä»¥é€‚åº”æ–°æƒ…å†µ
```

### STEVE-1çš„é€‰æ‹©

```
Minecraftæ˜¯éå¸¸å¤æ‚çš„å¼€æ”¾ä¸–ç•Œæ¸¸æˆï¼š

é—®é¢˜1: å¦‚ä½•ç”¨RLå®šä¹‰å¥–åŠ±ï¼Ÿ
  - ç æ ‘ï¼ŸæŒ–çŸ¿ï¼Ÿå»ºé€ ï¼Ÿæ¢ç´¢ï¼Ÿ
  - å¤ªéš¾äº†ï¼Œæ— æ³•ç”¨ç®€å•çš„rewardå‡½æ•°å®šä¹‰

è§£å†³: BC
  - äººç±»æ¼”ç¤ºå·²ç»åŒ…å«äº†å„ç§å¤æ‚è¡Œä¸º
  - ç›´æ¥å­¦ä¹ æ¨¡ä»¿ï¼Œæ— éœ€å®šä¹‰å¥–åŠ±

é—®é¢˜2: RLéœ€è¦å¤§é‡ç¯å¢ƒäº¤äº’
  - Minecraftç¯å¢ƒè¿è¡Œæ…¢
  - æ¢ç´¢ç©ºé—´å·¨å¤§

è§£å†³: BC
  - ä½¿ç”¨YouTube/VPTç°æˆçš„äººç±»å½•åƒ
  - ç¦»çº¿è®­ç»ƒï¼Œå¿«é€Ÿé«˜æ•ˆ
```

---

## ğŸ’» ä»£ç çº§å¯¹æ¯”

### STEVE-1å®é™…è®­ç»ƒä»£ç ï¼ˆBCï¼‰

```python
# src/training/steve1/training/train.py (ç®€åŒ–)

def train_bc(policy, dataloader, optimizer, device):
    """è¡Œä¸ºå…‹éš†è®­ç»ƒ"""
    
    policy.train()
    total_loss = 0
    
    for batch_idx, (obs, actions, firsts) in enumerate(dataloader):
        # obs: ä¸“å®¶è§‚å¯Ÿ
        # actions: ä¸“å®¶åŠ¨ä½œï¼ˆç›‘ç£æ ‡ç­¾ï¼‰
        
        # ç§»åˆ°GPU
        obs = {k: v.to(device) for k, v in obs.items()}
        actions = {k: v.to(device) for k, v in actions.items()}
        
        # å‰å‘ä¼ æ’­
        hidden_state = None
        total_batch_loss = 0
        
        for t in range(0, T, truncation_length):
            obs_chunk = slice_obs(obs, t, t + truncation_length)
            action_chunk = slice_actions(actions, t, t + truncation_length)
            
            # æ¨¡å‹é¢„æµ‹
            pi_logits, vpred, hidden_state = policy(
                obs_chunk, 
                hidden_state, 
                firsts[:, t:t+truncation_length]
            )
            
            # BCæŸå¤±ï¼šè®©é¢„æµ‹æ¥è¿‘ä¸“å®¶
            log_prob = compute_action_log_prob(pi_logits, action_chunk)
            loss = -log_prob.mean()
            
            total_batch_loss += loss
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        total_batch_loss.backward()
        torch.nn.utils.clip_grad_norm_(policy.parameters(), max_grad_norm)
        optimizer.step()
        
        total_loss += total_batch_loss.item()
    
    return total_loss / len(dataloader)
```

### å‡è®¾çš„RLè®­ç»ƒä»£ç ï¼ˆå¯¹æ¯”ï¼‰

```python
# å¦‚æœç”¨RLè®­ç»ƒï¼ˆSTEVE-1å®é™…ä¸ç”¨ï¼‰

def train_rl_ppo(policy, env, optimizer):
    """å¼ºåŒ–å­¦ä¹ PPOè®­ç»ƒ"""
    
    for iteration in range(num_iterations):
        # 1. æ”¶é›†è½¨è¿¹
        trajectories = []
        
        for episode in range(episodes_per_iteration):
            obs = env.reset()
            episode_data = []
            
            for t in range(max_steps):
                # é‡‡æ ·åŠ¨ä½œ
                with torch.no_grad():
                    pi_logits, vpred, _ = policy(obs, hidden_state, first)
                    action = sample_action(pi_logits)
                
                # ç¯å¢ƒäº¤äº’ â† å…³é”®åŒºåˆ«
                next_obs, reward, done, info = env.step(action)
                
                episode_data.append({
                    'obs': obs,
                    'action': action,
                    'reward': reward,  # â† BCæ²¡æœ‰è¿™ä¸ª
                    'value': vpred,
                    'done': done
                })
                
                obs = next_obs
                if done:
                    break
            
            trajectories.append(episode_data)
        
        # 2. è®¡ç®—å›æŠ¥å’Œä¼˜åŠ¿
        for traj in trajectories:
            returns = compute_returns(traj)  # åŸºäºreward
            advantages = compute_gae(traj)
        
        # 3. ç­–ç•¥ä¼˜åŒ–
        for epoch in range(ppo_epochs):
            for batch in make_batches(trajectories):
                # è®¡ç®—æ–°æ—§ç­–ç•¥æ¯”ç‡
                log_prob_new = policy.log_prob(batch['actions'], batch['obs'])
                log_prob_old = batch['old_log_prob']
                ratio = torch.exp(log_prob_new - log_prob_old)
                
                # PPOè£å‰ªæŸå¤±
                clipped_ratio = torch.clamp(ratio, 1-epsilon, 1+epsilon)
                policy_loss = -torch.min(
                    ratio * batch['advantages'],
                    clipped_ratio * batch['advantages']
                ).mean()
                
                # ä»·å€¼å‡½æ•°æŸå¤±
                value_loss = F.mse_loss(
                    policy.value(batch['obs']),
                    batch['returns']
                )
                
                # æ€»æŸå¤±
                loss = policy_loss + 0.5 * value_loss
                
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
```

---

## ğŸ”‘ æ ¸å¿ƒåŒºåˆ«æ€»ç»“

### BCï¼ˆSTEVE-1ä½¿ç”¨ï¼‰

```python
# è®­ç»ƒå¾ªç¯
for obs, actions in offline_dataset:  # â† ç¦»çº¿æ•°æ®
    pred_actions = policy(obs)
    loss = -log P(actions | obs)      # â† æ¨¡ä»¿æŸå¤±
    loss.backward()

# ç‰¹ç‚¹
âœ… ç›‘ç£å­¦ä¹ 
âœ… ç¦»çº¿è®­ç»ƒ
âœ… ä¸éœ€è¦ç¯å¢ƒ
âœ… ä¸éœ€è¦å¥–åŠ±
âœ… å¿«é€Ÿé«˜æ•ˆ
```

### RLï¼ˆä¼ ç»Ÿæ–¹æ³•ï¼‰

```python
# è®­ç»ƒå¾ªç¯
for iteration in range(N):
    obs = env.reset()                 # â† åœ¨çº¿äº¤äº’
    action = policy.sample(obs)
    next_obs, reward, done = env.step(action)  # â† éœ€è¦ç¯å¢ƒ
    
    returns = compute_returns(rewards)  # â† éœ€è¦å¥–åŠ±
    loss = policy_gradient(returns)    # â† RLæŸå¤±
    loss.backward()

# ç‰¹ç‚¹
âœ… å¼ºåŒ–å­¦ä¹ 
âœ… åœ¨çº¿è®­ç»ƒ
âœ… éœ€è¦ç¯å¢ƒäº¤äº’
âœ… éœ€è¦å¥–åŠ±å‡½æ•°
âœ… å¯ä»¥è¶…è¶Šä¸“å®¶
```

---

## ğŸ“ å®é™…æ–‡ä»¶å¯¹æ¯”

### STEVE-1ï¼ˆBCï¼‰å…³é”®æ–‡ä»¶

```bash
# è®­ç»ƒè„šæœ¬ï¼ˆæ— ç¯å¢ƒäº¤äº’ï¼‰
src/training/steve1/training/train.py
  â”œâ”€ åŠ è½½ç¦»çº¿æ•°æ®é›†
  â”œâ”€ è®¡ç®—BCæŸå¤±
  â””â”€ çº¯ç›‘ç£å­¦ä¹ 

# æ•°æ®é›†ï¼ˆç¦»çº¿ï¼‰
src/training/steve1/data/minecraft_dataset.py
  â”œâ”€ ä»æ–‡ä»¶åŠ è½½ä¸“å®¶æ¼”ç¤º
  â””â”€ æ— ç¯å¢ƒäº¤äº’

# æ²¡æœ‰å¥–åŠ±å‡½æ•°å®šä¹‰
# æ²¡æœ‰ç¯å¢ƒäº¤äº’ä»£ç 
```

### å¦‚æœæ˜¯RLï¼ˆVPTçš„RLå¾®è°ƒéƒ¨åˆ†ï¼‰

```bash
# RLå¾®è°ƒè„šæœ¬ï¼ˆæœ‰ç¯å¢ƒäº¤äº’ï¼‰
src/training/vpt/behavioural_cloning.py
  â”œâ”€ å…ˆBCé¢„è®­ç»ƒ
  â””â”€ ç„¶åå¯é€‰RLå¾®è°ƒ

# ç¯å¢ƒäº¤äº’
src/envs/
  â”œâ”€ MineDojoç¯å¢ƒå°è£…
  â””â”€ ç”¨äºRLäº¤äº’

# å¥–åŠ±å®šä¹‰
src/training/vpt/reward_shaping.py
  â””â”€ å®šä¹‰ä»»åŠ¡ç‰¹å®šå¥–åŠ±
```

---

## ğŸ¯ æœ€ç»ˆç­”æ¡ˆ

**STEVE-1æ˜¯çº¯BCï¼ˆè¡Œä¸ºå…‹éš†ï¼‰**ï¼š

1. âœ… **è®­ç»ƒæ–¹å¼**ï¼šç›‘ç£å­¦ä¹ ï¼Œä¸æ˜¯å¼ºåŒ–å­¦ä¹ 
2. âœ… **æ•°æ®æ¥æº**ï¼šç¦»çº¿äººç±»æ¼”ç¤ºï¼ˆVPTæ•°æ®é›†ï¼‰
3. âœ… **æŸå¤±å‡½æ•°**ï¼šè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆäº¤å‰ç†µï¼‰
4. âœ… **æ— éœ€ç¯å¢ƒ**ï¼šä¸éœ€è¦è¿è¡ŒMinecraftè¿›è¡Œè®­ç»ƒ
5. âœ… **æ— éœ€å¥–åŠ±**ï¼šç›´æ¥ä»ä¸“å®¶åŠ¨ä½œå­¦ä¹ 

è™½ç„¶å®ƒä½¿ç”¨äº†**Goal-Conditioned**çš„æ€æƒ³ï¼ˆæ¥è‡ªRLï¼‰ï¼Œä½†å®ç°ä¸Šæ˜¯**çº¯ç›‘ç£å­¦ä¹ **ã€‚

---

**ç›¸å…³æ–‡æ¡£**:
- æ•°æ®æµç¨‹: `docs/guides/STEVE1_DATA_FLOW_EXPLAINED.md`
- è®­ç»ƒåˆ†æ: `docs/technical/STEVE1_TRAINING_ANALYSIS.md`
- ä»£ç ä½ç½®: `src/training/steve1/training/train.py`

