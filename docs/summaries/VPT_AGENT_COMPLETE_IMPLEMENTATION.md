# VPT Agentå®Œæ•´ç‰ˆå®ç°æ€»ç»“

## ğŸ“… æ—¥æœŸ
2025-10-27

## ğŸ¯ ç›®æ ‡
åŸºäºOpenAI VPTå®˜æ–¹å®ç°ï¼ˆ[agent.py](https://github.com/openai/Video-Pre-Training/blob/main/agent.py)ï¼‰ï¼Œåˆ›å»ºå®Œæ•´ç‰ˆVPT Agentï¼ŒçœŸæ­£å‘æŒ¥VPTçš„é¢„è®­ç»ƒèƒ½åŠ›ã€‚

---

## ğŸ” é—®é¢˜åˆ†æ

### ä¹‹å‰çš„ç®€åŒ–ç‰ˆï¼ˆ`vpt_agent.py`ï¼‰å­˜åœ¨çš„é—®é¢˜

#### 1. **æ²¡æœ‰Hidden Stateï¼ˆæ— è®°å¿†ï¼‰**
```python
def predict(self, obs, deterministic=True):
    # æ¯æ¬¡è°ƒç”¨éƒ½æ˜¯ç‹¬ç«‹çš„
    x = self.vpt_policy.img_preprocess(agent_input)
    x = self.vpt_policy.img_process(x)
    vpt_features = self.vpt_policy.lastlayer(x)  # âŒ æ²¡æœ‰ä¼ å…¥state_in
    # ...
```

**å½±å“**ï¼š
- æ¯å¸§ç‹¬ç«‹å†³ç­–ï¼Œæ— æ³•è®°ä½"æ­£åœ¨ç æ ‘"ç­‰ä»»åŠ¡çŠ¶æ€
- çœ‹åˆ°æ ‘åå¯èƒ½å‰è¿›ã€åé€€ã€è·³è·ƒéšæœºåˆ‡æ¢
- æ— æ³•æ‰§è¡Œéœ€è¦å¤šæ­¥éª¤çš„ä»»åŠ¡

#### 2. **ä½¿ç”¨å¯å‘å¼è§„åˆ™è€Œä¸æ˜¯VPTçš„pi_head**
```python
def _generate_minedojo_action(self, vpt_features):
    # âŒ å®Œå…¨éšæœºï¼
    if rand_val < 0.7:
        minedojo_action[0] = 0  # 70%å‰è¿›
    if rand_val < 0.8:
        minedojo_action[2] = 0  # 80%è·³è·ƒ
    # ...
```

**å½±å“**ï¼š
- VPTå­¦åˆ°çš„70Kå°æ—¶æ¸¸æˆçŸ¥è¯†å®Œå…¨æ²¡ç”¨ä¸Š
- ä¸ä¼šæ ¹æ®ç”»é¢å†…å®¹åšæ™ºèƒ½å†³ç­–
- è¡Œä¸ºå®Œå…¨éšæœº

#### 3. **æ²¡æœ‰ä½¿ç”¨VPTçš„å®Œæ•´forwardæµç¨‹**
```python
# âŒ æ‰‹åŠ¨è°ƒç”¨å„å±‚
x = self.vpt_policy.img_preprocess(agent_input)
x = self.vpt_policy.img_process(x)
x = x.squeeze(1)
vpt_features = self.vpt_policy.lastlayer(x)

# âŒ è·³è¿‡äº†recurrentå±‚å’Œpi_head
```

**å½±å“**ï¼š
- VPTçš„Transformer/LSTMå±‚ï¼ˆæ—¶åºç†è§£ï¼‰æ²¡æœ‰ä½¿ç”¨
- pi_headï¼ˆåŠ¨ä½œå†³ç­–ï¼‰æ²¡æœ‰ä½¿ç”¨
- åªç”¨äº†è§†è§‰ç‰¹å¾æå–ï¼Œç›¸å½“äºåªç”¨äº†VPTçš„"çœ¼ç›"

---

## ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šå®Œæ•´ç‰ˆVPT Agent

### æ ¸å¿ƒæ”¹è¿›

#### 1. **ç»´æŠ¤Hidden Stateï¼ˆè®°å¿†ï¼‰**
```python
class VPTAgentComplete(AgentBase):
    def __init__(self, ...):
        # âœ“ åˆå§‹åŒ–hidden state
        self.hidden_state = self.policy.initial_state(1)
    
    def reset(self):
        """âœ“ Episodeå¼€å§‹æ—¶é‡ç½®è®°å¿†"""
        self.hidden_state = self.policy.initial_state(1)
    
    def predict(self, obs, deterministic=True):
        # âœ“ ä½¿ç”¨å¹¶æ›´æ–°hidden state
        agent_action, self.hidden_state, _ = self.policy.act(
            agent_input,
            self._dummy_first,
            self.hidden_state,  # â† è¾“å…¥è®°å¿†
            stochastic=not deterministic
        )
        # self.hidden_stateå·²ç»æ›´æ–° â† è¾“å‡ºæ–°è®°å¿†
```

**æ•ˆæœ**ï¼š
- âœ… Agentèƒ½è®°ä½"æ­£åœ¨æ‰§è¡Œä»€ä¹ˆä»»åŠ¡"
- âœ… æŒç»­è¡Œä¸ºï¼šçœ‹åˆ°æ ‘ â†’ èµ°å‘æ ‘ â†’ æŒç»­æ”»å‡»
- âœ… é¿å…é‡å¤ï¼šç»•è¿‡éšœç¢ä¸ä¼šå¡ä½

#### 2. **ä½¿ç”¨VPTçš„pi_headï¼ˆæ™ºèƒ½å†³ç­–ï¼‰**
```python
# âœ“ ä½¿ç”¨å®˜æ–¹çš„policy.act()
agent_action, self.hidden_state, _ = self.policy.act(
    agent_input, first, state_in, stochastic=True
)
# agent_actionæ˜¯VPTçš„pi_headè¾“å‡ºï¼ŒåŒ…å«çœŸæ­£çš„å†³ç­–ï¼
```

**æ•ˆæœ**ï¼š
- âœ… æ ¹æ®ç”»é¢å†…å®¹åšå†³ç­–ï¼ˆçœ‹åˆ°æ ‘â†’å‰è¿›ï¼Œçœ‹åˆ°æ•Œäººâ†’åé€€ï¼‰
- âœ… ä½¿ç”¨VPTçš„70Kå°æ—¶é¢„è®­ç»ƒçŸ¥è¯†
- âœ… æ™ºèƒ½è¡Œä¸ºè€Œä¸æ˜¯éšæœº

#### 3. **ä½¿ç”¨å®˜æ–¹çš„action_mapperå’Œaction_transformer**
```python
# âœ“ åˆ›å»ºå®˜æ–¹ç»„ä»¶
self.action_mapper = CameraHierarchicalMapping(n_camera_bins=11)
self.action_transformer = ActionTransformer(...)

# âœ“ å®Œæ•´çš„åŠ¨ä½œè½¬æ¢æµç¨‹
def _agent_action_to_minerl(self, agent_action):
    # VPTå†…éƒ¨è¡¨ç¤º â†’ MineRL factored
    minerl_action = self.action_mapper.to_factored(action)
    # å¤„ç†ç›¸æœºé‡åŒ–
    minerl_action_transformed = self.action_transformer.policy2env(minerl_action)
    return minerl_action_transformed
```

**æ•ˆæœ**ï¼š
- âœ… ä¸¥æ ¼æŒ‰ç…§å®˜æ–¹å®ç°
- âœ… æ­£ç¡®å¤„ç†ç›¸æœºåŠ¨ä½œçš„é‡åŒ–/åé‡åŒ–
- âœ… é¿å…åŠ¨ä½œç©ºé—´è½¬æ¢é”™è¯¯

---

## ğŸ“Š å¯¹æ¯”ï¼šç®€åŒ–ç‰ˆ vs å®Œæ•´ç‰ˆ

| ç‰¹æ€§ | ç®€åŒ–ç‰ˆ | å®Œæ•´ç‰ˆ | å½±å“ |
|------|-------|--------|------|
| **Hidden State** | âŒ æ—  | âœ… ç»´æŠ¤ | è®°å¿†ã€è¿è´¯æ€§ |
| **Firstæ ‡å¿—** | âŒ æ—  | âœ… æœ‰ | Episodeè¾¹ç•Œ |
| **VPT Forward** | âŒ æ‰‹åŠ¨è°ƒç”¨å„å±‚ | âœ… policy.act() | å®Œæ•´æ€§ |
| **åŠ¨ä½œå†³ç­–** | âŒ å¯å‘å¼è§„åˆ™ | âœ… pi_headè¾“å‡º | æ™ºèƒ½æ€§ |
| **Action Mapper** | âŒ æ‰‹å†™è½¬æ¢ | âœ… å®˜æ–¹ç»„ä»¶ | å‡†ç¡®æ€§ |
| **è¡Œä¸º** | ğŸ² éšæœº | ğŸ§  æ™ºèƒ½ | æˆåŠŸç‡ |

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬ä½¿ç”¨

```python
from src.training.agent import VPTAgentComplete
from src.envs import make_minedojo_env

# åˆ›å»ºAgent
agent = VPTAgentComplete(
    vpt_weights_path='data/pretrained/vpt/rl-from-early-game-2x.weights',
    device='cpu',
    verbose=True
)

# åˆ›å»ºç¯å¢ƒ
env = make_minedojo_env(task_id='harvest_1_log', image_size=(160, 256))

# Episodeå¾ªç¯
agent.reset()  # âš ï¸ é‡è¦ï¼æ¯ä¸ªepisodeå¼€å§‹å‰è°ƒç”¨
obs = env.reset()

for step in range(max_steps):
    action = agent.predict(obs, deterministic=True)
    obs, reward, done, info = env.step(action)
    
    if done:
        break

env.close()
```

### 2. æµ‹è¯•è„šæœ¬

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
bash scripts/run_minedojo_x86.sh python tools/test_vpt_agent_complete.py

# è¿è¡Œç‰¹å®šæµ‹è¯•
bash scripts/run_minedojo_x86.sh python tools/test_vpt_agent_complete.py --test basic
bash scripts/run_minedojo_x86.sh python tools/test_vpt_agent_complete.py --test compare
bash scripts/run_minedojo_x86.sh python tools/test_vpt_agent_complete.py --test hidden_state
bash scripts/run_minedojo_x86.sh python tools/test_vpt_agent_complete.py --test real_env
```

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```
src/training/agent/
â”œâ”€â”€ agent_base.py              # AgentæŠ½è±¡åŸºç±»
â”œâ”€â”€ vpt_agent.py              # ç®€åŒ–ç‰ˆï¼ˆä»…è§†è§‰ç‰¹å¾+å¯å‘å¼ï¼‰
â”œâ”€â”€ vpt_agent_complete.py     # âœ¨ å®Œæ•´ç‰ˆï¼ˆå®˜æ–¹æ ‡å‡†å®ç°ï¼‰
â””â”€â”€ __init__.py               # å¯¼å‡ºæ‰€æœ‰Agent

tools/
â”œâ”€â”€ test_vpt_agent.py         # æµ‹è¯•ç®€åŒ–ç‰ˆ
â””â”€â”€ test_vpt_agent_complete.py # âœ¨ æµ‹è¯•å®Œæ•´ç‰ˆ
```

---

## ğŸ”§ æŠ€æœ¯ç»†èŠ‚

### 1. VPTçš„å®Œæ•´Forwardæµç¨‹

```
è¾“å…¥: MineDojo obs (160x256 RGB)
  â†“
Resize: 128x128 (cv2.INTER_LINEAR)
  â†“
VPT Input: {"img": tensor of shape (1, 128, 128, 3)}
  â†“
policy.act(input, first, state_in, stochastic)
  â”œâ”€ img_preprocess (å½’ä¸€åŒ–)
  â”œâ”€ img_process (ImpalaCNN)
  â”œâ”€ recurrent_layer (Transformer) â† ä½¿ç”¨state_inï¼Œè¾“å‡ºstate_out
  â”œâ”€ lastlayer (å…¨è¿æ¥)
  â””â”€ pi_head.sample() (åŠ¨ä½œé‡‡æ ·)
  â†“
Agent Action: {"buttons": tensor, "camera": tensor}
  â†“
action_mapper.to_factored() (VPTå†…éƒ¨ â†’ MineRL factored)
  â†“
action_transformer.policy2env() (å¤„ç†ç›¸æœºé‡åŒ–)
  â†“
MineRL Action: {'forward': 0/1, 'jump': 0/1, 'camera': [pitch, yaw], ...}
  â†“
MineRLToMinedojoConverter.convert() (è‡ªå®šä¹‰è½¬æ¢å™¨)
  â†“
MineDojo Action: [dim0, dim1, ..., dim7]
```

### 2. Hidden Stateè¯¦è§£

**ç±»å‹**: `List[Tuple[Tensor, Tensor]]`ï¼ˆTransformerçš„keys/valuesï¼‰

**ç»´åº¦**: æ¯å±‚ç»´æŠ¤è‡ªå·±çš„key/valueï¼Œæ€»å…±4å±‚

**ç”Ÿå‘½å‘¨æœŸ**:
- `reset()`: åˆå§‹åŒ–ä¸ºå…¨0
- `predict()`: è¾“å…¥state_inï¼Œè¾“å‡ºstate_out
- Episodeç»“æŸ: ä¸‹æ¬¡`reset()`é‡æ–°åˆå§‹åŒ–

**ä½œç”¨**:
- è®°ä½è¿‡å»Nå¸§çš„è§†è§‰ä¿¡æ¯
- ç†è§£å½“å‰åœ¨æ‰§è¡Œä»€ä¹ˆä»»åŠ¡
- æä¾›æ—¶åºä¸Šä¸‹æ–‡

### 3. åŠ¨ä½œè½¬æ¢ç»†èŠ‚

#### action_mapperï¼ˆCameraHierarchicalMappingï¼‰

å°†VPTçš„å†…éƒ¨è¡¨ç¤ºè½¬æ¢ä¸ºMineRLçš„factored action spaceï¼š
- **è¾“å…¥**: `{"buttons": [b0, b1, ...], "camera": [c0, c1]}`
- **å¤„ç†**: å¤„ç†äº’æ–¥æŒ‰é’®ç»„ï¼ˆå¦‚forward/backï¼‰
- **è¾“å‡º**: `{"forward": 0/1, "back": 0/1, "jump": 0/1, ...}`

#### action_transformerï¼ˆActionTransformerï¼‰

å¤„ç†ç›¸æœºåŠ¨ä½œçš„é‡åŒ–/åé‡åŒ–ï¼š
- **ç›¸æœºé‡åŒ–**: è¿ç»­è§’åº¦ â†’ ç¦»æ•£bins
- **mu-lawç¼–ç **: æ›´å¥½çš„åˆ†è¾¨ç‡åˆ†é…
- **è¾“å‡º**: æœ€ç»ˆçš„MineRLåŠ¨ä½œ

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. å¿…é¡»åœ¨Episodeå¼€å§‹æ—¶è°ƒç”¨reset()

```python
# âœ“ æ­£ç¡®
agent.reset()  # åˆå§‹åŒ–hidden state
obs = env.reset()
for step in range(max_steps):
    action = agent.predict(obs)
    # ...

# âŒ é”™è¯¯
obs = env.reset()
for step in range(max_steps):
    action = agent.predict(obs)  # hidden stateä¼šç´¯ç§¯ä¸Šä¸ªepisodeçš„ä¿¡æ¯ï¼
    # ...
```

### 2. å›¾åƒæ ¼å¼è¦æ±‚

- MineDojo: ä»»æ„å°ºå¯¸ï¼ˆå¦‚160x256ï¼‰ï¼ŒHWCæˆ–CHWæ ¼å¼
- VPTå†…éƒ¨: è‡ªåŠ¨resizeåˆ°128x128ï¼Œè½¬æ¢ä¸ºHWC

### 3. åŠ¨ä½œç©ºé—´å…¼å®¹æ€§

- MineRLæœ‰ç‰©å“æ ã€GUIç­‰ä¿¡æ¯
- MineDojoåªæœ‰è§†è§‰ä¿¡æ¯
- VPTä½¿ç”¨`only_img_input=True`ï¼Œæ‰€ä»¥åªä¾èµ–å›¾åƒ

---

## ğŸ“ˆ é¢„æœŸæ”¹è¿›

### ç®€åŒ–ç‰ˆè¡¨ç°ï¼ˆå¯å‘å¼ï¼‰

- æˆåŠŸç‡: ~1-2%
- è¡Œä¸º: éšæœºç§»åŠ¨ã€è·³è·ƒ
- é—®é¢˜: å¡åœ¨æ–¹å—ä¸Šã€ä¸ä¼šä¸»åŠ¨å¯»æ‰¾æ ‘

### å®Œæ•´ç‰ˆé¢„æœŸè¡¨ç°

- æˆåŠŸç‡: 20-40%ï¼ˆé›¶æ ·æœ¬ï¼Œæ— fine-tuneï¼‰
- è¡Œä¸º: æ¢ç´¢ã€å¯»æ‰¾æ ‘ã€èµ°å‘æ ‘ã€æ”»å‡»
- ä¼˜åŠ¿: 
  - ä½¿ç”¨VPTçš„70Kå°æ—¶é¢„è®­ç»ƒçŸ¥è¯†
  - æ™ºèƒ½å†³ç­–è€Œä¸æ˜¯éšæœº
  - æŒç»­è¡Œä¸ºä¸ä¼šä¸­æ–­

---

## ğŸ”œ ä¸‹ä¸€æ­¥

### 1. é›¶æ ·æœ¬è¯„ä¼°

```bash
# è¯„ä¼°å®Œæ•´ç‰ˆVPTï¼ˆæ— fine-tuneï¼‰
bash scripts/run_minedojo_x86.sh python src/training/vpt/evaluate_vpt_complete_zero_shot.py \
  --vpt-weights data/pretrained/vpt/rl-from-early-game-2x.weights \
  --num-episodes 50 \
  --device cpu
```

### 2. Fine-tuneè®­ç»ƒ

ä½¿ç”¨å®Œæ•´ç‰ˆAgentæ›¿æ¢ä¹‹å‰çš„ç®€åŒ–ç‰ˆï¼Œè¿›è¡ŒBC fine-tuneï¼š
- å†»ç»“VPTå‚æ•°
- åªè®­ç»ƒæœ€åå‡ å±‚
- ä¿ç•™VPTçš„é¢„è®­ç»ƒçŸ¥è¯†

### 3. DAggerè¿­ä»£

ä½¿ç”¨å®Œæ•´ç‰ˆAgentè¿›è¡ŒDAggerè¿­ä»£è®­ç»ƒï¼š
- æ›´æ™ºèƒ½çš„rollout
- æ›´å°‘éœ€è¦äººå·¥æ ‡æ³¨
- æ›´å¿«æ”¶æ•›

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. **OpenAI VPTå®˜æ–¹å®ç°**:
   - Repository: https://github.com/openai/Video-Pre-Training
   - agent.py: https://github.com/openai/Video-Pre-Training/blob/main/agent.py

2. **VPTè®ºæ–‡**:
   - Title: "Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos"
   - Link: https://arxiv.org/abs/2206.11795

3. **æœ¬é¡¹ç›®ç›¸å…³æ–‡æ¡£**:
   - `docs/reference/VPT_MODELS_REFERENCE.md`: VPTæ¨¡å‹é€‰æ‹©æŒ‡å—
   - `docs/summaries/VPT_INTEGRATION_SUMMARY.md`: VPTé›†æˆæ€»ç»“
   - `docs/issues/VPT_LOW_SUCCESS_RATE_DIAGNOSIS.md`: ä½æˆåŠŸç‡è¯Šæ–­

---

## âœ… æ€»ç»“

### æ ¸å¿ƒæ”¹è¿›

1. âœ… **Hidden Stateç»´æŠ¤** â†’ æœ‰è®°å¿†ã€è¡Œä¸ºè¿è´¯
2. âœ… **ä½¿ç”¨pi_head** â†’ æ™ºèƒ½å†³ç­–ã€é¢„è®­ç»ƒçŸ¥è¯†
3. âœ… **å®˜æ–¹å®ç°æ ‡å‡†** â†’ æ­£ç¡®ã€å¯é 

### å…³é”®ä»£ç 

```python
# å®Œæ•´ç‰ˆæ ¸å¿ƒpredictæµç¨‹
def predict(self, obs, deterministic=True):
    # 1. MineDojo obs â†’ VPT input
    agent_input = self._minedojo_obs_to_agent_input(obs)
    
    # 2. VPTå®Œæ•´forwardï¼ˆåŒ…æ‹¬hidden stateå’Œpi_headï¼‰
    agent_action, self.hidden_state, _ = self.policy.act(
        agent_input, self._dummy_first, self.hidden_state,
        stochastic=not deterministic
    )
    
    # 3. VPT output â†’ MineRL action
    minerl_action = self._agent_action_to_minerl(agent_action)
    
    # 4. MineRL action â†’ MineDojo action
    minedojo_action = self.minerl_to_minedojo.convert(minerl_action)
    
    return minedojo_action
```

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. è¿è¡Œæµ‹è¯•éªŒè¯å®ç°æ­£ç¡®æ€§
2. é›¶æ ·æœ¬è¯„ä¼°å®Œæ•´ç‰ˆæ€§èƒ½
3. ä¸ç®€åŒ–ç‰ˆå¯¹æ¯”æˆåŠŸç‡
4. ç”¨äºåç»­BC/DAggerè®­ç»ƒ

å®Œæ•´ç‰ˆVPT Agentå®ç°å®Œæˆï¼ğŸ‰
