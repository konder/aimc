# MineCLIPå¥–åŠ±è®¾è®¡è¯¦è§£

> **æ ¸å¿ƒé—®é¢˜**: ä¸ºä»€ä¹ˆMineCLIPå¥–åŠ±æ˜¯"å½“å‰ç›¸ä¼¼åº¦ - å‰ä¸€æ­¥ç›¸ä¼¼åº¦"ï¼Ÿ

---

## ğŸ¯ **æ ¸å¿ƒä»£ç **

```python
# src/utils/mineclip_reward.py, line 487-492

# MineCLIP å¯†é›†å¥–åŠ± = ç›¸ä¼¼åº¦è¿›æ­¥é‡
if should_compute:
    mineclip_reward = current_similarity - self.previous_similarity
    self.previous_similarity = current_similarity
else:
    mineclip_reward = 0.0
```

**å…³é”®**: å¥–åŠ± = **å½“å‰ç›¸ä¼¼åº¦ - å‰ä¸€æ­¥ç›¸ä¼¼åº¦** = **ç›¸ä¼¼åº¦çš„å˜åŒ–é‡ï¼ˆè¿›æ­¥é‡ï¼‰**

---

## ğŸ¤” **ä¸ºä»€ä¹ˆè¦å–å·®å€¼ï¼Ÿ**

### **åŸå› 1: å¥–åŠ±åº”è¯¥åæ˜ "è¿›æ­¥"ï¼Œè€Œé"çŠ¶æ€"** â­â­â­

**å¦‚æœç›´æ¥ç”¨ç›¸ä¼¼åº¦ä½œä¸ºå¥–åŠ±**ï¼ˆé”™è¯¯è®¾è®¡ï¼‰:
```python
# âŒ é”™è¯¯è®¾è®¡
mineclip_reward = current_similarity  # 0.27
```

**é—®é¢˜**:
- æ™ºèƒ½ä½“åœ¨åŸåœ°ä¸åŠ¨ï¼Œç›¸ä¼¼åº¦ä¹Ÿæ˜¯0.27 â†’ æŒç»­è·å¾—å¥–åŠ±
- æ— æ³•åŒºåˆ†"è¿›æ­¥"å’Œ"åœæ»"
- å¯¼è‡´æ™ºèƒ½ä½“å­¦ä¼š"æ‰¾åˆ°æ ‘ååœæ­¢"è€Œä¸æ˜¯"ç æ ‘"

**ç¤ºä¾‹å¯¹æ¯”**:

| æ—¶åˆ» | è¡Œä¸º | ç›¸ä¼¼åº¦ | ç›´æ¥ç”¨ç›¸ä¼¼åº¦(âŒ) | ç”¨å·®å€¼(âœ…) |
|------|------|--------|----------------|-----------|
| t=0 | éšæœºä½ç½® | 0.25 | reward=0.25 | - |
| t=1 | é è¿‘æ ‘ | 0.27 | reward=0.27 | reward=+0.02 â­ |
| t=2 | ç»§ç»­é è¿‘ | 0.28 | reward=0.28 | reward=+0.01 â­ |
| t=3 | åŸåœ°ä¸åŠ¨ | 0.28 | reward=0.28 âŒ | reward=0.00 âœ… |
| t=4 | ç æ ‘ä¸­ | 0.29 | reward=0.29 | reward=+0.01 â­ |

**ç»“è®º**: 
- âœ… **å·®å€¼è®¾è®¡**ï¼šåªå¥–åŠ±"è¿›æ­¥"
- âŒ **ç›´æ¥ç”¨ç›¸ä¼¼åº¦**ï¼šä¼šå¥–åŠ±"åœæ»"

---

### **åŸå› 2: ç¬¦åˆå¼ºåŒ–å­¦ä¹ çš„"å¥–åŠ±å¡‘å½¢"åŸåˆ™** â­â­â­

**å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒ**:
```
Agentçš„ç›®æ ‡ = æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±
âˆ‘(rewards) = reward_t1 + reward_t2 + ... + reward_tn
```

**å¦‚æœç”¨å·®å€¼**:
```python
reward_t1 = sim_1 - sim_0
reward_t2 = sim_2 - sim_1
reward_t3 = sim_3 - sim_2
...
âˆ‘(rewards) = (sim_1 - sim_0) + (sim_2 - sim_1) + (sim_3 - sim_2)
           = sim_3 - sim_0  # ä¼¸ç¼©æ±‚å’Œï¼
           = æœ€ç»ˆç›¸ä¼¼åº¦ - åˆå§‹ç›¸ä¼¼åº¦
```

**æƒŠäººçš„å‘ç°**: ç´¯ç§¯çš„å·®å€¼å¥–åŠ± = æœ€ç»ˆè¿›æ­¥é‡ï¼

**å¦‚æœç›´æ¥ç”¨ç›¸ä¼¼åº¦**:
```python
reward_t1 = sim_1
reward_t2 = sim_2
reward_t3 = sim_3
...
âˆ‘(rewards) = sim_1 + sim_2 + sim_3
```

**é—®é¢˜**: ç´¯ç§¯å¥–åŠ±ä¸"æ€»è¿›æ­¥é‡"æ— å…³ï¼Œè€Œæ˜¯ä¸"åœç•™æ—¶é—´"ç›¸å…³ï¼

---

### **åŸå› 3: é¿å…"é«˜åŸé—®é¢˜"** â­â­

**åœºæ™¯**: æ™ºèƒ½ä½“é è¿‘æ ‘åï¼Œç›¸ä¼¼åº¦è¾¾åˆ°0.70

**å¦‚æœç”¨å·®å€¼**:
```python
t=100: é è¿‘æ ‘ï¼Œsim=0.70 â†’ reward = +0.15ï¼ˆå¤§å¥–åŠ±ï¼‰
t=101: åœ¨æ ‘æ—ï¼Œsim=0.70 â†’ reward = 0.00ï¼ˆæ— å¥–åŠ±ï¼‰
t=102: åœ¨æ ‘æ—ï¼Œsim=0.70 â†’ reward = 0.00ï¼ˆæ— å¥–åŠ±ï¼‰
t=103: ç æ ‘ä¸­ï¼Œsim=0.72 â†’ reward = +0.02ï¼ˆç»§ç»­è¿›æ­¥ï¼‰
```
âœ… é¼“åŠ±ç»§ç»­è¿›æ­¥

**å¦‚æœç›´æ¥ç”¨ç›¸ä¼¼åº¦**:
```python
t=100: é è¿‘æ ‘ï¼Œsim=0.70 â†’ reward = 0.70
t=101: åœ¨æ ‘æ—ï¼Œsim=0.70 â†’ reward = 0.70ï¼ˆæŒç»­é«˜å¥–åŠ±ï¼‰
t=102: åœ¨æ ‘æ—ï¼Œsim=0.70 â†’ reward = 0.70ï¼ˆæŒç»­é«˜å¥–åŠ±ï¼‰
t=103: ç æ ‘ä¸­ï¼Œsim=0.72 â†’ reward = 0.72ï¼ˆç•¥é«˜ï¼‰
```
âŒ æ™ºèƒ½ä½“å¯èƒ½å­¦ä¼š"ç«™åœ¨æ ‘æ—ä¸åŠ¨"ï¼ˆå› ä¸ºå·²ç»è·å¾—é«˜å¥–åŠ±ï¼‰

---

### **åŸå› 4: å¤„ç†è´Ÿåé¦ˆ** â­

**å·®å€¼è®¾è®¡å¤©ç„¶æ”¯æŒæƒ©ç½š**:

| è¡Œä¸º | ç›¸ä¼¼åº¦å˜åŒ– | å·®å€¼å¥–åŠ± | å«ä¹‰ |
|------|-----------|---------|------|
| é è¿‘æ ‘ | 0.25 â†’ 0.30 | **+0.05** | æ­£å‘å¥–åŠ± â­ |
| ç æ ‘ä¸­ | 0.30 â†’ 0.32 | **+0.02** | æ­£å‘å¥–åŠ± â­ |
| èµ°è¿œäº† | 0.32 â†’ 0.28 | **-0.04** | è´Ÿå‘æƒ©ç½š âš ï¸ |
| è½¬å‘å¤©ç©º | 0.28 â†’ 0.25 | **-0.03** | è´Ÿå‘æƒ©ç½š âš ï¸ |

**æ™ºèƒ½ä½“å­¦åˆ°çš„ç­–ç•¥**:
- âœ… åšè®©ç›¸ä¼¼åº¦ä¸Šå‡çš„äº‹ï¼ˆé è¿‘æ ‘ã€ç æ ‘ï¼‰
- âŒ é¿å…è®©ç›¸ä¼¼åº¦ä¸‹é™çš„äº‹ï¼ˆèµ°è¿œã€è½¬å‘åˆ«å¤„ï¼‰

---

## ğŸ“Š **å®é™…æ•ˆæœå¯¹æ¯”**

### **å®éªŒè®¾å®š**: ç æ ‘ä»»åŠ¡ï¼Œ200æ­¥

**æ–¹æ³•A: å·®å€¼å¥–åŠ±**ï¼ˆå½“å‰è®¾è®¡ï¼‰âœ…
```python
mineclip_reward = current_sim - previous_sim
```

**æ–¹æ³•B: ç›´æ¥ç›¸ä¼¼åº¦**ï¼ˆé”™è¯¯è®¾è®¡ï¼‰âŒ
```python
mineclip_reward = current_sim
```

### **é¢„æœŸç»“æœ**:

| æŒ‡æ ‡ | å·®å€¼å¥–åŠ± | ç›´æ¥ç›¸ä¼¼åº¦ | è¯´æ˜ |
|------|---------|-----------|------|
| é¼“åŠ±æ¢ç´¢ | âœ… é«˜ | âŒ ä½ | å·®å€¼é¼“åŠ±æŒç»­è¿›æ­¥ |
| é¿å…åœæ» | âœ… å¼º | âŒ å¼± | åœæ»æ—¶æ— å¥–åŠ± vs æŒç»­é«˜å¥–åŠ± |
| ä»»åŠ¡å®Œæˆç‡ | âœ… é«˜ | âŒ ä½ | å·®å€¼å¼•å¯¼åˆ°ç›®æ ‡ |
| è®­ç»ƒç¨³å®šæ€§ | âœ… å¥½ | âŒ å·® | å¥–åŠ±æ–¹å·®æ›´å° |

---

## ğŸ”¬ **æ•°å­¦åŸç†ï¼šPotential-Based Reward Shaping**

è¿™ç§è®¾è®¡å®é™…ä¸Šæ˜¯ä¸€ç§**åŸºäºåŠ¿å‡½æ•°çš„å¥–åŠ±å¡‘å½¢**ï¼ˆPotential-Based Reward Shapingï¼‰

### **ç†è®ºåŸºç¡€**

**åŠ¿å‡½æ•°ï¼ˆPotential Functionï¼‰**:
```
Î¦(s) = similarity(s, task)
```

**å¡‘å½¢å¥–åŠ±ï¼ˆShaped Rewardï¼‰**:
```
r'(s, a, s') = r(s, a, s') + Î³Â·Î¦(s') - Î¦(s)
```

å…¶ä¸­:
- `r(s, a, s')`: åŸå§‹ç¨€ç–å¥–åŠ±
- `Î¦(s')`: ä¸‹ä¸€çŠ¶æ€çš„åŠ¿èƒ½
- `Î¦(s)`: å½“å‰çŠ¶æ€çš„åŠ¿èƒ½
- `Î³`: æŠ˜æ‰£å› å­ï¼ˆé€šå¸¸=1ï¼‰

**åœ¨MineCLIPä¸­**:
```python
shaped_reward = sparse_reward + mineclip_weight * (current_sim - previous_sim)
                                                    â†‘
                                            åŠ¿å‡½æ•°çš„å·®å€¼ï¼
```

### **é‡è¦æ€§è´¨**

**å®šç†ï¼ˆNg et al., 1999ï¼‰**: åŸºäºåŠ¿å‡½æ•°çš„å¥–åŠ±å¡‘å½¢**ä¸æ”¹å˜æœ€ä¼˜ç­–ç•¥**

**è¯æ˜ç›´è§‰**:
```
ç´¯ç§¯å¡‘å½¢å¥–åŠ± = âˆ‘[r'(st, at, st+1)]
             = âˆ‘[r(st, at, st+1)] + âˆ‘[Î¦(st+1) - Î¦(st)]
             = âˆ‘[r(st, at, st+1)] + [Î¦(sT) - Î¦(s0)]
                                      â†‘
                                  å¸¸æ•°ï¼ˆä¸ç­–ç•¥æ— å…³ï¼‰
```

**ç»“è®º**: 
- âœ… æœ€ä¼˜ç­–ç•¥ä¸å˜
- âœ… ä½†å­¦ä¹ é€Ÿåº¦æ›´å¿«ï¼ˆå› ä¸ºå¯†é›†å¥–åŠ±æŒ‡å¼•ï¼‰

**å‚è€ƒæ–‡çŒ®**: 
- Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under reward transformations: Theory and application to reward shaping.

---

## âš ï¸ **å¸¸è§è¯¯åŒº**

### **è¯¯åŒº1: "ç›¸ä¼¼åº¦é«˜å°±åº”è¯¥å¥–åŠ±é«˜"**

**é”™è¯¯ç†è§£**:
```python
# âŒ ç›¸ä¼¼åº¦0.8åº”è¯¥ç»™0.8çš„å¥–åŠ±
reward = similarity  # 0.8
```

**æ­£ç¡®ç†è§£**:
```python
# âœ… ä»0.7è¿›æ­¥åˆ°0.8åº”è¯¥å¥–åŠ±
reward = similarity_now - similarity_before  # 0.8 - 0.7 = 0.1
```

**ç±»æ¯”**: 
- è€ƒè¯•è€ƒ90åˆ† â‰  åº”è¯¥å¥–åŠ±90å…ƒ
- ä»70åˆ†è¿›æ­¥åˆ°90åˆ† = åº”è¯¥å¥–åŠ±è¿›æ­¥çš„20åˆ†ï¼

---

### **è¯¯åŒº2: "å·®å€¼å¤ªå°ï¼Œä¸å¦‚ç›´æ¥ç”¨ç›¸ä¼¼åº¦"**

**å½“å‰é—®é¢˜**: MineCLIPç›¸ä¼¼åº¦å˜åŒ–åªæœ‰0.02ï¼ˆ2%ï¼‰

**é”™è¯¯æƒ³æ³•**:
```python
# âŒ å·®å€¼å¤ªå°ï¼ˆ0.02ï¼‰ï¼Œä¸å¦‚ç›´æ¥ç”¨ç›¸ä¼¼åº¦ï¼ˆ0.70ï¼‰
reward = current_similarity  # 0.70ï¼Œçœ‹èµ·æ¥æ›´å¤§
```

**æ­£ç¡®åšæ³•**:
```python
# âœ… å·®å€¼è™½å°ï¼Œä½†å¯ä»¥é€šè¿‡æƒé‡æ”¾å¤§
reward = (current_similarity - previous_similarity) * 40.0
       = 0.02 * 40.0 = 0.8
```

**å…³é”®**: 
- ä¿¡å·å¤ªå¼± â†’ è°ƒæ•´æƒé‡
- ä¸è¦æ”¹å˜å¥–åŠ±è®¾è®¡åŸç†

---

### **è¯¯åŒº3: "ç´¯ç§¯å·®å€¼ä¼šæŠµæ¶ˆ"**

**æ‹…å¿ƒ**:
```
å¦‚æœå…ˆè¿›æ­¥+0.05ï¼Œå†é€€æ­¥-0.05ï¼Œæ˜¯ä¸æ˜¯æŠµæ¶ˆäº†ï¼Ÿ
```

**è§£ç­”**: è¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼
```python
t=1: é è¿‘æ ‘ â†’ reward = +0.05  âœ… å¥½è¡Œä¸ºï¼Œå¥–åŠ±
t=2: èµ°è¿œäº† â†’ reward = -0.05  âŒ åè¡Œä¸ºï¼Œæƒ©ç½š
ç´¯ç§¯: +0.05 - 0.05 = 0        âœ… æœ€ç»ˆæ²¡è¿›æ­¥ï¼Œæ€»å¥–åŠ±ä¸º0
```

**è¿™æ˜¯æ­£ç¡®çš„**ï¼RLä¼šå­¦ä¹ é¿å…"å…ˆè¿›åé€€"çš„è¡Œä¸ºã€‚

---

## ğŸ¯ **å®é™…åº”ç”¨ç¤ºä¾‹**

### **ç æ ‘ä»»åŠ¡çš„å®Œæ•´å¥–åŠ±æµç¨‹**:

```python
# Episodeå¼€å§‹
t=0:   åœ¨å¹³åŸä¸Šï¼Œçœ‹å¤©ç©º
       sim=0.25, previous_sim=0.25
       reward = 0.25 - 0.25 = 0.00

t=50:  ç§»åŠ¨ä¸­ï¼Œçœ‹åˆ°è¿œå¤„çš„æ ‘
       sim=0.26, previous_sim=0.25
       reward = 0.26 - 0.25 = +0.01 â­

t=100: é è¿‘æ ‘æœ¨
       sim=0.30, previous_sim=0.26
       reward = 0.30 - 0.26 = +0.04 â­â­

t=150: å¯¹å‡†æ ‘å¹²
       sim=0.32, previous_sim=0.30
       reward = 0.32 - 0.30 = +0.02 â­

t=200: ç æ ‘ä¸­ï¼ˆæ‰‹è‡‚æŒ¥åŠ¨ï¼‰
       sim=0.33, previous_sim=0.32
       reward = 0.33 - 0.32 = +0.01 â­

t=250: æ ‘æœ¨ç ´åï¼Œè·å¾—æœ¨å¤´
       sim=0.31, previous_sim=0.33
       reward = 0.31 - 0.33 = -0.02 âš ï¸
       sparse_reward = +1.0 ğŸ‰
       total_reward = -0.02*40 + 1.0*1.0 = +0.2 âœ…

ç´¯ç§¯MineCLIPå¥–åŠ± = 0.31 - 0.25 = 0.06
æ”¾å¤§å = 0.06 * 40 = 2.4
```

**è§‚å¯Ÿ**:
1. æ¯æ¬¡é è¿‘æ ‘æœ¨éƒ½è·å¾—æ­£å¥–åŠ±
2. ç æ ‘è¿‡ç¨‹æŒç»­è·å¾—å°å¥–åŠ±
3. è·å¾—æœ¨å¤´åç›¸ä¼¼åº¦ä¸‹é™ï¼ˆæ ‘æ¶ˆå¤±äº†ï¼‰ï¼Œä½†ç¨€ç–å¥–åŠ±å¾ˆå¤§
4. æ€»ä½“å¼•å¯¼æ™ºèƒ½ä½“å®Œæˆä»»åŠ¡

---

## ğŸ”§ **å˜ä½“è®¾è®¡**

è™½ç„¶å·®å€¼æ˜¯æ ‡å‡†è®¾è®¡ï¼Œä½†ä¹Ÿæœ‰å…¶ä»–å˜ä½“ï¼š

### **å˜ä½“1: å½’ä¸€åŒ–å·®å€¼**

```python
# è€ƒè™‘åˆ°ç›¸ä¼¼åº¦èŒƒå›´[0,1]ï¼Œå½’ä¸€åŒ–å·®å€¼
mineclip_reward = (current_sim - previous_sim) / max(previous_sim, 0.01)
```

**ä¼˜ç‚¹**: æ—©æœŸè¿›æ­¥ï¼ˆ0.1â†’0.2ï¼‰æ¯”åæœŸè¿›æ­¥ï¼ˆ0.8â†’0.9ï¼‰å¥–åŠ±æ›´å¤§  
**ç¼ºç‚¹**: å¯èƒ½å¯¼è‡´åæœŸå¥–åŠ±å¤ªå¼±

---

### **å˜ä½“2: ç§»åŠ¨å¹³å‡**

```python
# ç”¨ç§»åŠ¨å¹³å‡å‡å°‘å™ªå£°
avg_sim = 0.9 * avg_sim + 0.1 * current_sim
mineclip_reward = avg_sim - previous_avg_sim
```

**ä¼˜ç‚¹**: æ›´å¹³æ»‘ï¼Œå‡å°‘å•å¸§å™ªå£°  
**ç¼ºç‚¹**: å»¶è¿Ÿåé¦ˆ

---

### **å˜ä½“3: é˜ˆå€¼å·®å€¼**

```python
# åªæœ‰æ˜¾è‘—è¿›æ­¥æ‰å¥–åŠ±
diff = current_sim - previous_sim
mineclip_reward = diff if abs(diff) > 0.01 else 0.0
```

**ä¼˜ç‚¹**: è¿‡æ»¤å¾®å°æ³¢åŠ¨  
**ç¼ºç‚¹**: å¯èƒ½ä¸¢å¤±æœ‰ç”¨ä¿¡å·

---

## ğŸ“š **ç›¸å…³ç†è®ºå’Œå‚è€ƒ**

### **æ ¸å¿ƒç†è®º**:

1. **Reward Shaping** (Ng et al., 1999)
   - Policy invariance under reward transformations
   - Potential-based shaping ä¿è¯æœ€ä¼˜ç­–ç•¥ä¸å˜

2. **Dense vs Sparse Rewards** (Sutton & Barto, 2018)
   - Sparse: åªåœ¨ä»»åŠ¡å®Œæˆæ—¶ç»™å¥–åŠ±
   - Dense: æ¯æ­¥éƒ½ç»™ä¸­é—´å¥–åŠ±å¼•å¯¼

3. **Curriculum Learning** (Bengio et al., 2009)
   - ä»ç®€å•åˆ°å¤æ‚é€æ­¥å­¦ä¹ 
   - MineCLIPæƒé‡è¡°å‡å°±æ˜¯ä¸€ç§è¯¾ç¨‹å­¦ä¹ 

### **åœ¨MineCLIPä¸­çš„åº”ç”¨**:

```python
# å®Œæ•´çš„å¥–åŠ±å…¬å¼
total_reward = (
    sparse_reward * sparse_weight +           # ä»»åŠ¡å®Œæˆå¥–åŠ±
    (current_sim - previous_sim) * mineclip_weight  # è¿›æ­¥å¥–åŠ±
)

# åŠ¨æ€è°ƒæ•´MineCLIPæƒé‡
mineclip_weight = initial_weight * (1 - step_count / decay_steps)
```

---

## ğŸ’¡ **æ€»ç»“**

### **ä¸ºä»€ä¹ˆç”¨å·®å€¼ï¼Ÿ**

| åŸå›  | é‡è¦æ€§ | è¯´æ˜ |
|------|--------|------|
| 1. å¥–åŠ±è¿›æ­¥è€ŒéçŠ¶æ€ | â­â­â­ | é˜²æ­¢"åŸåœ°ä¸åŠ¨"è·å¾—é«˜å¥–åŠ± |
| 2. ç¬¦åˆRLç†è®º | â­â­â­ | Potential-based shaping |
| 3. ç´¯ç§¯ç­‰äºæ€»è¿›æ­¥ | â­â­ | æ•°å­¦ä¸Šä¼˜é›… |
| 4. è‡ªç„¶æ”¯æŒè´Ÿåé¦ˆ | â­â­ | èµ°è¿œä¼šè¢«æƒ©ç½š |
| 5. é¿å…é«˜åŸé—®é¢˜ | â­ | é¼“åŠ±æŒç»­è¿›æ­¥ |

### **æ ¸å¿ƒå…¬å¼**:

```python
mineclip_reward = (current_similarity - previous_similarity) * weight
```

è¿™ä¸ªç®€å•çš„è®¾è®¡è•´å«äº†æ·±åˆ»çš„å¼ºåŒ–å­¦ä¹ åŸç†ï¼

---

### **ç±»æ¯”ç†è§£**:

**å­¦ä¹ æˆç»©**:
- âŒ é”™è¯¯: è€ƒ90åˆ† â†’ å¥–åŠ±90å…ƒ
- âœ… æ­£ç¡®: ä»70è¿›æ­¥åˆ°90 â†’ å¥–åŠ±è¿›æ­¥çš„20åˆ†

**MineCLIP**:
- âŒ é”™è¯¯: ç›¸ä¼¼åº¦0.70 â†’ å¥–åŠ±0.70
- âœ… æ­£ç¡®: ä»0.60è¿›æ­¥åˆ°0.70 â†’ å¥–åŠ±è¿›æ­¥çš„0.10

---

**å‚è€ƒæ–‡çŒ®**:
- Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under reward transformations: Theory and application to reward shaping. ICML.
- Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.
- Bengio, Y., Louradour, J., Collobert, R., & Weston, J. (2009). Curriculum learning. ICML.

---

**ç›¸å…³æ–‡æ¡£**:
- [`MINECLIP_EXPLAINED.md`](../../guides/MINECLIP_EXPLAINED.md) - MineCLIPåŸºç¡€æ¦‚å¿µ
- [`MINECLIP_REWARD_EXPLAINED.md`](../../guides/MINECLIP_REWARD_EXPLAINED.md) - å¥–åŠ±æœºåˆ¶è¯¦è§£
- [`MINECLIP_CURRICULUM_LEARNING.md`](../../guides/MINECLIP_CURRICULUM_LEARNING.md) - è¯¾ç¨‹å­¦ä¹ ç­–ç•¥

