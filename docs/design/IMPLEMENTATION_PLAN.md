# MineCLIP + å¯†é›†å¥–åŠ±å®æ–½è®¡åˆ’

> **ç›®æ ‡**: æ”¹å–„harvest woodè®­ç»ƒæ•ˆæœ  
> **ç­–ç•¥**: åˆ†é˜¶æ®µä¼˜åŒ–ï¼Œä»ç®€å•åˆ°å¤æ‚

---

## ğŸ“Š **é—®é¢˜æ€»ç»“**

1. âœ… **MineCLIPæ­£ç¡®å®ç°å·²éªŒè¯**ï¼šå½’ä¸€åŒ–å‚æ•°ã€temporal encoder
2. âŒ **å•å¸§MineCLIPåŒºåˆ†åº¦å¤ªä½**ï¼š0.007ï¼Œæ— æ³•æœ‰æ•ˆå¼•å¯¼
3. âŒ **çº¯ç¨€ç–å¥–åŠ±å¤±è´¥**ï¼š50ä¸‡æ­¥è®­ç»ƒå´©æºƒ
4. âœ… **éœ€è¦å¯†é›†å¥–åŠ±**ï¼šç”¨æˆ·ç¡®è®¤

---

## ğŸ¯ **ä¸‰é˜¶æ®µå®æ–½æ–¹æ¡ˆ**

### **é˜¶æ®µ1ï¼šç®€å•åº“å­˜å¥–åŠ±ï¼ˆ1å°æ—¶ï¼‰** â­â­â­â­â­

**ç›®æ ‡**: å¿«é€ŸéªŒè¯å¯†é›†å¥–åŠ±æ˜¯å¦æœ‰æ•ˆ

**å®ç°**:
```python
# æœ€ç®€å•çš„å¥–åŠ±ï¼šåº“å­˜ä¸­å‡ºç°æœ¨å¤´ = +1.0
class InventoryBasedReward(gym.Wrapper):
    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        inventory = info.get('inventory', {})
        if inventory.get('log', 0) > self.last_count:
            reward += 1.0  # è·å¾—æœ¨å¤´ç«‹å³å¥–åŠ±
        return obs, reward, done, info
```

**ä¼˜ç‚¹**:
- âœ… å®ç°ç®€å•ï¼ˆå·²å®Œæˆï¼‰
- âœ… ä¿¡å·æ˜ç¡®
- âœ… ä¸ä¾èµ–MineCLIP

**æµ‹è¯•æ–¹æ³•**:
```bash
# 10000æ­¥å¿«é€Ÿæµ‹è¯•
./scripts/train_get_wood.sh test \
    --timesteps 10000 \
    --learning-rate 0.0001 \
    --device cpu \
    --headless
# ä½¿ç”¨ç®€å•åº“å­˜å¥–åŠ±ï¼ˆåœ¨train_get_wood.pyä¸­é›†æˆï¼‰
```

**é¢„æœŸæ•ˆæœ**:
- explained_variance > 0
- è®­ç»ƒç¨³å®š
- å¯èƒ½åœ¨5000-10000æ­¥é¦–æ¬¡è·å¾—æœ¨å¤´

---

### **é˜¶æ®µ2ï¼šæ··åˆå¥–åŠ±ï¼ˆ2å°æ—¶ï¼‰** â­â­â­â­

**ç›®æ ‡**: ç»“åˆåº“å­˜å¥–åŠ± + åŠ¨ä½œé¼“åŠ±

**å®ç°**:
```python
class SimpleDenseRewardWrapper(gym.Wrapper):
    def step(self, action):
        dense_reward = 0.0
        
        # 1. åº“å­˜å¥–åŠ±ï¼ˆä¸»è¦ï¼‰
        if got_log:
            dense_reward += 10.0
        
        # 2. æ”»å‡»åŠ¨ä½œé¼“åŠ±ï¼ˆæ¬¡è¦ï¼‰
        if action[5] == 1:  # æ”»å‡»
            dense_reward += 0.01
        
        # 3. ç§»åŠ¨æƒ©ç½šï¼ˆå¯é€‰ï¼‰
        if action[0] != 0:
            dense_reward -= 0.001
        
        return obs, reward + dense_reward, done, info
```

**ä¼˜ç‚¹**:
- âœ… å¤šä¿¡å·å¼•å¯¼
- âœ… é¼“åŠ±æ¢ç´¢æ”»å‡»åŠ¨ä½œ
- âœ… å‡å°‘æ— æ„ä¹‰ç§»åŠ¨

**æƒé‡è°ƒæ•´**:
- åº“å­˜å¥–åŠ±: 1.0 - 10.0ï¼ˆæ ¹æ®é˜¶æ®µ1ç»“æœè°ƒæ•´ï¼‰
- æ”»å‡»å¥–åŠ±: 0.001 - 0.1
- ç§»åŠ¨æƒ©ç½š: -0.01 - 0.0

---

### **é˜¶æ®µ3ï¼š16å¸§MineCLIPï¼ˆ1å¤©ï¼‰** â­â­â­

**ç›®æ ‡**: ä½¿ç”¨å®Œæ•´MineCLIPï¼ˆå¦‚å»ºè®®æ‰€ç¤ºï¼‰

**éœ€è¦å®ç°**:

#### **3.1 å¸§ç¼“å­˜æœºåˆ¶**

```python
class FrameBuffer:
    def __init__(self, max_frames=16):
        self.max_frames = max_frames
        self.frames = []
    
    def add(self, frame):
        self.frames.append(frame)
        if len(self.frames) > self.max_frames:
            self.frames.pop(0)
    
    def get_video(self):
        """è¿”å› [1, T, 3, H, W] æ ¼å¼"""
        if len(self.frames) < self.max_frames:
            return None  # å¸§æ•°ä¸è¶³
        return np.stack(self.frames)[np.newaxis, ...]
```

#### **3.2 MineCLIPå¥–åŠ±è®¡ç®—**

```python
class MineCLIPRewardWrapper(gym.Wrapper):
    def __init__(self, env, mineclip, task_prompt, frame_stack=16):
        self.frame_buffer = FrameBuffer(frame_stack)
        self.mineclip = mineclip
        self.task_prompt = task_prompt
    
    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        
        # æ·»åŠ å¸§åˆ°ç¼“å­˜
        self.frame_buffer.add(obs)  # å‡è®¾obsæ˜¯å›¾åƒ
        
        # æ¯Næ­¥è®¡ç®—ä¸€æ¬¡MineCLIPå¥–åŠ±
        if self.step_count % 16 == 0:
            video = self.frame_buffer.get_video()
            if video is not None:
                # ä½¿ç”¨å®˜æ–¹å®Œæ•´æµç¨‹
                video_emb = self.mineclip.encode_video(video)
                text_emb = self.mineclip.encode_text(self.task_prompt)
                similarity = cosine_similarity(video_emb, text_emb)
                
                # MineCLIPå¥–åŠ±
                mineclip_reward = similarity * 2.0
                reward += mineclip_reward
        
        return obs, reward, done, info
```

#### **3.3 é¢„å¤„ç†å‡½æ•°**

```python
def preprocess_frames(frames):
    """
    Args:
        frames: [T, H, W, 3] numpy array, uint8, [0, 255]
    Returns:
        video: [1, T, 3, H, W] tensor, float32, normalized
    """
    processed = []
    for frame in frames:
        # 1. è°ƒæ•´å¤§å°
        frame = cv2.resize(frame, (256, 160))  # MineDojoæ ‡å‡†
        
        # 2. å½’ä¸€åŒ–åˆ°[0,1]
        frame = frame.astype(np.float32) / 255.0
        
        # 3. HWC -> CHW
        frame = frame.transpose(2, 0, 1)
        
        # 4. MineCraftå½’ä¸€åŒ–
        mean = np.array([0.3331, 0.3245, 0.3051]).reshape(3, 1, 1)
        std = np.array([0.2439, 0.2493, 0.2873]).reshape(3, 1, 1)
        frame = (frame - mean) / std
        
        processed.append(frame)
    
    # å †å å¹¶æ·»åŠ batchç»´åº¦
    video = np.stack(processed)[np.newaxis, ...]  # [1, T, 3, H, W]
    return torch.from_numpy(video).float()
```

**æŒ‘æˆ˜**:
- âš ï¸ æ€§èƒ½å¼€é”€ï¼ˆæ¯16æ­¥è®¡ç®—ä¸€æ¬¡MineCLIPï¼‰
- âš ï¸ å†…å­˜å ç”¨ï¼ˆç¼“å­˜16å¸§ï¼‰
- âš ï¸ å¥–åŠ±å»¶è¿Ÿï¼ˆ16æ­¥æ‰æœ‰MineCLIPä¿¡å·ï¼‰

**ä¼˜åŒ–**:
- ä½¿ç”¨GPUåŠ é€ŸMineCLIP
- é™ä½å¸§ç‡ï¼ˆå¦‚æ¯4å¸§é‡‡æ ·1å¸§ï¼Œæ€»å…±é‡‡æ ·16å¸§ï¼‰
- å¼‚æ­¥è®¡ç®—MineCLIPï¼ˆä¸é˜»å¡è®­ç»ƒï¼‰

---

## ğŸ“‹ **æ¨èæ‰§è¡Œé¡ºåº**

### **Day 1: é˜¶æ®µ1å¿«é€ŸéªŒè¯**

```bash
# 1. åœ¨train_get_wood.pyä¸­é›†æˆInventoryBasedReward
# 2. æµ‹è¯•10000æ­¥
./scripts/train_get_wood.sh test --timesteps 10000 --device cpu

# 3. è§‚å¯ŸTensorBoard
tensorboard --logdir logs/tensorboard

# 4. æ£€æŸ¥æŒ‡æ ‡ï¼š
# - explained_variance > 0 âœ…
# - é¦–æ¬¡è·å¾—æœ¨å¤´ < 5000æ­¥ âœ…
# - è®­ç»ƒç¨³å®šä¸å‘æ•£ âœ…
```

**å¦‚æœæˆåŠŸ** â†’ ç»§ç»­è®­ç»ƒ100000æ­¥  
**å¦‚æœå¤±è´¥** â†’ è°ƒæ•´å¥–åŠ±æƒé‡æˆ–æ£€æŸ¥ç¯å¢ƒ

---

### **Day 2: é˜¶æ®µ2ä¼˜åŒ–**

```bash
# æ·»åŠ åŠ¨ä½œé¼“åŠ±å’Œç§»åŠ¨æƒ©ç½š
# æµ‹è¯•ä¸åŒæƒé‡ç»„åˆï¼š
# - åº“å­˜: 1.0, æ”»å‡»: 0.01, ç§»åŠ¨: -0.001
# - åº“å­˜: 5.0, æ”»å‡»: 0.05, ç§»åŠ¨: -0.01
# - åº“å­˜: 10.0, æ”»å‡»: 0.1, ç§»åŠ¨: 0.0
```

---

### **Day 3-4: é˜¶æ®µ3ï¼ˆå¯é€‰ï¼‰**

åªæœ‰åœ¨é˜¶æ®µ1-2æ•ˆæœä»ä¸ç†æƒ³æ—¶æ‰è€ƒè™‘ã€‚

```python
# å®ç°16å¸§MineCLIP
# ç»“åˆåº“å­˜å¥–åŠ±ï¼š
total_reward = (
    inventory_reward * 10.0 +      # ä¸»è¦ä¿¡å·
    mineclip_reward * 2.0 +        # è¾…åŠ©ä¿¡å·
    action_reward * 0.1            # å¾®è°ƒä¿¡å·
)
```

---

## âš¡ **ç«‹å³è¡ŒåŠ¨ï¼šé›†æˆé˜¶æ®µ1**

ä¿®æ”¹ `train_get_wood.py`ï¼Œæ·»åŠ ç®€å•åº“å­˜å¥–åŠ±é€‰é¡¹ï¼š

```python
# åœ¨create_harvest_log_envä¸­æ·»åŠ 
def create_harvest_log_env(..., use_inventory_reward=False):
    env = make_minedojo_env(...)
    
    # ç®€å•åº“å­˜å¥–åŠ±ï¼ˆä¼˜å…ˆäºMineCLIPï¼‰
    if use_inventory_reward:
        from src.utils.simple_dense_reward import InventoryBasedReward
        env = InventoryBasedReward(env, target_item='log', reward_per_item=1.0)
    
    # MineCLIPå¥–åŠ±ï¼ˆå¯é€‰ï¼‰
    elif use_mineclip:
        env = MineCLIPRewardWrapper(...)
    
    env = Monitor(env)
    return env
```

ä¿®æ”¹ `train_get_wood.sh`ï¼š

```bash
# æ·»åŠ å‚æ•°
INVENTORY_REWARD=""  # é»˜è®¤ä¸å¯ç”¨

# è§£æå‚æ•°
--inventory-reward)
    INVENTORY_REWARD="--inventory-reward"
    shift
    ;;

# ä¼ é€’ç»™Python
$INVENTORY_REWARD
```

---

## ğŸ“Š **é¢„æœŸæ•ˆæœå¯¹æ¯”**

| æ–¹æ¡ˆ | å®ç°éš¾åº¦ | è®­ç»ƒæ—¶é—´ | æˆåŠŸç‡é¢„æµ‹ |
|------|---------|---------|-----------|
| **çº¯ç¨€ç–** | â­ | æ…¢ | 10% âŒ |
| **åº“å­˜å¥–åŠ±** | â­ | å¿« | 70% âœ… |
| **æ··åˆå¥–åŠ±** | â­â­ | ä¸­ | 85% âœ… |
| **16å¸§MineCLIP** | â­â­â­â­ | ä¸­ | 90% â­ |

---

## ğŸ¯ **å…³é”®å»ºè®®**

1. **å…ˆç®€å•åå¤æ‚**ï¼šåº“å­˜å¥–åŠ±å¯èƒ½å·²ç»è¶³å¤Ÿ
2. **å¿«é€Ÿè¿­ä»£**ï¼šæ¯ä¸ªé˜¶æ®µåªæµ‹è¯•10000æ­¥
3. **è§‚å¯ŸTensorBoard**ï¼šexplained_varianceæ˜¯å…³é”®æŒ‡æ ‡
4. **æƒé‡è°ƒæ•´**ï¼šæ ¹æ®å®é™…æ•ˆæœè°ƒæ•´ï¼Œä¸è¦ç›²ç›®ç…§æ¬

---

## ğŸ’¬ **ä¸‹ä¸€æ­¥**

**ç«‹å³æµ‹è¯•é˜¶æ®µ1ï¼Ÿ**
```bash
# æˆ‘å¯ä»¥å¸®ä½ ï¼š
# 1. ä¿®æ”¹train_get_wood.pyé›†æˆåº“å­˜å¥–åŠ±
# 2. æ›´æ–°train_get_wood.shæ·»åŠ å‚æ•°
# 3. è¿è¡Œ10000æ­¥å¿«é€Ÿæµ‹è¯•
# 4. åˆ†æç»“æœå†³å®šä¸‹ä¸€æ­¥
```

è¦ä¸è¦ç°åœ¨å°±å¼€å§‹å®æ–½é˜¶æ®µ1ï¼Ÿè¿™åº”è¯¥èƒ½åœ¨1å°æ—¶å†…çœ‹åˆ°æ•ˆæœï¼ğŸš€

