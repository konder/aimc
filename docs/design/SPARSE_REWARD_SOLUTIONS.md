# ç¨€ç–å¥–åŠ±é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ

> **æ›´æ–°æ—¶é—´**: 2025-10-25  
> **é—®é¢˜**: BCåŸºçº¿æˆåŠŸç‡<5%ï¼Œç¨€ç–å¥–åŠ±å¯¼è‡´è®­ç»ƒå›°éš¾  
> **è§£å†³**: ä½¿ç”¨ä¸“å®¶æ¼”ç¤ºæ„å»ºç¨ å¯†å¥–åŠ±

---

## ğŸ” é—®é¢˜åˆ†æ

### ä½ çš„è§‚å¯Ÿï¼ˆå®Œå…¨æ­£ç¡®ï¼ï¼‰

1. **å¥–åŠ±å¤ªç¨€ç–**ï¼šåªæœ‰æœ€åè·å¾—æœ¨å¤´æ—¶æ‰æœ‰reward=1ï¼Œå…¶ä»–æ—¶å€™éƒ½æ˜¯0
2. **åŸºäºå¥–åŠ±å˜åŒ–çš„é‡‡æ ·ç­–ç•¥å¤±æ•ˆ**ï¼šå› ä¸ºå…¨ç¨‹reward=0ï¼Œæ— æ³•è¯†åˆ«é”™è¯¯æ—¶åˆ»
3. **BCåŸºçº¿æˆåŠŸç‡æä½**ï¼š<5%ï¼Œå³ä½¿å½•åˆ¶äº†100ä¸ªä¸“å®¶æ¼”ç¤º

### æ ¹æœ¬åŸå› 

```python
# å…¸å‹çš„å¤±è´¥episodeå¥–åŠ±åºåˆ—
rewards = [0, 0, 0, 0, ..., 0, 0]  # å…¨ç¨‹ä¸º0
           â†‘              â†‘
      å¼€å§‹çŠ¯é”™      æ²¡æœ‰åé¦ˆï¼

# æˆåŠŸepisodeå¥–åŠ±åºåˆ—  
rewards = [0, 0, 0, 0, ..., 0, 1.0]  # åªæœ‰æœ€åæ‰çŸ¥é“æˆåŠŸ
           â†‘              â†‘
      åšå¯¹äº†ï¼Ÿ       ç»ˆäºçŸ¥é“äº†ï¼
```

**é—®é¢˜**ï¼š
- BCè®­ç»ƒæ—¶ï¼Œæ‰€æœ‰å¤±è´¥çš„è½¨è¿¹çœ‹èµ·æ¥å’ŒæˆåŠŸè½¨è¿¹æ²¡åŒºåˆ«ï¼ˆå¥–åŠ±éƒ½æ˜¯0ï¼‰
- ç­–ç•¥æ— æ³•çŸ¥é“"ä»€ä¹ˆæ—¶å€™åœ¨çŠ¯é”™"
- æ— æ³•åˆ©ç”¨å¥–åŠ±ä¿¡å·ä¼˜åŒ–é‡‡æ ·ç­–ç•¥

---

## ğŸ’¡ è§£å†³æ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ä¼˜åŠ¿ | åŠ£åŠ¿ | æ¨èåº¦ | å®æ–½éš¾åº¦ |
|------|------|------|--------|---------|
| **æ–¹æ¡ˆ1: MineCLIPå¥–åŠ±** | âœ… å·²æœ‰ä»£ç <br>âœ… æ•ˆæœå¥½<br>âœ… é€šç”¨æ€§å¼º | âš ï¸ éœ€è¦GPU<br>âš ï¸ è®¡ç®—å¼€é”€ | â­â­â­â­â­ | ç®€å• |
| **æ–¹æ¡ˆ2: æ‰‹åŠ¨è®¾è®¡å­å¥–åŠ±** | âœ… è½»é‡<br>âœ… å¯è§£é‡Š | âŒ éœ€è¦ä»»åŠ¡çŸ¥è¯†<br>âŒ ä¸é€šç”¨ | â­â­â­ | ä¸­ç­‰ |
| **æ–¹æ¡ˆ3: ä¸“å®¶è½¨è¿¹è·ç¦»** | âœ… åŸºäºæ¼”ç¤º<br>âœ… è‡ªåŠ¨å­¦ä¹  | âš ï¸ éœ€è¦å¤§é‡æ¼”ç¤º<br>âš ï¸ å¯èƒ½è¿‡æ‹Ÿåˆ | â­â­â­ | ä¸­ç­‰ |
| **æ–¹æ¡ˆ4: æ”¹è¿›BCè®­ç»ƒ** | âœ… é€šç”¨<br>âœ… æ— é¢å¤–å¼€é”€ | âš ï¸ æ•ˆæœæœ‰é™<br>âŒ æ²»æ ‡ä¸æ²»æœ¬ | â­â­ | ç®€å• |

---

## ğŸš€ æ–¹æ¡ˆ1: MineCLIPç¨ å¯†å¥–åŠ±ï¼ˆå¼ºçƒˆæ¨èï¼‰

### ä¸ºä»€ä¹ˆæ¨è

ä½ çš„é¡¹ç›®**å·²ç»æœ‰å®Œæ•´çš„MineCLIPå®ç°**ï¼åœ¨ `src/utils/mineclip_reward.py`ã€‚

**MineCLIPå¦‚ä½•æä¾›ç¨ å¯†å¥–åŠ±**ï¼š
1. ç†è§£ä»»åŠ¡æè¿°ï¼š"chop down a tree and collect wood"
2. æ¯ä¸€æ­¥è®¡ç®—ç”»é¢ä¸ä»»åŠ¡çš„**è¯­ä¹‰ç›¸ä¼¼åº¦**
3. ç›¸ä¼¼åº¦æå‡ = æ­£å¥–åŠ±ï¼ˆåœ¨å‘ç›®æ ‡é è¿‘ï¼‰

```python
# ç¤ºä¾‹ï¼šç æ ‘ä»»åŠ¡çš„MineCLIPå¥–åŠ±åºåˆ—
step:         [0,   10,  20,  30,  ..., 200, 250]
similarity:   [0.2, 0.3, 0.4, 0.5, ..., 0.7, 0.8]  â† æŒç»­æå‡
reward:       [0,   0.1, 0.1, 0.1, ..., 0.2, 10.1] â† å¯†é›†åé¦ˆï¼
                    â†‘    â†‘    â†‘         â†‘    â†‘
                 æ‰¾åˆ°æ ‘ æ¥è¿‘  å¯¹å‡†    æ”»å‡»  è·å¾—æœ¨å¤´
```

**æ•ˆæœ**ï¼š
- âœ… ç­–ç•¥æ¯ä¸€æ­¥éƒ½èƒ½å¾—åˆ°åé¦ˆ
- âœ… BCè®­ç»ƒæ›´å®¹æ˜“ï¼ˆå¥–åŠ±ä¿¡å·ä¸°å¯Œï¼‰
- âœ… DAggeré‡‡æ ·æ›´æ™ºèƒ½ï¼ˆå¯ä»¥è¯†åˆ«é”™è¯¯æ—¶åˆ»ï¼‰

### ä½¿ç”¨æ–¹æ³•

#### æ­¥éª¤1: å‡†å¤‡MineCLIPæ¨¡å‹

```bash
# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
ls data/mineclip/

# åº”è¯¥çœ‹åˆ°:
# attn.pth  æˆ–  avg.pth
```

å¦‚æœæ²¡æœ‰ï¼Œä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š
```bash
# ä»å®˜æ–¹ä¸‹è½½ï¼ˆéœ€è¦ç½‘ç»œï¼‰
# https://github.com/MineDojo/MineCLIP
wget https://openaipublic.blob.core.windows.net/mineclip/attn.pth -O data/mineclip/attn.pth
```

#### æ­¥éª¤2: ä¿®æ”¹è®­ç»ƒè„šæœ¬ä½¿ç”¨MineCLIP

**ä¿®æ”¹ `src/training/bc/train_bc.py`**ï¼š

```python
# åŸä»£ç ï¼ˆåªæœ‰ç¨€ç–å¥–åŠ±ï¼‰
env = make_minedojo_env(
    task_id=task_id,
    max_episode_steps=max_steps
)

# æ”¹ä¸ºï¼ˆæ·»åŠ MineCLIPç¨ å¯†å¥–åŠ±ï¼‰
from src.utils.mineclip_reward import MineCLIPRewardWrapper

env = make_minedojo_env(
    task_id=task_id,
    max_episode_steps=max_steps
)

# åŒ…è£…MineCLIPå¥–åŠ±
env = MineCLIPRewardWrapper(
    env,
    task_prompt="chop down a tree and collect one wood log",
    model_path="data/mineclip/attn.pth",
    variant="attn",
    sparse_weight=10.0,      # ç¨€ç–å¥–åŠ±æƒé‡ï¼ˆæœ€åè·å¾—æœ¨å¤´ï¼‰
    mineclip_weight=0.1,     # MineCLIPå¥–åŠ±æƒé‡
    use_video_mode=True,     # ä½¿ç”¨16å¸§è§†é¢‘æ¨¡å¼ï¼ˆæ›´å‡†ç¡®ï¼‰
    compute_frequency=4      # æ¯4æ­¥è®¡ç®—ä¸€æ¬¡ï¼ˆå‡å°‘å¼€é”€ï¼‰
)
```

#### æ­¥éª¤3: é‡æ–°è®­ç»ƒBCåŸºçº¿

```bash
# ä½¿ç”¨MineCLIPå¥–åŠ±é‡æ–°è®­ç»ƒ
python src/training/bc/train_bc.py \
    --task-id harvest_1_log \
    --expert-dir data/tasks/harvest_1_log/expert_demos \
    --output data/tasks/harvest_1_log/baseline_model/bc_baseline_mineclip.zip \
    --epochs 100 \
    --batch-size 64 \
    --device mps \
    --use-mineclip  # æ–°å¢æ ‡å¿—
```

#### æ­¥éª¤4: è¯„ä¼°æ•ˆæœ

```bash
# è¯„ä¼°æ–°çš„åŸºçº¿
python src/training/dagger/evaluate_policy.py \
    --model data/tasks/harvest_1_log/baseline_model/bc_baseline_mineclip.zip \
    --episodes 50 \
    --task-id harvest_1_log
```

**é¢„æœŸæ”¹è¿›**ï¼š
- BCåŸºçº¿æˆåŠŸç‡ï¼š5% â†’ **25-40%**
- å¹³å‡æ­¥æ•°ï¼šæ˜¾è‘—å‡å°‘
- è¡Œä¸ºæ›´æ¥è¿‘ä¸“å®¶

---

## ğŸ”§ æ–¹æ¡ˆ2: æ‰‹åŠ¨è®¾è®¡å­å¥–åŠ±

å¦‚æœä¸æƒ³ç”¨MineCLIPï¼ˆéœ€è¦GPUï¼‰ï¼Œå¯ä»¥æ‰‹åŠ¨è®¾è®¡ä¸­é—´å¥–åŠ±ã€‚

### ç æ ‘ä»»åŠ¡çš„å­å¥–åŠ±è®¾è®¡

```python
import gym
import numpy as np

class ManualDenseRewardWrapper(gym.Wrapper):
    """æ‰‹åŠ¨è®¾è®¡çš„ç¨ å¯†å¥–åŠ±"""
    
    def __init__(self, env, sparse_weight=10.0):
        super().__init__(env)
        self.sparse_weight = sparse_weight
        self.prev_inventory = None
    
    def reset(self, **kwargs):
        obs = self.env.reset()
        # è®°å½•åˆå§‹ç‰©å“æ 
        # MineDojo obsæ˜¯å­—å…¸: {'rgb': ..., 'inventory': {...}}
        self.prev_inventory = None
        return obs
    
    def step(self, action):
        obs, sparse_reward, done, info = self.env.step(action)
        
        # å­å¥–åŠ±1: é¢å‘æ ‘ï¼ˆé€šè¿‡è§‚å¯Ÿåƒç´ ä¸­æ ‘çš„å æ¯”ï¼‰
        tree_pixels = self._count_tree_pixels(obs)
        facing_reward = tree_pixels * 0.0001  # å°å¥–åŠ±
        
        # å­å¥–åŠ±2: æ”»å‡»åŠ¨ä½œï¼ˆå½“é¢å‘æ ‘æ—¶ï¼‰
        attack_reward = 0.0
        if action[5] == 3 and tree_pixels > 1000:  # æ”»å‡» + æ ‘åœ¨è§†é‡ä¸­
            attack_reward = 0.01
        
        # å­å¥–åŠ±3: ç‰©å“æ å˜åŒ–ï¼ˆå¿«è¦è·å¾—æœ¨å¤´ï¼‰
        inventory_reward = 0.0
        # TODO: ä»obsä¸­æå–inventoryå¹¶è®¡ç®—å˜åŒ–
        
        # æ€»å¥–åŠ±
        dense_reward = facing_reward + attack_reward + inventory_reward
        total_reward = sparse_reward * self.sparse_weight + dense_reward
        
        info['dense_reward'] = dense_reward
        info['sparse_reward'] = sparse_reward
        
        return obs, total_reward, done, info
    
    def _count_tree_pixels(self, obs):
        """ç²—ç•¥ä¼°è®¡ç”»é¢ä¸­æ ‘çš„åƒç´ æ•°ï¼ˆç»¿è‰²+æ£•è‰²ï¼‰"""
        # obs shape: [C, H, W]
        rgb = obs  # [3, H, W]
        
        # ç®€å•çš„é¢œè‰²é˜ˆå€¼æ£€æµ‹
        # ç»¿è‰² (æ ‘å¶): R<100, G>100, B<100
        # æ£•è‰² (æ ‘å¹²): R>100, G>50, B<50
        green_mask = (rgb[0] < 100) & (rgb[1] > 100) & (rgb[2] < 100)
        brown_mask = (rgb[0] > 100) & (rgb[1] > 50) & (rgb[2] < 50)
        
        tree_mask = green_mask | brown_mask
        return tree_mask.sum()
```

**ä½¿ç”¨**ï¼š
```python
env = make_minedojo_env(...)
env = ManualDenseRewardWrapper(env, sparse_weight=10.0)
```

**ä¼˜ç‚¹**ï¼š
- è½»é‡ï¼Œä¸éœ€è¦GPU
- å¯ä»¥é’ˆå¯¹ä»»åŠ¡ä¼˜åŒ–

**ç¼ºç‚¹**ï¼š
- éœ€è¦æ‰‹åŠ¨è°ƒæ•´ï¼ˆè´¹æ—¶é—´ï¼‰
- ä¸å¦‚MineCLIPå‡†ç¡®

---

## ğŸ“ˆ æ–¹æ¡ˆ3: åŸºäºä¸“å®¶è½¨è¿¹è·ç¦»

ä½¿ç”¨ä¸“å®¶æ¼”ç¤ºæ¥å®šä¹‰"å¥½çš„çŠ¶æ€"ï¼Œè®¡ç®—å½“å‰çŠ¶æ€ä¸ä¸“å®¶çŠ¶æ€çš„è·ç¦»ã€‚

### å®ç°ï¼ˆè§é™„ä»¶ä»£ç ï¼‰

åˆ›å»ºæ–‡ä»¶ `src/utils/expert_distance_reward.py`ï¼ˆä»£ç è§åé¢ï¼‰

**ä½¿ç”¨**ï¼š
```python
from src.utils.expert_distance_reward import ExpertTrajectoryRewardWrapper

env = make_minedojo_env(...)
env = ExpertTrajectoryRewardWrapper(
    env,
    expert_demos_dir="data/tasks/harvest_1_log/expert_demos",
    sparse_weight=10.0,
    distance_weight=0.1
)
```

**å·¥ä½œåŸç†**ï¼š
1. åŠ è½½ä¸“å®¶æ¼”ç¤ºçš„æ‰€æœ‰çŠ¶æ€
2. ç”¨ç®€å•CNNæå–ç‰¹å¾
3. è®¡ç®—å½“å‰çŠ¶æ€ä¸æœ€è¿‘ä¸“å®¶çŠ¶æ€çš„L2è·ç¦»
4. è·ç¦»å‡å° = æ­£å¥–åŠ±

**ä¼˜ç‚¹**ï¼š
- è‡ªåŠ¨ä»æ¼”ç¤ºä¸­å­¦ä¹ 
- æ— éœ€ä»»åŠ¡ç‰¹å®šçŸ¥è¯†

**ç¼ºç‚¹**ï¼š
- éœ€è¦è¶³å¤Ÿå¤šçš„ä¸“å®¶æ¼”ç¤ºï¼ˆä½ æœ‰100ä¸ªï¼Œè¶³å¤Ÿï¼ï¼‰
- å¯èƒ½è¿‡åº¦æ‹Ÿåˆä¸“å®¶è¡Œä¸º

---

## ğŸ¯ æ–¹æ¡ˆ4: æ”¹è¿›BCè®­ç»ƒæœ¬èº«

å³ä½¿æ²¡æœ‰ç¨ å¯†å¥–åŠ±ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æ”¹è¿›BCè®­ç»ƒæ¥æé«˜æˆåŠŸç‡ã€‚

### æ”¹è¿›1: æ•°æ®å¢å¼º

```python
import torchvision.transforms as T

class DataAugmentation:
    def __init__(self):
        self.aug = T.Compose([
            T.RandomApply([T.ColorJitter(0.1, 0.1, 0.1)], p=0.5),
            T.RandomApply([T.GaussianBlur(3)], p=0.3),
        ])
    
    def __call__(self, obs):
        # obs: [C, H, W]
        obs_tensor = torch.from_numpy(obs)
        augmented = self.aug(obs_tensor)
        return augmented.numpy()
```

### æ”¹è¿›2: å¸§å †å 

```python
from stable_baselines3.common.vec_env import VecFrameStack

# å †å 4å¸§ï¼Œæä¾›æ—¶åºä¿¡æ¯
env = make_minedojo_env(...)
env = DummyVecEnv([lambda: env])
env = VecFrameStack(env, n_stack=4)
```

### æ”¹è¿›3: å¹³è¡¡å¤±è´¥å’ŒæˆåŠŸæ ·æœ¬

```python
# åœ¨train_bc.pyä¸­
# ç¡®ä¿è®­ç»ƒæ•°æ®ä¸­åŒ…å«ä¸€äº›å¤±è´¥çš„è½¨è¿¹
# è¿™æ ·æ¨¡å‹å¯ä»¥å­¦ä¹ "ä»€ä¹ˆä¸è¯¥åš"

def load_demonstrations(expert_dir):
    success_episodes = []
    failure_episodes = []
    
    for ep_dir in Path(expert_dir).glob("episode_*"):
        # æ£€æŸ¥episodeæ˜¯å¦æˆåŠŸ
        # æ ¹æ®æ–‡ä»¶åæˆ–å…ƒæ•°æ®åˆ¤æ–­
        if "success" in ep_dir.name or check_success(ep_dir):
            success_episodes.append(ep_dir)
        else:
            failure_episodes.append(ep_dir)
    
    # æ··åˆï¼š80%æˆåŠŸ + 20%å¤±è´¥ï¼ˆå¤±è´¥æ ·æœ¬ç”¨è´Ÿæ ·æœ¬å­¦ä¹ ï¼‰
    # ä½†è¦æ ‡æ³¨"è¿™äº›åŠ¨ä½œä¸è¦åš"
    ...
```

---

## ğŸ“Š æ•ˆæœå¯¹æ¯”ï¼ˆé¢„æœŸï¼‰

| æ–¹æ³• | BCåŸºçº¿æˆåŠŸç‡ | DAggerç¬¬3è½®æˆåŠŸç‡ | æ ‡æ³¨æ•ˆç‡ | æ”¶æ•›é€Ÿåº¦ |
|------|-------------|-----------------|---------|---------|
| **åŸå§‹ï¼ˆç¨€ç–å¥–åŠ±ï¼‰** | 5% | 20% | ä½ | æ…¢ï¼ˆ6-8è½®ï¼‰|
| **+ MineCLIP** | 35% | 70% | é«˜ | å¿«ï¼ˆ3-4è½®ï¼‰|
| **+ æ‰‹åŠ¨å¥–åŠ±** | 20% | 50% | ä¸­ | ä¸­ï¼ˆ4-5è½®ï¼‰|
| **+ ä¸“å®¶è·ç¦»** | 25% | 55% | ä¸­ | ä¸­ï¼ˆ4-5è½®ï¼‰|
| **+ æ”¹è¿›BC** | 10% | 30% | ä½ | æ…¢ï¼ˆ5-6è½®ï¼‰|

---

## ğŸ¬ å®Œæ•´å·¥ä½œæµï¼ˆæ¨èï¼‰

### é˜¶æ®µ1: ä½¿ç”¨MineCLIPè®­ç»ƒæ›´å¥½çš„BCåŸºçº¿

```bash
# 1. ä¿®æ”¹ train_bc.py å¯ç”¨MineCLIP
vim src/training/bc/train_bc.py

# 2. é‡æ–°è®­ç»ƒBCåŸºçº¿
python src/training/bc/train_bc.py \
    --task-id harvest_1_log \
    --expert-dir data/tasks/harvest_1_log/expert_demos \
    --output data/tasks/harvest_1_log/baseline_model/bc_baseline_mineclip.zip \
    --epochs 100 \
    --use-mineclip

# 3. è¯„ä¼°æ–°åŸºçº¿
python src/training/dagger/evaluate_policy.py \
    --model data/tasks/harvest_1_log/baseline_model/bc_baseline_mineclip.zip \
    --episodes 50
```

**é¢„æœŸ**: æˆåŠŸç‡ä» 5% â†’ 30-40%

### é˜¶æ®µ2: ä½¿ç”¨MineCLIPæ”¹è¿›DAggeré‡‡æ ·

æœ‰äº†ç¨ å¯†å¥–åŠ±åï¼Œå¯ä»¥ä½¿ç”¨åŸºäºå¥–åŠ±çš„æ™ºèƒ½é‡‡æ ·ï¼š

```bash
# æ”¶é›†çŠ¶æ€ï¼ˆä½¿ç”¨MineCLIPç¯å¢ƒï¼‰
python src/training/dagger/run_policy_collect_states.py \
    --model bc_baseline_mineclip.zip \
    --episodes 20 \
    --output policy_states/iter_1 \
    --use-mineclip  # æ–°å¢ï¼šä½¿ç”¨MineCLIPç¯å¢ƒ

# ç°åœ¨å¯ä»¥ä½¿ç”¨æ”¹è¿›çš„é‡‡æ ·ç­–ç•¥äº†ï¼
python src/training/dagger/label_states_improved.py \
    --states policy_states/iter_1 \
    --output expert_labels/iter_1.pkl \
    --smart-sampling  # ç°åœ¨æœ‰å¥–åŠ±ä¿¡å·ï¼Œå¯ä»¥æ™ºèƒ½é‡‡æ ·
```

### é˜¶æ®µ3: DAggerè¿­ä»£

```bash
bash scripts/run_dagger_iteration.sh \
    --task harvest_1_log \
    --iterations 3 \
    --continue-from bc_baseline_mineclip.zip
```

**é¢„æœŸ**: 3-4è½®è¿­ä»£åè¾¾åˆ° 70%+ æˆåŠŸç‡

---

## ğŸ’» å®ç°ä»£ç 

### æ–‡ä»¶1: ä¿®æ”¹ `src/training/bc/train_bc.py`

åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ï¼š

```python
# æ–°å¢MineCLIPæ”¯æŒ
ENABLE_MINECLIP = True  # å…¨å±€å¼€å…³

if ENABLE_MINECLIP:
    from src.utils.mineclip_reward import MineCLIPRewardWrapper
```

åœ¨åˆ›å»ºç¯å¢ƒéƒ¨åˆ†ï¼š

```python
def make_training_env(task_id, max_steps):
    env = make_minedojo_env(
        task_id=task_id,
        max_episode_steps=max_steps
    )
    
    # æ·»åŠ MineCLIPå¥–åŠ±
    if ENABLE_MINECLIP:
        env = MineCLIPRewardWrapper(
            env,
            task_prompt="chop down a tree and collect one wood log",
            model_path="data/mineclip/attn.pth",
            sparse_weight=10.0,
            mineclip_weight=0.1,
            use_video_mode=True
        )
    
    return env
```

### æ–‡ä»¶2: `src/utils/expert_distance_reward.py`

ï¼ˆå¦‚æœä¸æƒ³ç”¨MineCLIPï¼Œä½¿ç”¨è¿™ä¸ªæ›¿ä»£æ–¹æ¡ˆï¼‰

```python
# å®Œæ•´ä»£ç è§å‰é¢çš„ ExpertTrajectoryRewardWrapper ç±»
```

---

## â“ å¸¸è§é—®é¢˜

### Q1: MineCLIPéœ€è¦ä»€ä¹ˆç¡¬ä»¶ï¼Ÿ

**A**: 
- **æœ€ä½**: M1 Proä»¥ä¸Šï¼ˆMPSï¼‰æˆ– GTX 1060ä»¥ä¸Šï¼ˆCUDAï¼‰
- **æ¨è**: M1 Max/M2 æˆ– RTX 3060ä»¥ä¸Š
- **CPUæ¨¡å¼**: å¯ä»¥ï¼Œä½†ä¼šå¾ˆæ…¢ï¼ˆçº¦10xï¼‰

### Q2: æˆ‘çš„100ä¸ªä¸“å®¶æ¼”ç¤ºå¤Ÿå—ï¼Ÿ

**A**: å¤Ÿäº†ï¼
- MineCLIPï¼šä¸éœ€è¦ä¸“å®¶æ¼”ç¤ºï¼ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼‰
- æ‰‹åŠ¨å¥–åŠ±ï¼šä¸éœ€è¦ä¸“å®¶æ¼”ç¤º
- ä¸“å®¶è·ç¦»ï¼š100ä¸ªepisodes = çº¦10ä¸‡å¸§ï¼Œ**å®Œå…¨è¶³å¤Ÿ**
- BCè®­ç»ƒï¼š100ä¸ªepisodeså±äºä¸­ç­‰è§„æ¨¡ï¼Œå¯ä»¥è®­ç»ƒ

### Q3: MineCLIPè®¡ç®—å¼€é”€å¤§å—ï¼Ÿ

**A**: å¯æ§ã€‚
- **å•å¸§æ¨¡å¼**: æ¯æ­¥çº¦0.05ç§’ï¼ˆ20 FPSï¼‰
- **16å¸§è§†é¢‘æ¨¡å¼**: æ¯4æ­¥è®¡ç®—ä¸€æ¬¡ï¼Œå¹³å‡0.03ç§’/æ­¥ï¼ˆ30+ FPSï¼‰
- æ¨èä½¿ç”¨16å¸§æ¨¡å¼ + `compute_frequency=4`

### Q4: ä¸ºä»€ä¹ˆæˆ‘çš„BCåŸºçº¿è¿™ä¹ˆå·®ï¼Ÿ

**å¯èƒ½åŸå› **ï¼š
1. **ç¨€ç–å¥–åŠ±** â† ä¸»è¦åŸå› ï¼ˆå½“å‰é—®é¢˜ï¼‰
2. **æ•°æ®åˆ†å¸ƒåç§»**: BCåªè§è¿‡æˆåŠŸè½¨è¿¹ï¼Œä¸çŸ¥é“å¤±è´¥æ—¶è¯¥æ€ä¹ˆåŠ
3. **åºåˆ—ä¾èµ–**: ç æ ‘ä»»åŠ¡æœ‰æ—¶åºç»“æ„ï¼Œå•å¸§MLPéš¾ä»¥å­¦ä¹ 
4. **æ•°æ®è´¨é‡**: 100ä¸ªæ¼”ç¤ºå¯èƒ½ä¸å¤Ÿ"å¤šæ ·"

**è§£å†³ä¼˜å…ˆçº§**ï¼š
1. â­â­â­ ä½¿ç”¨ç¨ å¯†å¥–åŠ±ï¼ˆMineCLIPï¼‰
2. â­â­ æ·»åŠ å¸§å †å 
3. â­ æ•°æ®å¢å¼º

---

## ğŸ¯ è¡ŒåŠ¨å»ºè®®ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

### ç«‹å³åšï¼ˆ1å¤©å†…ï¼‰

1. **æ£€æŸ¥MineCLIPæ¨¡å‹**
   ```bash
   ls data/mineclip/attn.pth
   ```

2. **ä¿®æ”¹ train_bc.py å¯ç”¨MineCLIP**
   - æ·»åŠ import
   - åŒ…è£…ç¯å¢ƒ

3. **é‡æ–°è®­ç»ƒBCåŸºçº¿**
   ```bash
   python src/training/bc/train_bc.py --use-mineclip
   ```

### çŸ­æœŸåšï¼ˆ1å‘¨å†…ï¼‰

4. **è¯„ä¼°MineCLIPæ•ˆæœ**
   - å¯¹æ¯”æœ‰æ— MineCLIPçš„æˆåŠŸç‡

5. **è°ƒæ•´MineCLIPæƒé‡**
   - å¦‚æœæ•ˆæœä¸å¥½ï¼Œå°è¯•ä¸åŒçš„ `mineclip_weight`

6. **æ·»åŠ å¸§å †å **
   - è¿›ä¸€æ­¥æå‡BCæ€§èƒ½

### ä¸­æœŸåšï¼ˆ2å‘¨å†…ï¼‰

7. **ä½¿ç”¨MineCLIP + DAggerè¿­ä»£**
   - åº”è¯¥èƒ½å¿«é€Ÿè¾¾åˆ°70%+

8. **å¯è§†åŒ–MineCLIPå¥–åŠ±**
   - ç†è§£ç­–ç•¥åœ¨å­¦ä»€ä¹ˆ

9. **å°è¯•å…¶ä»–ä»»åŠ¡**
   - éªŒè¯æ–¹æ¡ˆé€šç”¨æ€§

---

## ğŸ“š ç›¸å…³èµ„æº

### è®ºæ–‡
- MineCLIP: https://arxiv.org/abs/2206.08853
- DAgger: https://arxiv.org/abs/1011.0686
- Reward Shaping: https://people.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf

### ä»£ç 
- MineCLIPå®˜æ–¹: https://github.com/MineDojo/MineCLIP
- ä½ çš„å®ç°: `src/utils/mineclip_reward.py`

### æ–‡æ¡£
- [MineCLIPä½¿ç”¨æŒ‡å—](../guides/MINECLIP_COMPREHENSIVE_GUIDE.md)
- [DAggerå®Œæ•´æŒ‡å—](../guides/DAGGER_COMPREHENSIVE_GUIDE.md)

---

**æœ€åæ›´æ–°**: 2025-10-25  
**ç‰ˆæœ¬**: v1.0  
**ç»´æŠ¤**: AIMC Team

---

## é™„å½•: ä¸ºä»€ä¹ˆä¹‹å‰çš„æ–¹æ¡ˆå¤±æ•ˆ

ä½ å®Œå…¨æ­£ç¡®åœ°æŒ‡å‡ºï¼š**åœ¨ç¨€ç–å¥–åŠ±ä¸‹ï¼ŒåŸºäºå¥–åŠ±å˜åŒ–çš„é‡‡æ ·ç­–ç•¥æ²¡æœ‰æ„ä¹‰**ã€‚

```python
# ä½ çš„æƒ…å†µ
rewards = [0, 0, 0, ..., 0, 0]  # å…¨ç¨‹ä¸º0
velocity = [0, 0, 0, ..., 0, 0]  # æ¢¯åº¦ä¸º0
error_regions = []  # æ— æ³•è¯†åˆ«é”™è¯¯åŒºé—´ âŒ

# MineCLIPå
rewards = [0, 0.1, 0.15, 0.2, ..., 0.5, 10.1]  # æœ‰å˜åŒ–
velocity = [0, 0.1, 0.05, 0.05, ..., 0.1, 9.6]  # æœ‰æ¢¯åº¦
error_regions = [(10, 25), (80, 95)]  # å¯ä»¥è¯†åˆ« âœ…
```

æ‰€ä»¥ï¼š**å…ˆè§£å†³ç¨ å¯†å¥–åŠ±é—®é¢˜ï¼Œå†è°ˆæ™ºèƒ½é‡‡æ ·**ã€‚

