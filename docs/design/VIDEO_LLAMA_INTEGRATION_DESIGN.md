# Video-LLaMAé›†æˆæ–¹æ¡ˆè®¾è®¡

**åˆ›å»ºæ—¥æœŸ**: 2025-11-28  
**ç›®æ ‡**: ä½¿ç”¨Video-LLaMAæ›¿ä»£MineCLIPå®ç°æ–‡æœ¬â†’ç›®æ ‡ç”»é¢â†’VPTå¯¹é½  
**é¢„æœŸæ”¹è¿›**: æé«˜ä»»åŠ¡åŒºåˆ†åº¦50-80%

---

## ğŸ¯ æ–¹æ¡ˆæ¦‚è¿°

### å½“å‰æ¶æ„ï¼ˆSTEVE-1 + MineCLIPï¼‰

```
ç”¨æˆ·æŒ‡ä»¤ (text)
    â†“
MineCLIP Text Encoder [512-d]
    â†“
Prior VAE (CVAE) [512-d â†’ 512-d]
    â†“
z_goal (visual embedding) [512-d]
    â†“
VPT Policy (conditioned on z_goal)
    â†“
Minecraft Actions
```

**é—®é¢˜**ï¼š
- âŒ MineCLIPæ–‡æœ¬åŒºåˆ†åº¦ä½ï¼ˆ1.3%ï¼‰
- âŒ MineCLIPè§†è§‰ç›¸ä¼¼åº¦é«˜ï¼ˆ0.925ï¼‰
- âŒ Prior VAEè¿›ä¸€æ­¥collapseï¼ˆ0.873ï¼‰

### æ–°æ¶æ„ï¼ˆVideo-LLaMAæ–¹æ¡ˆï¼‰

```
ç”¨æˆ·æŒ‡ä»¤ (text)
    â†“
Video-LLaMA [7B/13B parameters]
    â”œâ”€ Text Encoder (LLM)
    â””â”€ Visual Query Tokens
        â†“
    Goal Video Features [NÃ—4096-d]
        â†“
    Alignment Module (Adapter/Projector) [4096-d â†’ 512-d]
        â†“
    z_goal (VPT-compatible) [512-d]
        â†“
    VPT Policy (conditioned on z_goal)
        â†“
    Minecraft Actions
```

**ä¼˜åŠ¿**ï¼š
- âœ… Video-LLaMAå¼ºå¤§çš„æ–‡æœ¬ç†è§£ï¼ˆåŸºäºLLaMAï¼‰
- âœ… æ›´å¥½çš„è§†é¢‘-æ–‡æœ¬å¯¹é½
- âœ… å¯ä»¥ç”Ÿæˆç›®æ ‡ç”»é¢çš„è¯¦ç»†æè¿°
- âœ… æ”¯æŒå¤æ‚æŒ‡ä»¤ç†è§£

---

## ğŸ“ è¯¦ç»†æ¶æ„è®¾è®¡

### é˜¶æ®µ1: Video-LLaMAåŸºç¡€æ¶æ„

#### 1.1 Video-LLaMAç»„ä»¶

```python
class VideoLLaMAForGoalPrediction:
    """
    ä½¿ç”¨Video-LLaMAé¢„æµ‹ç›®æ ‡ç”»é¢
    """
    def __init__(self):
        # 1. è§†è§‰ç¼–ç å™¨ (Q-Former from BLIP-2)
        self.visual_encoder = VisionTransformer()  # EVA-CLIP ViT-g/14
        self.visual_qformer = QFormer()  # 32 query tokens
        
        # 2. è¯­è¨€æ¨¡å‹ (LLaMA)
        self.language_model = LLaMA_7B()  # æˆ– 13B
        
        # 3. è§†é¢‘é€‚é…å™¨
        self.video_adapter = VideoQFormer()  # æ—¶åºå»ºæ¨¡
        
        # 4. ç›®æ ‡ç”»é¢é¢„æµ‹å¤´
        self.goal_predictor = GoalPredictionHead()
    
    def forward(self, text_instruction):
        """
        è¾“å…¥: æ–‡æœ¬æŒ‡ä»¤ "chop tree, get a log"
        è¾“å‡º: ç›®æ ‡ç”»é¢çš„visual features
        """
        # Step 1: æ–‡æœ¬ç¼–ç 
        text_embeds = self.language_model.encode_text(text_instruction)
        # [1, seq_len, 4096]
        
        # Step 2: ç”Ÿæˆvisual query tokens
        # ä½¿ç”¨LLaMAçš„è¾“å‡ºä½œä¸ºæ¡ä»¶ï¼Œç”Ÿæˆç›®æ ‡ç”»é¢çš„query
        visual_queries = self.goal_predictor.generate_queries(text_embeds)
        # [1, 32, 768] - 32ä¸ªquery tokens
        
        # Step 3: é€šè¿‡Q-Formerå¾—åˆ°visual features
        goal_visual_features = self.visual_qformer(visual_queries)
        # [1, 32, 768]
        
        return goal_visual_features
```

#### 1.2 ç›®æ ‡ç”»é¢é¢„æµ‹å¤´è®¾è®¡

```python
class GoalPredictionHead(nn.Module):
    """
    ä»æ–‡æœ¬é¢„æµ‹ç›®æ ‡ç”»é¢çš„visual features
    
    çµæ„Ÿæ¥æºï¼šBLIP-2çš„image generationï¼Œä½†è¿™é‡Œæ˜¯é¢„æµ‹è€Œéç”Ÿæˆ
    """
    def __init__(self, llama_dim=4096, query_dim=768, num_queries=32):
        super().__init__()
        
        # 1. Text-to-QueryæŠ•å½±å±‚
        self.text_to_query = nn.Sequential(
            nn.Linear(llama_dim, 2048),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(2048, num_queries * query_dim),
        )
        
        # 2. Query refinementï¼ˆå¯é€‰ï¼Œä½¿ç”¨Transformerï¼‰
        self.query_refiner = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=query_dim, nhead=8),
            num_layers=4
        )
        
        # 3. å¯å­¦ä¹ çš„query embeddingsï¼ˆç±»ä¼¼DETRçš„object queriesï¼‰
        self.learnable_queries = nn.Parameter(
            torch.randn(num_queries, query_dim)
        )
    
    def generate_queries(self, text_embeds):
        """
        ä»æ–‡æœ¬embeddingç”Ÿæˆvisual query tokens
        
        Args:
            text_embeds: [B, seq_len, 4096] - LLaMAçš„è¾“å‡º
        
        Returns:
            visual_queries: [B, 32, 768] - ç›®æ ‡ç”»é¢çš„query tokens
        """
        B = text_embeds.shape[0]
        
        # 1. æ± åŒ–æ–‡æœ¬ç‰¹å¾
        text_pooled = text_embeds.mean(dim=1)  # [B, 4096]
        
        # 2. æŠ•å½±åˆ°query space
        queries = self.text_to_query(text_pooled)  # [B, 32*768]
        queries = queries.view(B, 32, 768)
        
        # 3. ä¸å¯å­¦ä¹ çš„queriesç›¸åŠ ï¼ˆç±»ä¼¼positional encodingï¼‰
        queries = queries + self.learnable_queries.unsqueeze(0)
        
        # 4. Refine queries
        queries = queries.transpose(0, 1)  # [32, B, 768] for Transformer
        refined_queries = self.query_refiner(queries)
        refined_queries = refined_queries.transpose(0, 1)  # [B, 32, 768]
        
        return refined_queries
```

### é˜¶æ®µ2: å¯¹é½åˆ°VPT

#### 2.1 å¯¹é½æ¨¡å—è®¾è®¡

```python
class VideoLLaMAToVPTAligner(nn.Module):
    """
    å°†Video-LLaMAçš„visual featureså¯¹é½åˆ°VPTçš„visual embeddingç©ºé—´
    
    å…³é”®ï¼šVPTä½¿ç”¨çš„æ˜¯IDM (Inverse Dynamics Model) è®­ç»ƒçš„visual encoder
    è¾“å‡ºç»´åº¦ï¼š512-dï¼ˆä¸MineCLIPä¸€è‡´ï¼‰
    """
    def __init__(
        self, 
        videollama_dim=768,  # Q-Former output dim
        num_queries=32,
        vpt_dim=512,
        use_attention=True
    ):
        super().__init__()
        
        # æ–¹æ¡ˆA: ç®€å•æŠ•å½±ï¼ˆç±»ä¼¼MineCLIP Priorï¼‰
        if not use_attention:
            self.aligner = nn.Sequential(
                nn.Linear(num_queries * videollama_dim, 2048),
                nn.LayerNorm(2048),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(2048, 1024),
                nn.LayerNorm(1024),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(1024, vpt_dim),
            )
            self.use_attention = False
        
        # æ–¹æ¡ˆB: æ³¨æ„åŠ›æ± åŒ–ï¼ˆæ¨èï¼‰
        else:
            # 1. è·¨queryçš„æ³¨æ„åŠ›æ± åŒ–
            self.query_attention = nn.MultiheadAttention(
                embed_dim=videollama_dim,
                num_heads=8,
                batch_first=True
            )
            
            # 2. å¯å­¦ä¹ çš„pooling query
            self.pool_query = nn.Parameter(torch.randn(1, 1, videollama_dim))
            
            # 3. æŠ•å½±åˆ°VPTç©ºé—´
            self.projection = nn.Sequential(
                nn.Linear(videollama_dim, 1024),
                nn.LayerNorm(1024),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(1024, vpt_dim),
            )
            self.use_attention = True
    
    def forward(self, videollama_features):
        """
        Args:
            videollama_features: [B, 32, 768] - Video-LLaMAçš„visual features
        
        Returns:
            z_goal: [B, 512] - VPTå…¼å®¹çš„visual embedding
        """
        B = videollama_features.shape[0]
        
        if not self.use_attention:
            # æ–¹æ¡ˆA: ç®€å•flatten + MLP
            features_flat = videollama_features.view(B, -1)  # [B, 32*768]
            z_goal = self.aligner(features_flat)  # [B, 512]
        
        else:
            # æ–¹æ¡ˆB: æ³¨æ„åŠ›æ± åŒ–ï¼ˆæ¨èï¼‰
            # 1. ä½¿ç”¨å¯å­¦ä¹ çš„queryæ± åŒ–32ä¸ªvisual tokens
            pool_query = self.pool_query.expand(B, -1, -1)  # [B, 1, 768]
            pooled_features, _ = self.query_attention(
                query=pool_query,
                key=videollama_features,
                value=videollama_features
            )  # [B, 1, 768]
            
            # 2. æŠ•å½±åˆ°VPTç©ºé—´
            pooled_features = pooled_features.squeeze(1)  # [B, 768]
            z_goal = self.projection(pooled_features)  # [B, 512]
        
        return z_goal
```

#### 2.2 å®Œæ•´çš„æ–‡æœ¬â†’VPTæµç¨‹

```python
class TextToVPTGoalPredictor(nn.Module):
    """
    å®Œæ•´çš„æ–‡æœ¬æŒ‡ä»¤ â†’ VPT goal embeddingæµç¨‹
    """
    def __init__(self):
        super().__init__()
        
        # 1. Video-LLaMAï¼ˆé¢„è®­ç»ƒæƒé‡ï¼‰
        self.videollama = VideoLLaMAForGoalPrediction()
        
        # 2. å¯¹é½æ¨¡å—ï¼ˆéœ€è¦è®­ç»ƒï¼‰
        self.aligner = VideoLLaMAToVPTAligner(
            videollama_dim=768,
            num_queries=32,
            vpt_dim=512,
            use_attention=True
        )
        
        # 3. VPT Policyï¼ˆé¢„è®­ç»ƒæƒé‡ï¼Œfrozenï¼‰
        self.vpt_policy = VPTPolicy()
        self.vpt_policy.eval()
        for param in self.vpt_policy.parameters():
            param.requires_grad = False
    
    def forward(self, text_instruction, current_observation):
        """
        Args:
            text_instruction: str - "chop tree, get a log"
            current_observation: [B, 3, H, W] - å½“å‰æ¸¸æˆç”»é¢
        
        Returns:
            action: dict - Minecraft action
        """
        # Step 1: æ–‡æœ¬ â†’ Video-LLaMA visual features
        with torch.no_grad():  # Video-LLaMAå¯ä»¥frozenæˆ–å¾®è°ƒ
            videollama_features = self.videollama(text_instruction)
            # [B, 32, 768]
        
        # Step 2: å¯¹é½åˆ°VPTç©ºé—´
        z_goal = self.aligner(videollama_features)
        # [B, 512]
        
        # Step 3: VPT policyç”ŸæˆåŠ¨ä½œ
        action = self.vpt_policy(
            observation=current_observation,
            goal_embedding=z_goal
        )
        
        return action, z_goal
```

---

## ğŸ“ è®­ç»ƒç­–ç•¥

### è®­ç»ƒé˜¶æ®µ1: Video-LLaMAåœ¨Minecraftä¸Šçš„é¢„è®­ç»ƒ

#### æ•°æ®å‡†å¤‡

```python
# ä½¿ç”¨MineCLIPçš„730K YouTubeæ•°æ®
minecraft_youtube_data = {
    'video_paths': [...],  # 730Kè§†é¢‘
    'transcripts': [...],  # æ—¶é—´å¯¹é½çš„å­—å¹•
    'task_labels': [...],  # å¯é€‰ï¼šä»»åŠ¡ç±»åˆ«æ ‡æ³¨
}

# æ•°æ®å¢å¼º
def prepare_videollama_data(video_path, transcript):
    """
    ä¸ºVideo-LLaMAå‡†å¤‡è®­ç»ƒæ•°æ®
    
    ç­–ç•¥1: è§†é¢‘-å­—å¹•å¯¹æ¯”å­¦ä¹ ï¼ˆç±»ä¼¼MineCLIPï¼‰
    ç­–ç•¥2: è§†é¢‘æ‘˜è¦ç”Ÿæˆ
    ç­–ç•¥3: ç›®æ ‡ç”»é¢é¢„æµ‹
    """
    # 1. æå–è§†é¢‘ç‰‡æ®µï¼ˆ16å¸§ï¼‰
    video_clip = extract_frames(video_path, num_frames=16)
    
    # 2. æå–å¯¹åº”çš„å­—å¹•
    caption = get_aligned_caption(transcript, video_clip.timestamp)
    
    # 3. æå–"æˆåŠŸæ—¶åˆ»"çš„ç”»é¢ä½œä¸ºç›®æ ‡
    success_frames = extract_success_frames(video_path)
    
    return {
        'video': video_clip,
        'caption': caption,
        'goal_frames': success_frames,  # ç”¨äºè®­ç»ƒç›®æ ‡é¢„æµ‹
    }
```

#### è®­ç»ƒç›®æ ‡

```python
def train_videollama_minecraft(model, dataloader, epochs=10):
    """
    Video-LLaMAåœ¨Minecraftä¸Šçš„è®­ç»ƒ
    
    Lossç»„åˆ:
    1. å¯¹æ¯”å­¦ä¹ loss (ç±»ä¼¼CLIP)
    2. ç›®æ ‡é¢„æµ‹loss
    3. è¯­è¨€å»ºæ¨¡lossï¼ˆå¯é€‰ï¼‰
    """
    for epoch in range(epochs):
        for batch in dataloader:
            video = batch['video']  # [B, 16, 3, H, W]
            caption = batch['caption']  # [B, seq_len]
            goal_frames = batch['goal_frames']  # [B, 16, 3, H, W]
            
            # Loss 1: è§†é¢‘-æ–‡æœ¬å¯¹æ¯”å­¦ä¹ 
            video_features = model.encode_video(video)
            text_features = model.encode_text(caption)
            contrastive_loss = clip_loss(video_features, text_features)
            
            # Loss 2: ç›®æ ‡é¢„æµ‹
            # ä»æ–‡æœ¬é¢„æµ‹ç›®æ ‡ç”»é¢çš„visual features
            predicted_goal_features = model.predict_goal(caption)
            actual_goal_features = model.encode_video(goal_frames)
            goal_prediction_loss = mse_loss(
                predicted_goal_features, 
                actual_goal_features
            )
            
            # æ€»loss
            total_loss = contrastive_loss + 0.5 * goal_prediction_loss
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            optimizer.step()
```

### è®­ç»ƒé˜¶æ®µ2: å¯¹é½æ¨¡å—è®­ç»ƒ

#### æ–¹æ¡ˆA: ä½¿ç”¨æˆåŠŸtrialså¯¹é½ï¼ˆæ¨èï¼‰

```python
def train_aligner_with_success_trials(aligner, videollama, vpt_encoder):
    """
    ä½¿ç”¨æˆåŠŸçš„æ¸¸æˆtrialsè®­ç»ƒå¯¹é½æ¨¡å—
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. å¯¹äºæˆåŠŸçš„trialï¼Œæå–æˆåŠŸæ—¶åˆ»çš„ç”»é¢
    2. ç”¨VPTçš„visual encoderç¼–ç è¿™äº›ç”»é¢
    3. è®­ç»ƒVideo-LLaMAé¢„æµ‹çš„featuresä¸VPT featureså¯¹é½
    """
    # æ•°æ®ï¼šresults/evaluation/all_tasks_*/
    success_trials = load_success_trials('results/evaluation/')
    
    for trial in success_trials:
        instruction = trial['instruction']  # "chop tree, get a log"
        success_frames = trial['success_frames']  # æˆåŠŸæ—¶åˆ»çš„16å¸§
        
        # 1. Video-LLaMAé¢„æµ‹goal features
        videollama_features = videollama.predict_goal(instruction)
        # [1, 32, 768]
        
        # 2. VPT encoderç¼–ç å®é™…æˆåŠŸç”»é¢
        with torch.no_grad():
            vpt_visual_embedding = vpt_encoder(success_frames)
            # [1, 512]
        
        # 3. å¯¹é½
        predicted_embedding = aligner(videollama_features)
        # [1, 512]
        
        # Loss: å¯¹é½é¢„æµ‹çš„embeddingå’Œå®é™…çš„VPT embedding
        alignment_loss = nn.MSELoss()(
            predicted_embedding, 
            vpt_visual_embedding
        )
        
        # ä¹Ÿå¯ä»¥ç”¨cosine similarity
        cosine_loss = 1 - F.cosine_similarity(
            predicted_embedding,
            vpt_visual_embedding
        ).mean()
        
        total_loss = alignment_loss + 0.1 * cosine_loss
        total_loss.backward()
```

#### æ–¹æ¡ˆB: å¼ºåŒ–å­¦ä¹ fine-tuneï¼ˆå¯é€‰ï¼‰

```python
def finetune_with_rl(model, env, num_episodes=1000):
    """
    ä½¿ç”¨RL fine-tuneæ•´ä¸ªpipeline
    
    æ ¸å¿ƒæ€æƒ³ï¼š
    1. ç”¨Video-LLaMAé¢„æµ‹goal
    2. ç”¨VPTæ‰§è¡Œ
    3. æ ¹æ®ä»»åŠ¡æˆåŠŸä¸å¦è°ƒæ•´Video-LLaMAçš„é¢„æµ‹
    """
    for episode in range(num_episodes):
        instruction = sample_instruction()
        obs = env.reset()
        
        # 1. é¢„æµ‹goal
        z_goal = model.predict_goal(instruction)
        
        # 2. VPTæ‰§è¡Œ
        done = False
        total_reward = 0
        while not done:
            action = vpt_policy(obs, z_goal)
            obs, reward, done, info = env.step(action)
            total_reward += reward
        
        # 3. æ ¹æ®æˆåŠŸä¸å¦æ›´æ–°
        # ä½¿ç”¨REINFORCEæˆ–å…¶ä»–PGç®—æ³•
        if info['success']:
            # å¢å¼ºè¿™ä¸ªgoal prediction
            loss = -log_prob * total_reward
        else:
            # æƒ©ç½šè¿™ä¸ªprediction
            loss = log_prob * total_reward
        
        loss.backward()
```

---

## ğŸ“Š å®æ–½è®¡åˆ’

### Phase 1: å‡†å¤‡é˜¶æ®µï¼ˆ1-2å‘¨ï¼‰

**ä»»åŠ¡**ï¼š
1. âœ… æ­å»ºVideo-LLaMAç¯å¢ƒ
2. âœ… ä¸‹è½½é¢„è®­ç»ƒæƒé‡
3. âœ… å‡†å¤‡Minecraftæ•°æ®
4. âœ… å®ç°åŸºç¡€æ¶æ„ä»£ç 

**ä»£ç **ï¼š
```bash
# 1. å®‰è£…Video-LLaMA
git clone https://github.com/DAMO-NLP-SG/Video-LLaMA
cd Video-LLaMA
pip install -r requirements.txt

# 2. ä¸‹è½½é¢„è®­ç»ƒæƒé‡
bash download_checkpoints.sh

# 3. å‡†å¤‡æ•°æ®
python scripts/prepare_minecraft_data.py \
    --youtube_dir data/mineclip_youtube \
    --output_dir data/videollama_minecraft
```

### Phase 2: Video-LLaMAè®­ç»ƒï¼ˆ2-4å‘¨ï¼‰

**ä»»åŠ¡**ï¼š
1. åœ¨Minecraft YouTubeæ•°æ®ä¸Šé¢„è®­ç»ƒ
2. è®­ç»ƒç›®æ ‡é¢„æµ‹å¤´
3. è¯„ä¼°è§†é¢‘-æ–‡æœ¬å¯¹é½è´¨é‡

**ç¡¬ä»¶éœ€æ±‚**ï¼š
- GPU: 8x A100 40GBï¼ˆæˆ– 4x A100 80GBï¼‰
- è®­ç»ƒæ—¶é—´: 2-3å‘¨
- ä¼°è®¡æˆæœ¬: $5,000-10,000ï¼ˆäº‘GPUï¼‰

**è®­ç»ƒè„šæœ¬**ï¼š
```bash
# åˆ†å¸ƒå¼è®­ç»ƒ
torchrun --nproc_per_node=8 train_videollama_minecraft.py \
    --data_dir data/videollama_minecraft \
    --model_size 7B \
    --batch_size 2 \
    --gradient_accumulation_steps 4 \
    --epochs 10 \
    --learning_rate 1e-5 \
    --output_dir checkpoints/videollama_minecraft_7b
```

### Phase 3: å¯¹é½æ¨¡å—è®­ç»ƒï¼ˆ1å‘¨ï¼‰

**ä»»åŠ¡**ï¼š
1. æå–æˆåŠŸtrialsçš„VPT visual embeddings
2. è®­ç»ƒå¯¹é½æ¨¡å—
3. éªŒè¯å¯¹é½è´¨é‡

**ä»£ç **ï¼š
```bash
# 1. æå–VPT embeddings
python scripts/extract_vpt_embeddings.py \
    --success_trials results/evaluation/all_tasks_* \
    --vpt_checkpoint data/weights/vpt_policy.pt \
    --output data/vpt_visual_embeddings.pkl

# 2. è®­ç»ƒå¯¹é½æ¨¡å—
python train_aligner.py \
    --videollama_checkpoint checkpoints/videollama_minecraft_7b \
    --vpt_embeddings data/vpt_visual_embeddings.pkl \
    --epochs 50 \
    --batch_size 32 \
    --learning_rate 1e-4
```

### Phase 4: é›†æˆå’Œè¯„ä¼°ï¼ˆ1-2å‘¨ï¼‰

**ä»»åŠ¡**ï¼š
1. é›†æˆåˆ°ç°æœ‰STEVE-1ç³»ç»Ÿ
2. è¿è¡ŒPriorè¯„ä¼°
3. å¯¹æ¯”MineCLIP vs Video-LLaMA

**è¯„ä¼°è„šæœ¬**ï¼š
```bash
# ç”Ÿæˆinstruction-video pairsï¼ˆä½¿ç”¨Video-LLaMAï¼‰
python scripts/generate_pairs_videollama.py \
    --videollama_checkpoint checkpoints/videollama_minecraft_7b \
    --aligner_checkpoint checkpoints/aligner_best.pt \
    --eval_result_dir results/evaluation/all_tasks_* \
    --output_dir results/instruction_video_pairs_videollama

# è¿è¡ŒPriorè¯„ä¼°
bash scripts/run_prior_evaluation.sh \
    --instruction-video-pairs results/instruction_video_pairs_videollama \
    --output-dir results/prior_evaluation_videollama

# å¯¹æ¯”åˆ†æ
python scripts/compare_models.py \
    --mineclip_results results/prior_evaluation/prior_eval_* \
    --videollama_results results/prior_evaluation_videollama \
    --output comparison_report.html
```

---

## ğŸ“ˆ é¢„æœŸæ”¹è¿›

### æŒ‡æ ‡å¯¹æ¯”

| æŒ‡æ ‡ | MineCLIP | Video-LLaMAï¼ˆé¢„æœŸï¼‰ | æ”¹è¿› |
|------|---------|------------------|------|
| **æ–‡æœ¬åŒºåˆ†åº¦** | 1.3% | 15-30% | âœ… +13-28% |
| **è§†è§‰ç›¸ä¼¼åº¦** | 0.925 | 0.70-0.80 | âœ… -12-22% |
| **Goal Accuracy** | 0.91-0.99 | 0.60-0.75 | âœ… æ›´çœŸå® |
| **Discriminability** | 0.12 | 0.50-0.70 | âœ… +38-58% |
| **è¯­ä¹‰é²æ£’æ€§** | 0.96-0.99 | 0.85-0.92 | âœ… æ›´åˆç† |

### å®šæ€§æ”¹è¿›

**MineCLIPé—®é¢˜**ï¼š
```
"kill pig" vs "chop tree": 0.854 â† å‡ ä¹ä¸€æ ·
"kill pig" vs "build house": 0.878 â† å·®å¼‚ä»…2.4%
```

**Video-LLaMAé¢„æœŸ**ï¼š
```
"kill pig" vs "chop tree": 0.65-0.75 â† æ˜æ˜¾ä¸åŒ
"kill pig" vs "build house": 0.45-0.55 â† å®Œå…¨ä¸åŒ
```

---

## ğŸ”§ ä»£ç å®ç°ç¤ºä¾‹

### å®Œæ•´çš„æ¨ç†æµç¨‹

```python
class STEVE1WithVideoLLaMA:
    """
    é›†æˆVideo-LLaMAçš„STEVE-1ç³»ç»Ÿ
    """
    def __init__(
        self,
        videollama_checkpoint,
        aligner_checkpoint,
        vpt_checkpoint
    ):
        # 1. åŠ è½½Video-LLaMA
        self.videollama = VideoLLaMAForGoalPrediction()
        self.videollama.load_state_dict(
            torch.load(videollama_checkpoint)
        )
        self.videollama.eval()
        
        # 2. åŠ è½½å¯¹é½æ¨¡å—
        self.aligner = VideoLLaMAToVPTAligner()
        self.aligner.load_state_dict(
            torch.load(aligner_checkpoint)
        )
        
        # 3. åŠ è½½VPT policy
        self.vpt_policy = load_vpt_policy(vpt_checkpoint)
    
    def play_minecraft(self, instruction, max_steps=1000):
        """
        æ ¹æ®æ–‡æœ¬æŒ‡ä»¤ç©Minecraft
        
        Args:
            instruction: str - "chop tree, get a log"
            max_steps: int - æœ€å¤§æ­¥æ•°
        
        Returns:
            success: bool
            trajectory: list
        """
        # 1. ä»æ–‡æœ¬é¢„æµ‹goal embedding
        with torch.no_grad():
            # Video-LLaMAé¢„æµ‹
            videollama_features = self.videollama(instruction)
            
            # å¯¹é½åˆ°VPTç©ºé—´
            z_goal = self.aligner(videollama_features)
        
        # 2. åˆå§‹åŒ–ç¯å¢ƒ
        env = gym.make('MineRLBasaltFindCave-v0')
        obs = env.reset()
        
        # 3. æ‰§è¡Œ
        trajectory = []
        for step in range(max_steps):
            # VPTç”ŸæˆåŠ¨ä½œ
            action = self.vpt_policy(obs, z_goal)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, info = env.step(action)
            
            trajectory.append({
                'obs': obs,
                'action': action,
                'reward': reward,
            })
            
            if done or info.get('success'):
                break
        
        env.close()
        
        return info.get('success', False), trajectory
```

---

## âš ï¸ æŒ‘æˆ˜å’Œé£é™©

### æŠ€æœ¯æŒ‘æˆ˜

1. **è®¡ç®—èµ„æºéœ€æ±‚é«˜**
   - Video-LLaMA 7B: ~28GB GPUå†…å­˜
   - è®­ç»ƒéœ€è¦8x A100
   - æ¨ç†é€Ÿåº¦å¯èƒ½è¾ƒæ…¢ï¼ˆvs MineCLIPï¼‰

2. **è®­ç»ƒæ•°æ®è´¨é‡**
   - MineCLIPçš„YouTubeæ•°æ®å¯èƒ½ä¸å¤Ÿç²¾ç¡®
   - éœ€è¦äººå·¥æ ‡æ³¨"æˆåŠŸæ—¶åˆ»"
   - å¯èƒ½éœ€è¦é¢å¤–æ”¶é›†æ•°æ®

3. **å¯¹é½éš¾åº¦**
   - Video-LLaMAå’ŒVPTçš„visual spaceå¯èƒ½å·®å¼‚å¤§
   - éœ€è¦å¤§é‡æˆåŠŸtrialsä½œä¸ºè®­ç»ƒæ•°æ®
   - å¯¹é½è´¨é‡ç›´æ¥å½±å“æœ€ç»ˆæ€§èƒ½

### è§£å†³æ–¹æ¡ˆ

1. **é™ä½è®¡ç®—æˆæœ¬**
   ```python
   # ä½¿ç”¨é‡åŒ–
   model = load_videollama_4bit()  # 4-bité‡åŒ–
   
   # ä½¿ç”¨LoRAå¾®è°ƒ
   from peft import get_peft_model, LoraConfig
   lora_config = LoraConfig(r=16, lora_alpha=32)
   model = get_peft_model(model, lora_config)
   ```

2. **æ•°æ®å¢å¼º**
   ```python
   # åˆ©ç”¨ç°æœ‰æˆåŠŸtrials
   # ä½¿ç”¨æ•°æ®å¢å¼ºç”Ÿæˆæ›´å¤šæ ·æœ¬
   # å¯èƒ½éœ€è¦äººå·¥æ ‡æ³¨100-500ä¸ªé«˜è´¨é‡æ ·æœ¬
   ```

3. **æ¸è¿›å¼è®­ç»ƒ**
   ```python
   # Phase 1: å…ˆç”¨MineCLIPçš„visual embeddingsè®­ç»ƒå¯¹é½
   # Phase 2: é€æ¸æ›¿æ¢ä¸ºVPTçš„visual embeddings
   # Phase 3: ç«¯åˆ°ç«¯fine-tune
   ```

---

## ğŸ“š å‚è€ƒå®ç°

### å…³é”®ä»£ç æ–‡ä»¶ç»“æ„

```
src/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ videollama_goal_predictor.py  # Video-LLaMAç›®æ ‡é¢„æµ‹
â”‚   â”œâ”€â”€ vpt_aligner.py                # å¯¹é½æ¨¡å—
â”‚   â””â”€â”€ steve1_videollama.py          # é›†æˆç³»ç»Ÿ
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_videollama.py           # Video-LLaMAè®­ç»ƒ
â”‚   â”œâ”€â”€ train_aligner.py              # å¯¹é½æ¨¡å—è®­ç»ƒ
â”‚   â””â”€â”€ train_end2end.py              # ç«¯åˆ°ç«¯è®­ç»ƒ
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ eval_videollama_prior.py      # Priorè¯„ä¼°
â”‚   â””â”€â”€ compare_models.py             # æ¨¡å‹å¯¹æ¯”
â””â”€â”€ utils/
    â”œâ”€â”€ videollama_utils.py
    â””â”€â”€ vpt_utils.py
```

---

## âœ… æ€»ç»“

### æ–¹æ¡ˆä¼˜åŠ¿

1. **âœ… æ˜¾è‘—æé«˜åŒºåˆ†åº¦**ï¼šé¢„æœŸ15-30%æ–‡æœ¬åŒºåˆ†åº¦ï¼ˆvs 1.3%ï¼‰
2. **âœ… æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›**ï¼šVideo-LLaMAåŸºäº7B LLM
3. **âœ… æ”¯æŒå¤æ‚æŒ‡ä»¤**ï¼šå¯ä»¥ç†è§£é•¿å¥ã€å¤æ‚æè¿°
4. **âœ… å¯è§£é‡Šæ€§å¼º**ï¼šVideo-LLaMAå¯ä»¥ç”Ÿæˆæ–‡æœ¬æè¿°

### å®æ–½å»ºè®®

**çŸ­æœŸï¼ˆå¦‚æœèµ„æºæœ‰é™ï¼‰**ï¼š
- ä½¿ç”¨Video-LLaMAçš„é¢„è®­ç»ƒæƒé‡
- åªè®­ç»ƒå¯¹é½æ¨¡å—ï¼ˆ1å‘¨ï¼Œå•å¡A100ï¼‰
- å¿«é€ŸéªŒè¯æ•ˆæœ

**ä¸­æœŸï¼ˆæ¨èï¼‰**ï¼š
- åœ¨Minecraftæ•°æ®ä¸Šfine-tune Video-LLaMA
- è®­ç»ƒé«˜è´¨é‡å¯¹é½æ¨¡å—
- å®Œæ•´è¯„ä¼°å’Œå¯¹æ¯”

**é•¿æœŸ**ï¼š
- ç«¯åˆ°ç«¯fine-tuneæ•´ä¸ªç³»ç»Ÿ
- æ”¶é›†æ›´å¤šé«˜è´¨é‡æ•°æ®
- å»ºç«‹Minecraftè§†é¢‘-æ–‡æœ¬benchmark

---

**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**ï¼š
1. è¯„ä¼°GPUèµ„æºå¯ç”¨æ€§
2. æ­å»ºVideo-LLaMAç¯å¢ƒ
3. å®ç°å¯¹é½æ¨¡å—åŸå‹
4. å°è§„æ¨¡æµ‹è¯•éªŒè¯

å¦‚æœ‰å……è¶³èµ„æºï¼Œè¿™ä¸ªæ–¹æ¡ˆé¢„æœŸèƒ½å°†ä»»åŠ¡åŒºåˆ†åº¦æå‡50-80%ï¼ğŸš€






